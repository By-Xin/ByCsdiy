{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Support Vector Machines\n",
    "\n",
    "**CS229/Notes**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Decision Boundary / Separating Hyperplane"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**回忆Logistics Regression**\n",
    "\n",
    "logistics的拟合函数是：$h_\\theta(x) = g(\\theta^Tx)$.\n",
    "\n",
    "若以正常的阈值$p=0.5$作为分界来看，$\\theta^Tx>0$与否便构成了一个决策边界，或称separating hyperplane. 数据点距离该决策平面越远，则判断其属于某一类别的confidence越高."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 绘制sigmoid函数图像\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    " \n",
    "z = np.arange(-10, 10, 0.1)\n",
    "phi_z = sigmoid(z)\n",
    "\n",
    "plt.plot(z, phi_z)\n",
    "plt.axvline(0.0, color='k') # 在x=0处画一条垂直的线\n",
    "plt.axhspan(0.0, 1.0, facecolor='1.0', edgecolor='k', ls='dashed') # 在y=0和y=1处画一条水平的虚线\n",
    "plt.axhline(y=0.5, ls='dashed', color='k') # 在y=0.5处画一条水平的虚线\n",
    "plt.yticks([0.0, 0.5, 1.0]) # 设置y轴的刻度\n",
    "plt.ylim(-0.1, 1.1) # 设置y轴的范围\n",
    "plt.xlabel('z')\n",
    "plt.ylabel('sigmoid (z)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Some Notations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在本小节，我们将主要讨论一个**二元分类问题**，采用**linear classifier**. \n",
    "\n",
    "记标签为$y\\in \\{-1,1\\}$，相关特征为$x$，模型参数为$\\omega, \\beta$，构建的拟合模型为:\n",
    "$$ h_{\\omega, b}(x) = g(\\omega^Tx + b)$$\n",
    "其中$g(z) = \\text{sign}(z)$，即$g(z) = 1$ if $z\\geq 0$，$g(z) = -1$ if $z<0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Functional & Geometic Margins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Functional Margin $\\gamma$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functional Margin 作用 ：\n",
    "\n",
    "度量预测的*正确性*与*置信度*。具体而言，Func Margin为正说明预测正确，其绝对值越大说明预测越置信。\n",
    "\n",
    "> 注：其实由下面的构造形式可知，正的Func Margin意味着$y^{(i)}$和$\\omega^Tx^{(i)}$同号，即预测正确。\n",
    "\n",
    "#### Functional Margin 定义公式：\n",
    "  \n",
    "  $$\\hat{\\gamma}^{(i)} = y^{(i)}(\\omega^Tx^{(i)} + b)$$\n",
    "\n",
    "#### Functional Margin 存在的问题\n",
    "\n",
    "缺少normalization。同倍改变$\\omega$和$b$，Func Margin会变大，但并不会改变实际的决策边界提高预测效果。\n",
    "\n",
    "#### 训练集的Functional Margin\n",
    "\n",
    "给定训练集$S = \\{(x^{(i)}, y^{(i)}); i = 1, ..., m\\}$，则训练集的Func Margin（记为$\\hat\\gamma$）为所有样本的Func Margin的最小值，即\n",
    "\n",
    "$$\\hat\\gamma = \\min_{i=1, ..., m} \\hat\\gamma^{(i)}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Geometric Margin $\\hat\\gamma$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现给出如图所示的决策边界$\\omega^Tx+b=0$：\n",
    "\n",
    "![](https://michael-1313341240.cos.ap-shanghai.myqcloud.com/202308131632970.png)\n",
    "\n",
    "在上图中，我们可以给出如下事实陈述：\n",
    "\n",
    "- 由解析几何知识可知，向量$\\omega$定与超平面$\\omega^Tx+b=0$垂直（正交），即$\\omega$是超平面的法向量\n",
    "- 记$A$为$y^{(i)}=1$的一样本点（其坐标可表示为向量$x^{(i)}$），则A到决策边界的距离为垂线段$AB$，记之为$\\gamma^{(i)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**下面具体求解$\\gamma^{(i)}$**\n",
    "\n",
    "求解思路：我们已知 *(1) AB垂直于超平面; (2) B位于超平面$\\omega^Txb=0$上; (3)$\\omega$是超平面的一个法向量*。 因此我们可以求解B的坐标，通过 **B落在超平面上** 这一事实构造等式求解方程。\n",
    "\n",
    "具体而言，B的坐标等于*A的坐标沿着法向量方向回退$\\gamma^{(i)}$个长度的距离*，因此我们有：\n",
    "$$ \\omega^T(x^{(i)}-\\gamma^{(i)}\\frac{\\omega}{||\\omega||})+b=0$$\n",
    "\n",
    "通过求解上述方程，即可求得：\n",
    "$$\n",
    "\\gamma^{(i)}=\\frac{w^T x^{(i)}+b}{\\|w\\|}=\\left(\\frac{w}{\\|w\\|}\\right)^T x^{(i)}+\\frac{b}{\\|w\\|}\n",
    "$$\n",
    "\n",
    "更一般地，考虑到正负样本的不同，更一般的公式为：\n",
    "$$ \\gamma^{(i)}=y^{(i)}\\left(\\left(\\frac{w}{\\|w\\|}\\right)^T x^{(i)}+\\frac{b}{\\|w\\|}\\right)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**说明：**\n",
    "1. 当$||\\omega|| =1$时，Geometric Margin 等价于 Functional Margin\n",
    "2. 由于进行了正则化，Geo Margin杜绝了Func Margin存在的缩放问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同理，定义训练集（$S = \\{(x^{(i)}, y^{(i)}); i = 1, \\dots, m\\}$）的geometric margin：\n",
    "$$\\gamma = \\min_{i=1,\\dots,m} \\hat{\\gamma}^{(i)}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Optimal Margin Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "朴素地，我们希望得到的分割平面尽可能使得上文的margin $\\gamma$足够大，下面就是要量化这一想法。\n",
    "\n",
    "在本节的讨论中，我们都假设数据集是**线性可分的（linearly separable）**，即存在一些超平面可以将正负样本完全分开。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 朴素的优化问题\n",
    "\n",
    "上述的想法可以概括为如下优化问题：\n",
    "\n",
    "$$ \\begin{align*} \\max_{\\gamma, \\omega, b} \\quad &\\gamma \\\\\n",
    "\\text{s.t. } \\quad &y^{(i)}(\\omega^Tx^{(i)} + b) \\geq \\gamma, \\quad i = 1, \\ldots, m \\\\ \n",
    "&||\\omega|| = 1 \\end{align*} $$\n",
    "\n",
    "即，希望能够最大化$\\gamma$，同时满足所有的训练样本的*func. margin*都至少为$\\gamma$。同时参见5.3的讨论可知，$||\\omega|| = 1$使得* geo. margin *等价于* func. margin*。\n",
    "\n",
    "然而上述内容是***非凸优化*** 的，因此下给出一个等价的凸优化问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 改进的优化问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "归根到底我们希望得到的是Geo Margin最大的分类器，因此通过下列方式去掉非凸部分$||\\omega||=1$：\n",
    "\n",
    "$$ \\begin{align*} \\max_{\\hat\\gamma, \\omega, b} \\quad &\\frac{\\hat\\gamma}{||\\omega||} \\\\\n",
    "\\text{s.t. } \\quad &y^{(i)}(\\omega^Tx^{(i)} + b) \\geq \\hat\\gamma, \\quad i = 1, \\ldots, m \\end{align*} $$\n",
    "\n",
    "然而这里的目标函数$\\frac{\\hat\\gamma}{||\\omega||}$依然是非凸的(sad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 最终的凸优化问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最终我们得到的凸优化问题如下所示（这里我们先给出具体形式再解释其原理）：\n",
    "$$\n",
    "\\begin{align}\n",
    "\\min_{\\gamma,\\omega,b} \\quad & \\frac{1}{2}||\\omega||^2 \\\\\n",
    "\\text{s.t.} \\quad & y^{(i)}(\\omega^Tx^{(i)}+b) \\geq 1, \\quad i=1,\\cdots,m\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 首先我们可以规定$\\hat\\gamma = 1$，则原始优化问题可以转化为$\\max \\{1/||\\omega||\\}$\n",
    "\n",
    "    这是因为正如前文在介绍func. margin时所说，我们可以通过任意成倍地更改$\\omega, b$的值来改变func. margin的值，而这不会影响到我们的最优化问题的解。\n",
    "\n",
    "- 其次，我们可以将$\\max \\{1/||\\omega||\\}$转化为$\\min \\{||\\omega||^2\\}$，这是显然的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过上述变换最终得到的优化问题是凸优化可解的。这里的优化解即为**optimal margin classifier**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 Lagrange Duality 拉格朗日对偶"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[bilibili: “拉格朗日对偶问题”如何直观理解？“KKT条件” “Slater条件” “凸优化”打包理解](https://www.bilibili.com/video/BV1HP4y1Y79e/?share_source=copy_web&vd_source=9471c7cd3fca9ffedd9167aefed57c6d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intro: Lagrange Multiplier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "考虑如下形式的优化问题：\n",
    "$$\\begin{aligned}\n",
    "\\min_{w} &\\quad f(w) \\\\\n",
    "\\text{s.t.} &\\quad h_i(w) = 0, \\quad i = 1, \\ldots, l \n",
    "\\end{aligned}$$\n",
    "\n",
    "上述的优化问题可以通过**拉格朗日乘子法**来求解。\n",
    "\n",
    "\n",
    "具体地，我们可以得到*Lagarangian*：\n",
    "$$ \\mathcal{L(w,\\beta)}= f(w) + \\sum_{i=1}^l \\beta_i h_i (w)$$\n",
    "其中一系列引入的参数$\\beta_i$称为拉格朗日乘子 *(Lagrange multipliers)* .\n",
    "\n",
    "通过求解下列方程组求解Lagrange Mult.：\n",
    "$$ \\partial\\mathcal{L}/\\partial w = 0; \\quad \\partial\\mathcal{L}/\\partial \\beta_i = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Primal Optimization Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里我们需要把上面的Lagrange Mult进行推广，考虑**约束条件中存在不等式**的情况（我们称这种优化问题为 ***primal optimization problem（原始问题）*** 。用数学语言表示为：\n",
    "$$ \\begin{align}\n",
    "\\min_{w} \\quad & f(w) \\\\\n",
    "\\text{s.t.} \\quad & g_i(w) \\leq 0, \\quad i = 1, \\cdots, k\n",
    "\\\\& h_i(w) = 0, \\quad i = 1, \\cdots, l\n",
    "\\end{align} $$\n",
    "\n",
    "$\\diamond$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因此引入***Generalized Lagrangian***，即\n",
    "$$\n",
    "\\mathcal{L}(w,\\alpha,\\beta) = f(w) + \\sum_{i=1}^k \\alpha_i g_i(w) + \\sum_{i=1}^l\\beta_i h_i(w)\n",
    "$$\n",
    "\n",
    "其中$\\alpha_i$和$\\beta_i$是拉格朗日乘子.\n",
    "\n",
    "$\\diamond$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**另外定义$\\theta_{\\mathcal{P}}$** *(primal)*\n",
    "$$ \\theta_{\\mathcal{P}}(w) = \\max_{\\alpha,\\beta \\textit{ s.t. } \\alpha_i\\ge 0} \\mathcal{L(w,\\alpha,\\beta)} \\quad\\dagger$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以证明,当存在违反约束（式(2)/(3)）中的条件时，$\\theta_{\\mathcal{P}} \\to \\infty$ （即令 $\\mathcal{L}$ 中违反条件的项前的系数$\\alpha / \\beta \\to \\infty$即可）；相反地，若全部满足约束，则$\\theta_{\\mathcal{P}} = f(w)$，即：\n",
    "$$\n",
    "\\theta_{\\mathcal{P}}(w)= \\begin{cases}f(w) & \\text { if } w \\text { satisfies primal constraints } \\\\ \\infty & \\text { otherwise. }\\end{cases}\n",
    "$$\n",
    "\n",
    "$\\diamond$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因此，通过$\\theta_{\\mathcal{P}}$，我们就可以将*primal problem*的（1）～（3）式加以概括。\n",
    "\n",
    "式（1）描述的问题是：$\\min_{w}f(w)$，在满足约束时，就等价于$\\min_w\\theta_{\\mathcal{P}}(w,b)$，再代入式$(\\dagger)$有：\n",
    "$$\\min_w\\theta_{\\mathcal{P}}(w,b)=\\min_w\\max_{\\alpha,\\beta: \\alpha_i\\geq0} \\mathcal{L}(w,\\alpha,\\beta)\\quad \\star$$\n",
    "\n",
    "而这一优化问题与primal problem是完全等价的，且具有相同的解。为方便起见，我们记$p^* := \\min_w\\theta_{\\mathcal{P}}(w)$，称$p^*$为 ***primal optimal value***。\n",
    "\n",
    "$\\diamond$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dual Optimization Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **理解：** 对偶是实质相同但从不同角度提出不同提法的一对问题。有时候原问题 (Primal Problem) 不太好解，但是对偶问题 (Dual Problem) 却很好解，我们就可以通过求解对偶问题来迂回地解答原问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "类似地**定义Lagrange 对偶函数$\\theta_{\\mathcal{D}}$** *(dual)*：\n",
    "$$\\theta_{\\mathcal{D}}(\\alpha,\\beta) = \\min_{w} \\mathcal{L}(w,\\alpha,\\beta) ,\\alpha\\ge0    \\quad \\dagger^2$$ \n",
    "\n",
    "*注意，$\\theta_{\\mathcal{D}}$和$\\theta_{\\mathcal{P}}$的区别在于：$\\theta_{\\mathcal{D}}$是关于$\\alpha$和$\\beta$的优化函数，而$\\theta_{\\mathcal{P}}$是关于$w$的优化函数。*\n",
    "\n",
    "另外可以证明，无论原问题函数$\\theta_{\\mathcal{P}}$的凹凸性，恒有对偶函数$\\theta_{\\mathcal{D}}$是一个凹函数.\n",
    "\n",
    "> 凹函数：$ f(\\theta x_1 + (1-\\theta)x_2) \\ge \\theta f(x_1) + (1-\\theta)f(x_2) $\n",
    "\n",
    "$\\diamond$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "顺势，我们引入***dual optimization problem***:\n",
    "$$ \\max \\limits_{\\alpha,\\beta:\\alpha_i\\ge0} \\theta_{\\mathcal{D}}(\\alpha,\\beta) = \\max\\limits_{\\alpha,\\beta:\\alpha_i\\ge0} \\min_w \\mathcal{L}(w,\\alpha,\\beta) \\quad \\star\\star$$\n",
    "\n",
    "并同理记之为$d^*$。\n",
    "\n",
    "注意到，$p^*$与$d^*$的唯一区别是二者$\\min,\\max$的顺序不同。\n",
    "\n",
    "\n",
    "$\\diamond$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 强、弱对偶条件\n",
    "\n",
    "#### a. 弱对偶\n",
    "\n",
    "在任何条件下，可以证明下列不等关系是恒成立的：\n",
    "$$d^* = \\max_{\\alpha,\\beta:\\alpha_i\\ge0} \\min_w L(w,\\alpha,\\beta) \\le \\min_w \\max_{\\alpha,\\beta:\\alpha_i\\ge0} L(w,\\alpha,\\beta) = p^*$$\n",
    "\n",
    "记$p^*-d^*$为duality gap.\n",
    "\n",
    "#### b. 强对偶\n",
    "\n",
    "而在一些*特定条件*下，该不等式取等，即\n",
    "$$d^* = p^*$$\n",
    "在这些条件下，我们就可以通过求解对偶问题来求解原始问题。\n",
    "\n",
    "##### b-1 强对偶条件（1）: Slater's condition （充要条件）\n",
    "\n",
    "- $f, g_i$是凸函数\n",
    "- $h_i$是仿射函数\n",
    "  - 仿射函数: $h_i(w) = a_i^Tw + b_i$\n",
    "- $g_i$是strictly feasible的\n",
    "  - strictly feasible: $\\exist w, \\textit{ s.t. } \\forall i, g_i(w) < 0$\n",
    "\n",
    "此时，则$\\exist w^*, \\alpha^*, \\beta^*$，使得 $w^*$是primal problem的解，$\\alpha^*, \\beta^*$是dual problem的解，且$p^* = d^*=\\mathcal{L}(w^*,\\alpha^*,\\beta^*)$。\n",
    "\n",
    "##### b-2 强对偶条件（2）: KKT条件（充要条件）必要条件\n",
    "\n",
    "此外，还有KKT条件，即\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial}{\\partial w_i} \\mathcal{L}\\left(w^*, \\alpha^*, \\beta^*\\right) & =0, \\quad i=1, \\ldots, n \\\\\n",
    "\\frac{\\partial}{\\partial \\beta_i} \\mathcal{L}\\left(w^*, \\alpha^*, \\beta^*\\right) & =0, \\quad i=1, \\ldots, l \\\\\n",
    "\\alpha_i^* g_i\\left(w^*\\right) & =0, \\quad i=1, \\ldots, k \\\\\n",
    "g_i\\left(w^*\\right) & \\leq 0, \\quad i=1, \\ldots, k \\\\\n",
    "\\alpha^* & \\geq 0, \\quad i=1, \\ldots, k\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "其中，称等式（3）为***dual complementarity***条件，其含义为：若$\\alpha_i^*>0$，则$g_i(w^*)=0$. \n",
    "\n",
    "通过后面的描述可见，SVM只需要少量的*support vectors*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.6 Optimal Margin Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回忆，在*Optimal Margin Classifiers*中，我们考虑如下优化问题：\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\min_{\\gamma, w, b} \\quad & \\frac{1}{2} \\|w\\|^2 \\\\\n",
    "\\text{s.t.} \\quad & g_i(w) = -y^{(i)}(w^Tx^{(i)} + b) + 1 \\leq 0, \\quad i = 1, \\ldots, m\n",
    "\\end{aligned}$$\n",
    "\n",
    "由上文KKT条件中的 *(3)dual complementarity* 条件可知，只有那些满足 $g_i(w) = 0$ 的样本才会对最优解有贡献（贡献体现在$a_i>0$）。而这里的 $g_i(w)=-y^{(i)}(w^Tx^{(i)} + b) + 1$表示恰好落在margin上的样本，即支持向量。\n",
    "\n",
    "具体参见下图：\n",
    "\n",
    "![](https://michael-1313341240.cos.ap-shanghai.myqcloud.com/202308151603962.png)\n",
    "\n",
    "图中实线表示的是决策边界，虚线表示的是margin。我们可以看到，只有在虚线上的一个x和两个o样本是对边际有贡献的，称为***support vectors***\n",
    "\n",
    "$\\diamond$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们由此往下，尝试通过对偶问题解决原问题。\n",
    "\n",
    "首先引入向量内积符号：$ \\langle x, y \\rangle = x^T y $\n",
    "\n",
    "我们目前需要解决的optimal margin classifier的优化问题是：\n",
    "$$ \\begin{align*}\n",
    "\\mathcal{L}(w,b,\\alpha) = \\frac12 ||w||^2 - \\sum_{i=1}^m \\alpha_i \\left( y^{(i)} (w^T x^{(i)} + b) - 1 \\right)\n",
    "\\end{align*} \\quad [\\dagger] $$\n",
    "\n",
    "回忆：\n",
    "$$\\theta_{\\mathcal{D}}(\\alpha,\\beta) = \\min_{w} \\mathcal{L}(w,\\alpha,\\beta) ,\\alpha\\ge0 $$ \n",
    "\n",
    "因此我们分别对$w$和$b$求偏导，令其为0，得到：\n",
    "$$ \\begin{align}\n",
    "\\nabla_w \\mathcal{L}(w,b,\\alpha) &= w - \\sum_{i=1}^m \\alpha_i y^{(i)} x^{(i)} = 0 \\\\\n",
    "\\nabla_b \\mathcal{L}(w,b,\\alpha) &= \\sum_{i=1}^m \\alpha_i y^{(i)} = 0\n",
    "\\end{align} $$\n",
    "\n",
    "由(1) 解得：\n",
    "$$w = \\sum_{i=1}^m \\alpha_i y^{(i)} x^{(i)} \\quad [\\ast]$$\n",
    "\n",
    "直观的理解，这里的含义是指参数$w$一定是原始的训练集的线性组合。一个直觉的理解可以回忆线性回归中$\\hat\\beta = (X^T X)^{-1} X^T y$\n",
    "\n",
    "将这里的$w$代入$[\\dagger]$并化简，得到\n",
    "$$\\begin{align*}\n",
    "\\mathcal{L}(w,b,\\alpha) &= \\sum_{i=1}^m \\alpha_i - \\frac12 \\sum_{i,j=1}^m \\alpha_i \\alpha_j y^{(i)} y^{(j)} \\langle x^{(i)}, x^{(j)} \\rangle - b\\sum_{i=1}^m \\alpha_i y^{(i)} \\\\ \n",
    "&= \\sum_{i=1}^m \\alpha_i - \\frac12 \\sum_{i,j=1}^m \\alpha_i \\alpha_j y^{(i)} y^{(j)} \\langle x^{(i)}, x^{(j)} \\rangle  \\textit{ (by eqn. (2)) } \\\\\n",
    "\\end{align*}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最终可以得到如下的对偶问题 **(Dual Optimizatoin Problem of SVM)**：\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\max_{\\alpha} \\quad & W(\\alpha) = \\sum_{i=1}^m \\alpha_i - \\frac{1}{2} \\sum_{i,j=1}^m y^{(i)} y^{(j)} \\alpha_i \\alpha_j \\langle x^{(i)}, x^{(j)} \\rangle \\\\\n",
    "\\text{s.t.} \\quad & \\alpha_i \\geq 0, \\quad i = 1, \\ldots, m \\\\\n",
    "& \\sum_{i=1}^m \\alpha_i y^{(i)} = 0\n",
    "\\end{align*}$$\n",
    "\n",
    "可验证，本对偶问题符合KKT条件。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**预测过程**\n",
    "1. 求解 $a_i, b$\n",
    "2. 预测$h_{w,b}(x)=g(w^Tx+b) = g((\\sum_i a_i y^{(i)} x^{(i)})^Tx+b)=g(\\sum_i\\alpha_iy^{(i)}\\langle x^{(i)},x\\rangle+b)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.7 Kernels 核方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel Methods 思路\n",
    "\n",
    "**(1) 将算法写作含$\\langle x^{(i)},x^{(j)}\\rangle \\textit{( or note as } \\langle x,z\\rangle \\textit{) }$的形式**\n",
    "\n",
    "**(2) 映射 $x_\\textit{ (low dim)} \\rightarrow \\phi(x) _\\textit{ (high dim)}$**\n",
    "\n",
    "**(3) 寻找容易可行的计算$K(x,z) = \\phi(x)^T\\phi(z)$的方法**\n",
    "\n",
    "**(4) 讲上述算法中的$\\langle x,z\\rangle$替换为$K(x,z)$**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 例子"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设$x,z\\in\\mathbb{R}^n$\n",
    "\n",
    "在本例中，假设有三个特征$x_1,...,x_3$，以及映射函数：\n",
    "$$ \\phi(x) = [x_1x_1, x_1x_2,x_1x_3,...,x_3x_3]^T $$\n",
    "需要指出，正常直接计算$\\phi(x)$ （或$\\phi(x)^T\\phi(z)$）需要$O(n^2)$的时间\n",
    "\n",
    "$\\diamond$\n",
    "\n",
    "为化简计算，考虑如下核方法：\n",
    "$$ K(x,z) = \\phi(x)^T\\phi(z) =^{?} (x^Tz)^2 = ... = \\sum_{i,j=1}^n (x_ix_j)(z_iz_j) $$\n",
    "这个核方法的计算复杂度为$O(n)$\n",
    "\n",
    "可以通过单纯的代数运算证明，[?]处等式在给定映射关系$\\phi$后是可以直接推出的。\n",
    "\n",
    "$\\diamond$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "再考虑另外一种核方法：\n",
    "\n",
    "$$\\begin{align*}\n",
    "K(x,z) &= (x^Tz+c)^2\\\\\n",
    "&= \\sum \\limits_{i,j=1}^n (x_ix_j)(z_i z_j) + \\sum \\limits_{i=1}^n (\\sqrt{2c}x_i)(\\sqrt{2c}z_i) + c^2\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5* SVM & Kernel Methods \n",
    "\n",
    "**现代数字信号处理_张颢 / 笔记**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Support Vector Machine (Vapnik, 1960's)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.1 SVM 处理的问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM主要用于处理*分类任务*. \n",
    "\n",
    "具体地，我们有如下训练集：$(x_1,y_1),..., (x_n,y_n)$, 希望能够构造一个函数$f(\\cdot)$，*s.t.* \n",
    "$$f(x_i)=y_i,\\\\~\\textit{  where  } i=1,2,...,n, ~ x_i\\in \\mathbb{R}^n, ~ y_i\\in \\{-1,1\\} $$\n",
    "\n",
    "$\\diamond$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "进一步，为了模型的简便性，我们希望构造的函数是线性的，或至少是仿射的 *(Affine)*，即\n",
    "$$f(x) = w^Tx + b$$\n",
    "\n",
    "可以认为上述$f(x)$构造了一个超平面 *(Hyperplane)* ，由解析几何知识可知，$w$是超平面的法向量，$b$是超平面的截距。\n",
    "\n",
    "$\\diamond$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.2 SVM 的朴素优化目标"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**首先定义点$x$到平面的距离$d(x,f)$**\n",
    "\n",
    "$$d(x,f) = \\frac{1}{\\|w\\|} |w^Tx + b|$$\n",
    "\n",
    "> $\\dagger\\text{ Proof.}$\n",
    "> \n",
    "> 已知$w$是平面上的法向量，$x$是平面上的一点，$x_0$是$x$向平面做垂线的垂足，则有\n",
    "> $$x-x_0 \\perp f \\Rightarrow x-x_0 \\mathop{//} w$$\n",
    ">\n",
    "> 故\n",
    "> \n",
    "> $$|w^T(x-x_0)| =|w^Tx-W^Tx_0|\\\\ = |w^Tx + b| = ||w||\\cdot \\underbrace{||x-x_0||}_{d(x,f)}$$\n",
    "> 命题由上式最后一个等号变形即证\n",
    "> \n",
    "> $\\square$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**再定义点集$A$到超平面的距离$d(A,f)$**\n",
    "$$ d(A,f) = \\min_{x\\in A} d(x,f) $$\n",
    "\n",
    "将这个距离称为***Margin***\n",
    "\n",
    "$\\diamond$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**下开始推导SVM的优化目标**\n",
    "\n",
    "在给出上述定义后，我们给出理想超平面的分类准则，即 *希望得到的超平面对任意类别的集合的距离最大*：\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\max_{w,b}\\left ( \\frac{w^Tx^{(A)}+b}{||w||} + \\frac{w^Tx^{(B)}+b}{||w||} \\right ) \\quad\\quad(1)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "由对称性，我们有$|w^Tx^{(A)}+b| = |w^Tx^{(B)}+b| := a $，故上(1)式可化为：\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\max_{w,b}\\left( \\frac{2a}{||w||} \\right) \\quad\\quad(2)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "观察上式，会发现$a$对于超平面的优化是没有影响的；换言之，我们可以通过坐标变换，在不改变超平面的法向量$w$和截距$b$的情况下，改变$a$的值。\n",
    "\n",
    "因此，我们可以不妨**令$a$归一化为1**. 此时上式变为：\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\max_{w,b}\\left( \\frac{2}{||w||} \\right) \\Leftrightarrow \\min_{w,b}{||w||}  \\quad\\quad(3)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$\\diamond$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "但是我们还需要在优化问题中指示分类的正误与否。具体而言，对于Margin上的两个点$x^{(1)}$和$x^{(-1)}$，我们希望它们被正确分类，即：\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\begin{cases}\n",
    "w^Tx^{(1)}+b = 1 \\\\\n",
    "w^Tx^{(-1)}+b = -1\n",
    "\\end{cases}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "考虑到，Margin上的点已经是该类别的点中的最小值，则其余点的距离都将大于1. 由此整合，可以得到**正确分类时的条件**：\n",
    "\n",
    "$$ \\begin{aligned}\n",
    "(w^Tx_i+b)y_i \\ge 1 \\quad\\quad (4)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$\\diamond$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "结合（3）（4）两式，最终得到**正式的（朴素的）分类优化目标**：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\min_{w,b} \\quad & ||w|| \\\\\n",
    "\\textit{s.t.} \\quad & y^{(i)}(w^Tx^{(i)} + b) \\geq 1, \\quad i = 1, \\ldots, n\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$\\diamond$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.3 SVM 优化目标的数学推导"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了方便，继续对上述优化目标进行变形.\n",
    "\n",
    "首先该优化目标等价于：\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\min_{w,b} \\quad & \\frac12 ||w||^2 \\\\\n",
    "\\textit{s.t.} \\quad & y^{(i)}(w^Tx^{(i)} + b) \\geq 1, \\quad i = 1, \\ldots, n\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$\\diamond$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为解决这一优化问题，引入***Lagrange Multiplier***，希望优化目标Lagrange函数为：\n",
    "$$\n",
    "\\begin{aligned}\n",
    "L^*(w,b,\\boldsymbol{\\lambda}) = \\frac{1}{2}||w||^2 + \\sum_{i=1}^m {\\lambda}_i \\underbrace{\\left [- y^{(i)}(w^Tx^{(i)}+b)+ 1\\right ]}_{\\textit{Penalty}} \\quad \\ast\n",
    "\\end{aligned}\n",
    "$$\n",
    "其中，记$\\boldsymbol{\\lambda} = [\\lambda_1, \\lambda_2, \\cdots, \\lambda_m]^T$. 此外，注意式中*Penalty*项中的符号方向.\n",
    "\n",
    "$\\diamond$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "但是$*$中的优化函数存在问题：\n",
    "\n",
    "- 考虑数据集中远离超平面的数据点，会发现这些点会使得*Penalty*项很负，从而极大地影响了优化的结果\n",
    "- 然而，事实上超平面的位置并不应该被这些临近点所影响\n",
    "- ideally，我们只需要优化函数指示分类是否是大于1的即可，对于大于1的case（即不在margin上的点），我们并不关心它们的具体位置。\n",
    "\n",
    "因此得到修改后的**最终的Lagrange优化函数：**\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "L(w,b,\\boldsymbol{\\lambda}) &= \\frac{1}{2}||w||^2 + \\sum_{i=1}^n \\lambda_i \\cdot \\underbrace{\\mathcal{Hinge}\\left [ 1 - y^{(i)}(w^Tx^{(i)} + b)\\right]  }_{\\textit{Nonlinear}}\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "其中引入截断函数:\n",
    "$$ \\mathcal{Hinge}(z) = \\max(0, z) $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Hinge Function Plot\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "x = np.linspace(-2, 2, 100)\n",
    "y = np.maximum(0, x)\n",
    "plt.plot(x, y)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Hinge Function')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.4 SVM 优化目标的高级凸优化处理变形"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 处理1：Squeezing / Relaxing （去除非线性部分）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先，继续整理上述优化内容，得到：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    " [\\dagger^1]:\\quad \n",
    "\\min_{w,b,z} \\quad & \\frac{1}{2} ||w||^2 + \\sum_{i=1}^n z_i \\\\\n",
    "\\textit{s.t.} \\quad &  z_i \\geq 1 - y^{(i)} (w^T x^{(i)} + b), \\quad i = 1, \\ldots, n \\quad\\quad \\\\\n",
    "& z_i \\geq 0, \\quad i = 1, \\ldots, n\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "下给出两个*intuition*试图对上述内容进行直观上的解释."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### \\# Intuition 1 - *Squeezing*\n",
    "\n",
    "- 上述优化内容中，$\\frac12||w||^2$表示对是需要优化的margin，后面$\\sum z_i$是penalty.\n",
    "\n",
    "- 由于$\\sum z_i$ 的加入，我们在优化中我们希望这一项尽可能的小（但至少要大于等于0）\n",
    "  - 而我们又可以近似将$z_i$看作是$1-y_i(w^Tx_i+b)$的*inf*\n",
    "  - 因此在不断缩小$z_i$的同时，我们就在不断缩小 *(squeezing)*  $1-y_iw^Tx_i$，也就是在不断增大margin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### \\# Intuition 2 - *Relaxing*\n",
    "\n",
    "首先有下列事实：\n",
    "\n",
    "$$\\begin{aligned}\n",
    "& z_i \\ge 1-y_i\\cdot (w^Tx_i+b) \\ge 0 \n",
    "\\\\ \\Leftrightarrow ~& \\quad y_i\\cdot(w^Tx_i+b) \\ge 1-z_i\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "而这一事实可以理解为：\n",
    "\n",
    "- 我们可以放宽原先的 $y_i(w^Tx_i+b) \\ge 1$ 约束，允许一些误分类的点存在；即将原约束放宽为：$ y_i(w^Tx_i+b) \\ge 1-z_i$，其中 $z_i \\ge 0$\n",
    "- 接着我们希望误分类的情况尽可能少的出现，即最小化 $\\sum_{i=1}^m z_i$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ***Squeezing* 和 *Relaxing* 都是在优化中常见的处理非线性等复杂情况等手段。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 处理2：Lagrange Duality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Lagrange Duality的一般原理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一般地，有下列约束问题：\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\min_{x} \\quad & f_0(x) \\\\\n",
    "\\text{s.t.} \\quad & g_i(x) \\leq 0, \\quad i = 1, \\ldots, n \\\\\n",
    "& h_i(x) = 0, \\quad i = 1, \\ldots, m\n",
    "\\end{aligned}$$\n",
    "\n",
    "得到对应的拉格朗日函数：\n",
    "\n",
    "$$ \\begin{aligned}\n",
    "L(x, \\lambda, \\mu) &= f_0(x) + \\sum_{i=1}^n \\lambda_i g_i(x) + \\sum_{i=1}^m \\mu_i h_i(x) \n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$\\diamond$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "为了解决上述优化问题，可以将Lagrange函数处理成Inner和Outer两个优化问题：\n",
    "\n",
    "***Inner Optimization Problem ( 对$L$关于$x$进行优化 )***:\n",
    "\n",
    "$$x^*_{\\lambda,\\mu} = \\arg\\min_{x\\in D} L(x,\\lambda,\\mu)$$\n",
    "\n",
    "***Outer Optimization Problem ( 对$L$关于$\\lambda,\\mu$进行优化 )***:\n",
    "\n",
    "代入上面已经得到的最优化$x^*$,进行优化有\n",
    "\n",
    "$$\\lambda^*,\\mu^* = \\arg\\max_{\\lambda,\\mu} L(x^*_{\\lambda,\\mu},\\lambda,\\mu)$$\n",
    "\n",
    "**这是一个*Minimax*的策略**，下对其合理性给出理解：\n",
    "\n",
    "> $\\dagger \\text{ Explanation.}$\n",
    "> \n",
    "> 首先当$x\\in D$即处于可行域中时，我们一定恒有\n",
    "> $$f_0(x)\\ge f_0(x) +\\underbrace{  \\sum \\lambda_i g_i}_{g_i\\le 0} + \\underbrace{ \\sum \\mu_i h_i}_{h_i=0} \\quad\\quad \\text{(1)}$$\n",
    ">\n",
    "> 而LHS就是我们的*Lagrange Function*，即$f_0(x)\\ge L$，由此得到了$L$的一个*sup*.\n",
    ">\n",
    "> 由此我们推出，定有：\n",
    ">\n",
    "> $$f_0(x) \\ge \\min_{x\\in D} L(x, \\lambda, \\mu) ,\\quad \\forall x\\in D \\quad\\quad \\text{(2)}$$\n",
    ">\n",
    "> 进一步，亦有：\n",
    ">\n",
    "> $$\\min_{x\\in D} f_0(x) \\ge \\min_{x\\in D} L(x, \\lambda, \\mu) \\quad\\quad \\text{(3)}$$\n",
    ">\n",
    "> 当控制住了$x$的取值时，再改变$\\lambda,\\mu$将不会再改变(3)中的符号方向，故有\n",
    ">\n",
    "> $$\\min_{x\\in D} f_0(x) \\ge \\max_{\\lambda,\\mu}\\min_{x\\in D} L(x, \\lambda, \\mu) \\quad\\quad \\text{(4)}$$\n",
    ">\n",
    "> 而上式中对$\\lambda,\\mu$取$\\max$的原因是，$f_0(x)$是我们的优化目标，在满足约束条件的前提下，我们希望最终优化的结果可以尽可能的接近$f_0(x)$.\n",
    ">\n",
    "\n",
    "\n",
    "$\\diamond$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "说明：\n",
    "- 在满足一定条件下，(4)中的不等号将严格取等，即对于Lagrange Function的 Minimax策略最终完全逼近其*sup*\n",
    "- 我们称该条件为***Slater's Condition***（具体条件内容略）\n",
    "- 对于能够严格取等的情况，称之为*Strong Duality*，反之则为*Weak Duality*.\n",
    "\n",
    "$\\diamond$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "进一步处理，首先恒有如下不等式（即全局最小值定不大于局部最小）：\n",
    "\n",
    "$$f_0(x) \\ge \\min_{x\\in D}L(x,\\lambda,\\mu) \\ge \\min_{x}L(x,\\lambda,\\mu) $$\n",
    "\n",
    "代入到上述(3)(4)中，就可以将约束优化转化为无约束优化：\n",
    "$$\\min_{x\\in D} f_0(x) \\ge \\min_{x} L(x, \\lambda, \\mu) \\quad\\quad \\text{(3*)}\n",
    "\\\\\n",
    "\\min_{x\\in D} f_0(x) \\ge \\max_{\\lambda,\\mu}\\min_{x} L(x, \\lambda, \\mu) \\quad\\quad \\text{(4*)}\n",
    "$$\n",
    "\n",
    "可以证明，对于满足*Slater's Condition*的情况，等号依然可以严格取到。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 一个例子"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "考虑如下的优化问题：*寻找一个经过$x$的平面使得经过远点的距离是最小的，即*\n",
    "$$\\begin{aligned}\n",
    "\\min_{w,b} \\quad & \\frac12||w||^2 \\\\\n",
    "\\text{s.t.} \\quad & w^Tx+b=0\n",
    "\\end{aligned}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $\\dagger$ **Solution**\n",
    ">\n",
    "> *构造Lagrange Function:*\n",
    ">\n",
    "> $$ L(w,b,\\lambda) = \\frac{1}{2}||w||^2 + \\lambda(w^Tx+b)$$\n",
    "> \n",
    "> *再构造对偶问题：*\n",
    ">\n",
    "> - *Inner Optimization*\n",
    ">\n",
    "> $$ \\nabla_w L(w,b,\\lambda) = w + \\lambda x = 0 \\\\ \\Rightarrow w^* = -\\lambda x$$\n",
    "> \n",
    "> - *Outer Optimization*\n",
    ">  \n",
    "> $$ L_{\\omega^*}(\\lambda) = -\\frac12(x^Tx)\\lambda^2+\\lambda b \\\\ \\Rightarrow \\lambda^* = \\arg\\min_\\lambda L_{\\omega^*}(\\lambda)  $$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**评价**\n",
    "\n",
    "上例很好地体现了Lagrange Dual Problem的优势：其有效地将原问题转化为了Inner和Outer两个相对更好解决的问题：\n",
    "- Inner Problem由前面的证明已知，是一个无约束问题\n",
    "- Outer Problem 在代入Inner Problem之后得到了一个更为简单的优化问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Lagrange Duality 处理 SVM 的优化问题"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
