{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a1368f0",
   "metadata": {},
   "source": [
    "# 作业5：RNN 生成模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997082ed",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "以 `data/names.txt` 中的英文名作为训练集，利用 RNN 或 LSTM 等方法对字母序列数据进行建模，然后使用拟合的模型随机生成20个名字。本次作业为开放式，不指定各类超参数（如网络结构、学习率、迭代次数等），但需提供必要的输出和诊断结果支持你的选择（如模型是否收敛、效果评价等）。\n",
    "\n",
    "提示：可以参照 `lec12-rnn-generation.zip` 中的代码，但注意英文名不需要像中文那样构建字典，因为可以直接使用26个字母作为字典。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 处理数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-04T11:42:04.645497700Z",
     "start_time": "2023-12-04T11:42:00.191807700Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import itertools\n",
    "import collections\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-04T11:42:08.783637Z",
     "start_time": "2023-12-04T11:42:08.623557100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abbas', 'abbey', 'abbott', 'abdi', 'abel']\n"
     ]
    }
   ],
   "source": [
    "# load txt file\n",
    "\n",
    "def read_txt_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        content = file.read().split()\n",
    "        return content\n",
    "\n",
    "dat = read_txt_file('data/names.txt')\n",
    "print(dat[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-04T11:42:23.907980700Z",
     "start_time": "2023-12-04T11:42:23.892846700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '<EOS>']\n"
     ]
    }
   ],
   "source": [
    "# construct dictionary\n",
    "charset_size = 27 # 26 letters  + 1 <EOS>\n",
    "dictionary = list('abcdefghijklmnopqrstuvwxyz') + ['<EOS>'] \n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-04T11:42:26.701001Z",
     "start_time": "2023-12-04T11:42:26.639429500Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "          [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]]),\n",
       " array([4, 6]),\n",
       " tensor([[ 4,  0],\n",
       "         [14,  2],\n",
       "         [13,  7],\n",
       "         [26,  4],\n",
       "         [26, 11],\n",
       "         [26, 26]]))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# names to tensor\n",
    "def char2index(char):\n",
    "    \"\"\"Transform a character to its index in the dictionary\n",
    "    Args:\n",
    "        char (str): a character\n",
    "        \n",
    "    Returns:\n",
    "       int: the index of the character in the dictionary\n",
    "        \n",
    "    \"\"\"\n",
    "    return dictionary.index(char.lower()) \n",
    "\n",
    "def names2tensor(NameList):\n",
    "    \"\"\"Transform a list of names to one-hot tensor\n",
    "    Args:\n",
    "        NameList (array): a list of names\n",
    "        \n",
    "    Returns:\n",
    "        tensor: a tensor of shape (LongestNameLength, NumberOfNames, charset_size=27), storing the one-hot representation of names\n",
    "        array: a numpy array of shape (NumberOfNames), storing each name's length\n",
    "        target: a tensor of shape (LongestNameLength, NumberOfNames), storing the index of the next letter\n",
    "        \n",
    "    \"\"\"\n",
    "    names_num = len(NameList) # number of names\n",
    "    names_lens = [len(name) for name in NameList] # a list storing each name's length\n",
    "    max_name_len = max(names_lens) # the longest name's length\n",
    "    \n",
    "    tensor = torch.zeros(max_name_len, names_num, charset_size) # (each char in a name, each name, one-hot vector)\n",
    "    target = torch.zeros(max_name_len, names_num, dtype=int) + charset_size - 1 # initialize with <EOS>\n",
    "    \n",
    "    for name_i in range(names_num): # for each name(idx) in data set\n",
    "        name = NameList[name_i] # get the name\n",
    "        for char_i in range(names_lens[name_i]): # for each char(idx) in the name\n",
    "            # set tensor\n",
    "            tensor[char_i, name_i, char2index(name[char_i])] = 1 # set the corresponding one-hot vector\n",
    "            # set target\n",
    "            if char_i < names_lens[name_i] - 1: # if not the last char (here note that python index starts from 0)\n",
    "                target[char_i, name_i] = char2index(name[char_i + 1]) # target for name_i, char_i is char_i+1\n",
    "                \n",
    "    return tensor, np.array(names_lens), target\n",
    "\n",
    "# test names2tensor\n",
    "names2tensor([\"leon\",\"rachel\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 构建LSTM模型类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "![](https://michael-1313341240.cos.ap-shanghai.myqcloud.com/202312042025464.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters Definition:\n",
    "\n",
    "- Suppose there are $h$ hidden units, batch size is $n$, number of inputs is $d$. Thus, $X_t \\in \\mathbb{R}^{n\\times h}, H_t = \\mathbb{R}^{n\\times h}.$\n",
    "- Define Gates (at time $t$):\n",
    "    - Input gate ($I_t \\in \\mathbb{R}^{n\\times h}$) : $I_t = \\text{sigmoid}(X_t W_{xi} + H_{t-1} W_{hi} + b_i)$\n",
    "    - Forget gate ($F_t \\in \\mathbb{R}^{n\\times h}$) : $F_t = \\text{sigmoid}({X_t W_{xf} + H_{t-1} W_{hf} + b_f)}$\n",
    "    - Output gate ($O_t \\in  \\mathbb{R}^{n\\times h}$) : $O_t = \\text{sigmoid}{X_t W_{xo} + H_{t-1} W_{ho} + b_o)}$\n",
    "  \n",
    "  where $W_{x,\\cdot} \\in \\mathbb{R}^{d\\times h}$, $W_{h,\\cdot} \\in \\mathbb{R}^{h\\times h}$, $b_{\\cdot} \\in \\mathbb{R}^{1\\times h}$.\n",
    "\n",
    "- Define Candidate Memory Cell $\\tilde C$:\n",
    "\n",
    "$$\n",
    "  \\tilde C_t = \\text{tanh}(X_t W_{xc} + H_{t-1} W_{hc} + b_c)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "> *Reference*\n",
    ">\n",
    "> *1. Dive Into Deep Learning (https://zh.d2l.ai/chapter_recurrent-modern/lstm.html)*\n",
    ">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LSTM\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(LSTM,self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size #?\n",
    "        \n",
    "        self.X2F = nn.Linear(in_features = input_size + hidden_size,\n",
    "                             out_features = hidden_size )\n",
    "        self.X2I = nn.Linear(in_features = input_size + hidden_size,\n",
    "                             out_features = hidden_size )\n",
    "        self.X2O = nn.Linear(in_features = input_size + hidden_size,\n",
    "                             out_features = hidden_size )\n",
    "        \n",
    "        self.X2Ct = nn.Linear(in_features = input_size + hidden_size,\n",
    "                             out_features = hidden_size )\n",
    "        \n",
    "        self.O2O = nn.Linear(in_features = hidden_size,\n",
    "                             out_features = hidden_size)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.logsoftmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, input, hidden, MemCell):\n",
    "        \n",
    "        input_combined = torch.cat((input,hidden),1)     #? how\n",
    "        \n",
    "        ForgetGate = torch.sigmoid(self.X2F(input_combined))   \n",
    "        InputGate = torch.sigmoid(self.X2I(input_combined))\n",
    "        OutputGate = torch.sigmoid(self.X2O(input_combined))\n",
    "        \n",
    "        CandidateMemCell = torch.tanh(self.X2Ct(input_combined))\n",
    "        \n",
    "        MemCell = ForgetGate * MemCell + InputGate * CandidateMemCell\n",
    "        \n",
    "        hidden = OutputGate * torch.tanh(MemCell)\n",
    "        print(f\"hidden:{hidden.shape}\")\n",
    "        \n",
    "        output = self.O2O(hidden)\n",
    "        output = self.dropout(output)\n",
    "        output = self.logsoftmax(output)\n",
    "        \n",
    "        return output, hidden, MemCell\n",
    "    \n",
    "    def init_hidden(self, batch_size, device):\n",
    "        return torch.zeros((batch_size, self.hidden_size), device = device)\n",
    "\n",
    "    def init_MemCell(self, batch_size, device):\n",
    "        return torch.zeros((batch_size, self.hidden_size), device = device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Test Demo*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden = 64\n",
    "lstm = LSTM(charset_size, n_hidden)\n",
    "def name2tensor(name):\n",
    "    \"\"\"将名字转换为 one-hot 编码的张量\"\"\"\n",
    "    tensor = torch.zeros(len(name), 1, charset_size) #一个tensor（其实是2d矩阵的感觉），第一个维度是名字的长度（名字中的各个字符），第二个维度是1，第三个维度是每个字符的onehot\n",
    "    for i, char in enumerate(name): #enmuerate\n",
    "        tensor[i, 0, char2index(char)] = 1\n",
    "    return tensor\n",
    "\n",
    "input = name2tensor(\"leon\")\n",
    "hidden = lstm.init_hidden(batch_size=1, device='cpu')\n",
    "MemCell = lstm.init_MemCell(batch_size=1, device='cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden:torch.Size([1, 64])\n",
      "tensor([[-4.1763, -4.1556, -4.1706, -4.0239, -4.0729, -4.0976, -4.1486, -4.0253,\n",
      "         -4.1109, -4.2063, -4.1630, -4.2120, -4.0853, -4.1391, -4.1411, -4.1772,\n",
      "         -4.2263, -4.1145, -4.1622, -4.1795, -4.1732, -4.0293, -4.2042, -4.1001,\n",
      "         -4.1556, -4.1367, -4.1838, -4.1063, -4.1650, -4.2221, -4.1949, -4.1355,\n",
      "         -4.1426, -4.0684, -4.1765, -4.2133, -4.0348, -4.1360, -4.2442, -4.2703,\n",
      "         -4.2960, -4.2445, -4.2595, -4.0416, -4.2614, -4.1556, -4.1200, -4.1157,\n",
      "         -4.2865, -4.1889, -4.1684, -4.2901, -4.2269, -4.1797, -4.0315, -4.1643,\n",
      "         -4.1004, -4.1872, -4.2393, -4.2148, -4.1439, -4.1907, -4.2906, -4.0472]],\n",
      "       grad_fn=<LogSoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "output, next_hidden, next_MemCell  = lstm(input[0], hidden, MemCell)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Build Training*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seed \n",
    "np.random.seed(123)\n",
    "torch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
