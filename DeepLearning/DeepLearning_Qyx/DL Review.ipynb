{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DL 复习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 目录"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Lect 2 线性回归，二分类回归，损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 线性回归"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- 在线性回归中， 如果假定误差是正态，极大似然可以推导出OLS，即MSE准则\n",
    "- 若假定误差是Laplace分布，极大似然可以推导出MAE准则"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 线性二分类\n",
    "\n",
    "$$Y|x \\sim Bernoulli(\\rho(\\beta'x))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- $\\rho(\\beta'x)$把$\\beta'x$映射到$(0,1)$，表示$Y=1$的概率\n",
    "- 当$\\rho(x)=e^x/(e^x+1)$时，称为logistic 回归\n",
    "\n",
    "***Lositics 回归***\n",
    "- 损失函数\n",
    "  $$\\begin{aligned} \n",
    "  L(\\beta)&=n^{-1}\\sum_{i=1}^n l(\\rho(\\beta'x_i),y_i) \n",
    "  \\\\\n",
    "  &= n^{-1}\\sum_{i=1}^n \\left[ y_i\\log \\rho(\\beta'x_i) + (1-y_i)\\log(1-\\rho(\\beta'x_i)) \\right]\n",
    "  \\end{aligned}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistics MLE loss function\n",
    "\n",
    "def loss_fn_logistic(bhat, x, y):\n",
    "    rhohat = sigmoid(torch.matmul(x,bhat))\n",
    "    loss = -torch.sum(y*torch.log(rhohat) + (1-y) * torch.log(1-rhohat))/y.shape[0]\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 其他损失函数\n",
    "\n",
    "- KL 散度\n",
    "- 最小化KL散度等价于MLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Lect 3 Pytorch 基础、线性多分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch 基本操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基本数据操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 5.])\n",
      "tensor([[1.0000, 2.0000, 2.0000],\n",
      "        [3.0000, 5.0000, 4.5000]])\n",
      "torch.Size([3])\n",
      "torch.Size([2, 3])\n",
      "tensor([[1.],\n",
      "        [2.],\n",
      "        [5.]])\n",
      "tensor([[1.0000, 2.0000],\n",
      "        [2.0000, 3.0000],\n",
      "        [5.0000, 4.5000]])\n",
      "tensor([[1.0000, 2.0000, 2.0000],\n",
      "        [3.0000, 5.0000, 4.5000]])\n",
      "tensor([4.0000, 7.0000, 6.5000])\n",
      "tensor([ 5.0000, 12.5000])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 3.0000],\n",
       "        [2.0000, 5.0000],\n",
       "        [2.0000, 4.5000]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# 随机数种子\n",
    "np.random.seed(123456)\n",
    "torch.manual_seed(123456)\n",
    "\n",
    "# 创建向量/矩阵\n",
    "vec = torch.tensor([1.0, 2.0, 5.0])\n",
    "print(vec)\n",
    "mat = torch.tensor([[1.0, 2.0, 2.0], [3.0, 5.0, 4.5]])\n",
    "print(mat)\n",
    "\n",
    "# 特殊向量\n",
    "torch.ones(3, 2)\n",
    "torch.zeros(5)\n",
    "torch.linspace(3, 10, steps=5)\n",
    "torch.randn(2, 3)\n",
    "\n",
    "# 矩阵形状与变形\n",
    "print(vec.shape)\n",
    "print(mat.shape)\n",
    "n = mat.shape[0]\n",
    "p = mat.shape[1]\n",
    "\n",
    "print(vec.view(3, 1))\n",
    "print(mat.view(3, 2))\n",
    "\n",
    "# 汇总\n",
    "print(mat)\n",
    "print(torch.sum(mat, dim=0))\n",
    "print(torch.sum(mat, dim=1))\n",
    "\n",
    "# 矩阵计算\n",
    "torch.matmul(torch.t(mat), mat)\n",
    "mat.t()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "统计分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 1.])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.distributions as D\n",
    "import math\n",
    "\n",
    "# 正态分布【注意，scale传入标准差】\n",
    "## 生成分布形式\n",
    "norm = D.Normal(loc=torch.tensor([1.0]), scale=torch.tensor([math.sqrt(3.0)]))\n",
    "## 求正态分布某点对数密度函数\n",
    "norm.log_prob(torch.tensor([1.0, 2.0, 3.0]))\n",
    "## 生成正态随机数\n",
    "norm.sample(sample_shape=(5,))\n",
    "\n",
    "# Bernoulli分布\n",
    "## 可以同时指定多组Bernoulli分布\n",
    "bern = D.Bernoulli(probs=torch.tensor([0.1, 0.5, 0.9]))\n",
    "bern.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其他常见的操作可以参考[官方教程](https://pytorch.org/tutorials/beginner/basics/tensorqs_tutorial.html)，完整的函数列表可以查看[官方 API 文档](https://pytorch.org/docs/stable/torch.html)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "例Ex：计算 $d$ 维标准正态分布的对数密度函数 $\\log[p_Z(z)]$，其中\n",
    "\n",
    "$$p_{Z}(z)=(2\\pi)^{-d/2}\\exp\\left(-\\frac{1}{2}\\Vert z\\Vert^{2}\\right).$$\n",
    "\n",
    "其中$z \\in \\mathbb{R}^{n\\times d}$, $l \\in \\mathbb{R}^{n\\times l}$, `l` 的第 $i$ 个元素是 $\\log[p_Z(\\cdot)]$ 在 `z` 的第 $i$ 行上的取值\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_normal_pdf(z):\n",
    "    Norm = torch.norm(z,dim=1)\n",
    "    d = z.size()[1]\n",
    "    pz = (2*math.pi)**(-d/2)*torch.exp(-1/2*Norm**2)\n",
    "    l = torch.log(pz)\n",
    "    return l\n",
    "\n",
    "# pytorch: torch.logaddexp(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 线性多分类\n",
    "\n",
    "$$Y|x \\sim Multinomial(\\rho(Wx))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $x_i \\in \\mathbb{R}^p$: 每个obs的数据包括p个特征\n",
    "- $y_i \\in \\mathbb{R}^k$: 每个obs的数据包括k个类别，且此处是one-hot编码，即$y_i$只有一个元素为1，其余为0\n",
    "- $W \\in \\mathbb{R}^{k \\times p}$: 负责将$p$维的特征映射到$k$维的类别\n",
    "- $\\rho(Wx)$: 将$Wx$映射到$(0,1)^k$，表示每个类别的概率\n",
    "\n",
    "***Softmax 回归***\n",
    "- 当取$\\rho(Wx)=\\frac{e^{Wx}}{\\sum_{j=1}^k e^{Wx_j}}$时，称为Softmax回归\n",
    "- 多项函数的对数似然为：$l(p;y) = \\sum_{j=1}^k y_j \\log p_j$, 其中$p = (p_1,\\dots,p_k) = \\rho(Wx)$\n",
    "- 损失函数为：$L(W) = n^{-1} \\sum_{i=1}^n l(\\rho(Wx_i);Y_i)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "线性多分类完整作业习题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 2, 1, 0, 3, 3, 3, 3, 0, 3, 0, 0, 2, 2, 0, 3, 0, 3, 3])\n",
      "torch.Size([200, 4])\n",
      "tensor([[0, 1, 0, 0],\n",
      "        [0, 0, 1, 0],\n",
      "        [0, 0, 1, 0],\n",
      "        [0, 1, 0, 0],\n",
      "        [1, 0, 0, 0],\n",
      "        [0, 0, 0, 1],\n",
      "        [0, 0, 0, 1],\n",
      "        [0, 0, 0, 1],\n",
      "        [0, 0, 0, 1],\n",
      "        [1, 0, 0, 0]])\n",
      "tensor(3.3032)\n",
      "tensor(3.3032)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.distributions as D\n",
    "import math\n",
    "import torch.nn as nn\n",
    "\n",
    "# 模拟生成数据X、标签l，one-hot编码\n",
    "np.random.seed(123456)\n",
    "torch.manual_seed(123456)\n",
    "\n",
    "n = 200  # 样本量\n",
    "p = 10   # 变量数\n",
    "k = 4    # 类别数\n",
    "x = torch.randn(n, p)\n",
    "l = torch.tensor(np.random.choice(range(4), size=n, replace=True), dtype=int)\n",
    "print(l[:20])\n",
    "\n",
    "y = torch.nn.functional.one_hot(l)\n",
    "print(y.shape)\n",
    "print(y[:10])\n",
    "\n",
    "# 模拟矩阵W\n",
    "norm = D.Normal(loc=torch.tensor([1.0]), scale=torch.tensor([math.sqrt(1.0)]))\n",
    "w = norm.sample(sample_shape=(k,p)).view(k,p) #?\n",
    "\n",
    "# p = softmax(Wx)\n",
    "u = torch.matmul(x, w.t())\n",
    "assert u.shape == (n, k), \"u 形状有误\"\n",
    "p  = torch.softmax( u, dim = 1 )\n",
    "assert p.shape == (n, k), \"p 形状有误\"\n",
    "\n",
    "# 编写损失函数\n",
    "def loss_fn_softmax(w, x, y):\n",
    "    rho = torch.softmax(torch.matmul(x,w.t()),dim=1)\n",
    "    l = torch.sum(torch.log(rho)*y)/y.shape[0]\n",
    "    return - l\n",
    "loss2 = loss_fn_softmax(w, x, y)\n",
    "print(loss2)\n",
    "\n",
    "# 与现成函数CrossEntropyLoss()\n",
    "ce_softmax = nn.CrossEntropyLoss()\n",
    "loss1 = ce_softmax(u, l)\n",
    "print(loss1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Lect4 前馈神经网络(数值稳定)、反向传播"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 激活函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://michael-1313341240.cos.ap-shanghai.myqcloud.com/202312261301460.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 数值稳定"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Sigmoid***\n",
    "\n",
    "$\\exp(x)$当$x$过大时发生溢出。可以上下同除以$\\exp(x)$得到：\n",
    "$$ \\sigma(x) = \\frac{1}{1+\\exp(-x)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2457, 0.1365, 0.7361,  ..., 0.5106, 0.5940, 0.5134],\n",
      "        [0.6236, 0.3778, 0.3801,  ..., 0.4742, 0.5296, 0.4320],\n",
      "        [0.7602, 0.8008, 0.7452,  ..., 0.6917, 0.5353, 0.3651],\n",
      "        ...,\n",
      "        [0.5429, 0.1368, 0.7679,  ..., 0.5520, 0.3152, 0.7340],\n",
      "        [0.2388, 0.4695, 0.4343,  ..., 0.0747, 0.6731, 0.3631],\n",
      "        [0.6627, 0.3355, 0.2810,  ..., 0.7246, 0.3699, 0.2867]])\n"
     ]
    }
   ],
   "source": [
    "def sigmoid2(x):\n",
    "    sig  = 1 / ( 1 + torch.exp(-x) )\n",
    "    return sig\n",
    "# PyTorch 自带函数\n",
    "print(torch.sigmoid(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***tanh***\n",
    "\n",
    "当 $x>>0$时，分子$e^x$会发生溢出。可以上下同乘以$\\exp(-x)$得到：\n",
    "$$ \\sigma(x) = \\frac{1-\\exp(-2x)}{1+\\exp(-2x)}$$\n",
    "当 $x<<0$时，分母$e^{-x}$会发生溢出。可以上下同乘以$\\exp(x)$得到：\n",
    "$$ \\sigma(x) = \\frac{\\exp(2x)-1}{\\exp(2x)+1}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.8082, -0.9512,  0.7722,  ...,  0.0423,  0.3633,  0.0537],\n",
      "        [ 0.4658, -0.4611, -0.4535,  ..., -0.1030,  0.1181, -0.2671],\n",
      "        [ 0.8190,  0.8834,  0.7907,  ...,  0.6684,  0.1405, -0.5028],\n",
      "        ...,\n",
      "        [ 0.1704, -0.9510,  0.8326,  ...,  0.2056, -0.6503,  0.7678],\n",
      "        [-0.8207, -0.1216, -0.2582,  ..., -0.9870,  0.6182, -0.5094],\n",
      "        [ 0.5886, -0.5938, -0.7350,  ...,  0.7476, -0.4875, -0.7218]])\n"
     ]
    }
   ],
   "source": [
    "def tanh2(x):\n",
    "    exp = torch.exp( -2 * torch.abs(x) ) # 此行代码灵感借鉴自ds2023s中hw4的参考答案\n",
    "    th = torch.where( x>0 , (1-exp)/(1+exp) , (exp-1)/(exp+1) ) \n",
    "    return th\n",
    "# PyTorch 自带函数\n",
    "print(torch.tanh(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***softplus***\n",
    "\n",
    "当x>>0时，$e^x$会发生溢出。可以有如下变形：\n",
    "$$ \\mathrm{softplus}(x) = \\log(1+e^x) = x + \\log(1+e^{-x})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2819, 0.1468, 1.3322,  ..., 0.7145, 0.9015, 0.7204],\n",
      "        [0.9770, 0.4746, 0.4782,  ..., 0.6428, 0.7542, 0.5656],\n",
      "        [1.4279, 1.6132, 1.3673,  ..., 1.1765, 0.7664, 0.4544],\n",
      "        ...,\n",
      "        [0.7829, 0.1471, 1.4606,  ..., 0.8029, 0.3786, 1.3242],\n",
      "        [0.2729, 0.6339, 0.5697,  ..., 0.0777, 1.1180, 0.4511],\n",
      "        [1.0869, 0.4087, 0.3299,  ..., 1.2897, 0.4618, 0.3379]])\n"
     ]
    }
   ],
   "source": [
    "def softplus2(x):\n",
    "    lg = torch.log( torch.exp( -torch.abs(x)) + 1)\n",
    "    sp = torch.where(x>0, x + lg, lg )\n",
    "    return sp\n",
    "# PyTorch 自带函数\n",
    "print(torch.nn.functional.softplus(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***log sigmoid (logistics loss function)***\n",
    "\n",
    "$$\\log\\hat\\rho = \\log\\frac{e^{X\\beta}}{1+e^{X\\beta}} = X\\beta - \\log(1+e^{X\\beta}) = X\\beta  - \\mathrm{softplus}(X\\beta)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn_logistic(bhat, x, y):\n",
    "    xbhat = torch.matmul(x,bhat)\n",
    "    logrho1 = xbhat - softplus_fn(xbhat)\n",
    "    logrho2 = - softplus_fn (xbhat)\n",
    "    loss = -torch.sum( y*logrho1 + (1-y)* logrho2 ) / y.shape[0]\n",
    "    return loss  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***logaddexp()***\n",
    "\n",
    "逐元素计算 $f(x,y)=\\log(e^x+e^y)$\n",
    "\n",
    "原式等价于 $\\log(e^{x-y}+1)+y  = \\log(e^{y-x}+1)+x$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_add_exp(x, y):\n",
    "    abs_m = torch.abs(x-y)\n",
    "    return torch.log(torch.exp(-abs_m)+1)+ torch.max(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 前馈神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中的$x_1,\\dots,x_p$为输入层，分别输入的是一个obs中的不同的features；不同的obs相当于一次丢进这个网络中\n",
    "\n",
    "每一层的网络都可以看做是一组features，是activated的状态；经过线性映射W^(l)被映射到下一层，归纳成了另外$M_l$个元素，而经过激活函数激活后，这些元素就是下一层的输入\n",
    "\n",
    "![](https://michael-1313341240.cos.ap-shanghai.myqcloud.com/202312222351646.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***通用近似定理***\n",
    "\n",
    "两层的网络几乎可以拟合任意函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 反向传播"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Lect5 反向传播、自动微分\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 向量微分法则"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 对于 $dl / dW$，导数结果的维度与参数$W$的维度保持一致；\n",
    "\n",
    "- 逐元素计算的激活函数的导数也是逐元素计算的；\n",
    "\n",
    "- 对于线性变换$z=Wa+b, W \\in \\mathbb{R}^{n\\times m}, a = (a_1,...,a_m)', z = (z_1,...,z_n)'$，已知$dl/dz$，则\n",
    "$$\n",
    "\\frac{\\mathrm{d}l}{\\mathrm{d}W} = \\frac{\\mathrm{d}l}{\\mathrm{d}z} \\cdot a'\\\\\n",
    "\\frac{\\mathrm{d}l}{\\mathrm{d}b} = \\frac{\\mathrm{d}l}{\\mathrm{d}z}\n",
    "\\\\\n",
    "\\frac{\\mathrm{d}l}{\\mathrm{d}a} = W' \\cdot \\frac{\\mathrm{d}l}{\\mathrm{d}z}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### n行矩阵实操"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ Z_{n\\times p} = A_{n\\times m} W_{m\\times p} + 1_{n} \\times (b_{p\\times1})'$$\n",
    "\n",
    "- $n$ 表示有$n$个样本\n",
    "- $A$ 的第$i$行表示第$i$个obs在上一层的激活神经元\n",
    "- $Z$ 的第$i$行表示第$i$个观测在下一层要被激活的神经元"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 自动微分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**例1：**求$f(x,y)=x\\cdot \\log(x)+\\sin(xy)$在 $(x,y)=(1,2)$ 处对 $x$ 和 $y$ 的偏导。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], requires_grad=True) tensor([2.], requires_grad=True)\n",
      "tensor([0.9093], grad_fn=<AddBackward0>)\n",
      "tensor([0.1677]) tensor([-0.4161])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 创建对象，requires_grad = True 表示要进行求导操作\n",
    "x = torch.tensor([1.0], requires_grad = True)\n",
    "y = torch.tensor([2.0], requires_grad = True)\n",
    "print(x,y)\n",
    "\n",
    "# 进行函数运算\n",
    "f = x * torch.log(x) + torch.sin(x * y)\n",
    "print(f)\n",
    "\n",
    "# backward()进行反向传播，即进行求导！\n",
    "f.backward()\n",
    "\n",
    "# 通过调用` .grad()`即可查看导数\n",
    "print(x.grad,y.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**例2：**(对于向量与矩阵的求导)  $f(x,y)=(x+y)'(x+y)$，其中 $x$ 和 $y$ 为向量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0717,  0.6340, -0.1064,  0.3226, -2.1567]) tensor([-0.0717,  0.6340, -0.1064,  0.3226, -2.1567])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "x = torch.randn(5)\n",
    "y = torch.rand(5)\n",
    "x.requires_grad = True\n",
    "y.requires_grad = True\n",
    "\n",
    "f = (x + y).dot(x + y) \n",
    "f.backward()\n",
    "\n",
    "print(x.grad,y.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**思考题**：给定一个行列式为正的矩阵 $X$，定义 $f(X)=\\log\\det(X)$，其中 $\\det(X)$ 为 $X$ 的行列式。那么 $\\partial f/\\partial X$ 应该是什么？\n",
    "\n",
    "**Answer**\n",
    "\n",
    "对于一个可逆矩阵$X$，有：\n",
    "$$\n",
    "\\frac{\\partial \\det(X)}{\\partial X} = \\det(X) \\cdot (X^{-1})^T\n",
    "$$\n",
    "\n",
    "故\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial X} = \\frac{\\partial \\log\\det(X)}{\\partial \\det(X)} \\cdot \\frac{\\partial \\det(X)}{\\partial X}= \\frac{1}{\\det(X)} \\cdot \\det(X) \\cdot (X^{-1})^T = (X^{-1})^T\n",
    "$$\n",
    "\n",
    "即，函数  $f(X) = \\log\\det(X)$ 相对于 $X$ 的导数是 $X$ 的逆矩阵的转置。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**例ex：**\n",
    "给定矩阵 $X=(x_{ij})$ 和函数 $f(X)=\\log|\\det(Y'Y)|$，其中 $Y=(y_{ij})$, $y_{ij}=\\log(10+\\exp(x_{ij}))$，$\\det(A)$ 表示方阵 $A$ 的行列式，$|\\det(A)|$ 是 $\\det(A)$ 的绝对值。\n",
    "计算 $f(X)$ 关于 $X$ 的导数在 $X=X_0$ 处的取值，其中 $X_0$ 由如下代码给出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(2023)\n",
    "# 生成数据\n",
    "X0 = 100.0 * torch.randn(10, 5)\n",
    "X0.requires_grad = True\n",
    "\n",
    "# 函数主体\n",
    "Y = log_add_exp(torch.log(torch.tensor(10)),X0) # !!\n",
    "YtY = torch.matmul(Y.t(),Y)\n",
    "#print(YtY)\n",
    "detYtY = torch.det(YtY)\n",
    "#print(detYtY)\n",
    "f = torch.log(torch.abs(detYtY))\n",
    "f.backward()\n",
    "#print(X0.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模块化编程（模板）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 定义神经网络class，搭建网络结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "np.random.seed(123456)\n",
    "torch.random.manual_seed(123456)\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, para):\n",
    "        super(MyModel, self).__init__()\n",
    "        \n",
    "        # ...\n",
    "        # 此处创建网络主要blocks\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # ...\n",
    "        # 此处编写网络连接方式\n",
    "        \n",
    "\n",
    "model = MyModel(para= ) #此处具体化网络实例\n",
    "print(list(model.parameters())) # 该命令可以查看网络的训练参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 定义训练超参数、优化方法等"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 迭代次数\n",
    "nepoch = 500\n",
    "# 学习率，即步长\n",
    "learning_rate = 0.01\n",
    "# 记录损失函数值\n",
    "losses = []\n",
    "\n",
    "# 指定优化方法\n",
    "opt = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 开始循环训练主体"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(nepoch):\n",
    "    \n",
    "    # 计算模型\n",
    "    yhat = model(x) #将数据丢入模型，计算yhat\n",
    "    loss = torch.mean(torch.square(y - yhat)) #通过某种方法计算loss\n",
    "    \n",
    "    # 反向传播，优化参数\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    \n",
    "    # 将参数加入列表\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    # 打印损失\n",
    "    if epoch % 50 == 0:\n",
    "        print(f\"iteration {epoch}, loss = {loss.item()}\")\n",
    "\n",
    "# 绘制损失训练图象\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 合集：几个典型网络的实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 前馈神经网络实现XOR\n",
    "\n",
    "(手动实现全链接部分）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "初始化数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[0.0, 0.0],\n",
    "                  [0.0, 1.0],\n",
    "                  [1.0, 0.0],\n",
    "                  [1.0, 1.0]])\n",
    "y = torch.tensor([[0.0],\n",
    "                  [1.0],\n",
    "                  [1.0],\n",
    "                  [0.0]])\n",
    "\n",
    "n = x.shape[0]  # 样本量\n",
    "p = x.shape[1]  # 输入维度\n",
    "d = y.shape[1]  # 输出维度\n",
    "r = 5           # 隐藏层维度\n",
    "\n",
    "import torch.distributions as D\n",
    "norm = D.Normal(loc=torch.tensor([0.0]), scale=torch.tensor(1.0))\n",
    "torch.manual_seed(123456)\n",
    "# 完成此处程序\n",
    "w1 = norm.sample(sample_shape=(p,r)).view(p,r)\n",
    "b1 = norm.sample(sample_shape=(r,)).view(r,1)\n",
    "w2 = norm.sample(sample_shape=(r,d)).view(r,d)\n",
    "b2 = norm.sample(sample_shape=(d,)).view(d,1)\n",
    "\n",
    "w1.requires_grad = True\n",
    "b1.requires_grad = True\n",
    "w2.requires_grad = True\n",
    "b2.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.6505321264266968: 100%|██████████| 200/200 [00:00<00:00, 953.02it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f3c0ddb0940>]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 构建FF模型类\n",
    "class XOR(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(XOR, self).__init__()\n",
    "        self.w1 = nn.Parameter(torch.randn(input_dim, hidden_dim))\n",
    "        self.b1 = nn.Parameter(torch.rand(hidden_dim, 1))\n",
    "        self.w2 = nn.Parameter(torch.randn(hidden_dim, output_dim))\n",
    "        self.b2 = nn.Parameter(torch.rand(output_dim, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        n = x.shape[0]  # 样本量\n",
    "        onevect = torch.ones(size=(n,)).view(n,1)\n",
    "\n",
    "        z1 = torch.matmul(x,self.w1) + torch.matmul(onevect,self.b1.t()) \n",
    "        a1 = softplus2(z1)\n",
    "        z2 = torch.matmul(a1,self.w2) + torch.matmul(onevect,self.b2.t()) \n",
    "        a2 = sigmoid2(z2)\n",
    "\n",
    "        return a2\n",
    "\n",
    "# 实现网络\n",
    "torch.random.manual_seed(123456)\n",
    "\n",
    "model = XOR(input_dim=2, hidden_dim=3, output_dim=1)\n",
    "# print(list(model.parameters()))\n",
    "\n",
    "# 初始化训练\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "nepoch = 200 # 迭代次数\n",
    "learning_rate = 0.1 # 学习率，即步长\n",
    "losses = [] # 记录损失函数值\n",
    "\n",
    "opt = torch.optim.SGD(model.parameters(), lr=learning_rate) # pytorch提供了一个优化器，对于model这个对象里的所有para进行优化\n",
    "\n",
    "# 开始训练\n",
    "pbar = tqdm(range(nepoch))\n",
    "for i in pbar:\n",
    "    a2 = model(x)\n",
    "    loss = -torch.mean(y * torch.log(a2) + (1.0 - y) * torch.log(1.0 - a2))\n",
    "\n",
    "    opt.zero_grad() # 梯度清零\n",
    "    loss.backward() # 反向传播\n",
    "    opt.step() \n",
    "\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    pbar.set_description(\"loss: %s\" % loss.item())\n",
    "\n",
    "# print(a2)\n",
    "\n",
    "# 绘制损失函数训练图\n",
    "import matplotlib.pylab as plt\n",
    "plt.plot(losses)\n",
    "\n",
    "# 查看预测结果\n",
    "# print(a2)\n",
    "# print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
