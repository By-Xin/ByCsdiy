{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DL 复习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 目录"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lect 2 线性回归，二分类回归，损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 线性回归\n",
    "\n",
    "- 在线性回归中， 如果假定误差是正态，极大似然可以推导出OLS，即MSE准则\n",
    "- 若假定误差是Laplace分布，极大似然可以推导出MAE准则"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 线性二分类\n",
    "\n",
    "$$Y|x \\sim Bernoulli(\\rho(\\beta'x))$$\n",
    "\n",
    "- $\\rho(\\beta'x)$把$\\beta'x$映射到$(0,1)$，表示$Y=1$的概率\n",
    "- 当$\\rho(x)=e^x/(e^x+1)$时，称为logistic 回归\n",
    "\n",
    "***Lositics 回归***\n",
    "- 损失函数\n",
    "  $$\\begin{aligned} \n",
    "  L(\\beta)&=n^{-1}\\sum_{i=1}^n l(\\rho(\\beta'x_i),y_i) \n",
    "  \\\\\n",
    "  &= n^{-1}\\sum_{i=1}^n \\left[ y_i\\log \\rho(\\beta'x_i) + (1-y_i)\\log(1-\\rho(\\beta'x_i)) \\right]\n",
    "  \\end{aligned}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistics MLE loss function\n",
    "\n",
    "def loss_fn_logistic(bhat, x, y):\n",
    "    rhohat = sigmoid(torch.matmul(x,bhat))\n",
    "    loss = -torch.sum(y*torch.log(rhohat) + (1-y) * torch.log(1-rhohat))/y.shape[0]\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 其他损失函数\n",
    "\n",
    "- KL 散度\n",
    "- 最小化KL散度等价于MLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lect 3 Pytorch 基础、线性多分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch 基本操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基本数据操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# 随机数种子\n",
    "np.random.seed(123456)\n",
    "torch.manual_seed(123456)\n",
    "\n",
    "# 创建向量/矩阵\n",
    "vec = torch.tensor([1.0, 2.0, 5.0])\n",
    "print(vec)\n",
    "mat = torch.tensor([[1.0, 2.0, 2.0], [3.0, 5.0, 4.5]])\n",
    "print(mat)\n",
    "\n",
    "# 特殊向量\n",
    "torch.ones(3, 2)\n",
    "torch.zeros(5)\n",
    "torch.linspace(3, 10, steps=5)\n",
    "torch.randn(2, 3)\n",
    "\n",
    "# 矩阵形状与变形\n",
    "print(vec.shape)\n",
    "print(mat.shape)\n",
    "n = mat.shape[0]\n",
    "p = mat.shape[1]\n",
    "\n",
    "print(vec.view(3, 1))\n",
    "print(mat.view(3, 2))\n",
    "\n",
    "# 汇总\n",
    "print(mat)\n",
    "print(torch.sum(mat, dim=0))\n",
    "print(torch.sum(mat, dim=1))\n",
    "\n",
    "# 矩阵计算\n",
    "torch.matmul(torch.t(mat), mat)\n",
    "mat.t()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "统计分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.distributions as D\n",
    "import math\n",
    "\n",
    "# 正态分布【注意，scale传入标准差】\n",
    "## 生成分布形式\n",
    "norm = D.Normal(loc=torch.tensor([1.0]), scale=torch.tensor([math.sqrt(3.0)]))\n",
    "## 求正态分布某点对数密度函数\n",
    "norm.log_prob(torch.tensor([1.0, 2.0, 3.0]))\n",
    "## 生成正态随机数\n",
    "norm.sample(sample_shape=(5,))\n",
    "\n",
    "# Bernoulli分布\n",
    "## 可以同时指定多组Bernoulli分布\n",
    "bern = D.Bernoulli(probs=torch.tensor([0.1, 0.5, 0.9]))\n",
    "bern.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其他常见的操作可以参考[官方教程](https://pytorch.org/tutorials/beginner/basics/tensorqs_tutorial.html)，完整的函数列表可以查看[官方 API 文档](https://pytorch.org/docs/stable/torch.html)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 线性多分类\n",
    "\n",
    "$$Y|x \\sim Multinomial(\\rho(Wx))$$\n",
    "\n",
    "- $x_i \\in \\mathbb{R}^p$: 每个obs的数据包括p个特征\n",
    "- $y_i \\in \\mathbb{R}^k$: 每个obs的数据包括k个类别，且此处是one-hot编码，即$y_i$只有一个元素为1，其余为0\n",
    "- $W \\in \\mathbb{R}^{k \\times p}$: 负责将$p$维的特征映射到$k$维的类别\n",
    "- $\\rho(Wx)$: 将$Wx$映射到$(0,1)^k$，表示每个类别的概率\n",
    "\n",
    "***Softmax 回归***\n",
    "- 当取$\\rho(Wx)=\\frac{e^{Wx}}{\\sum_{j=1}^k e^{Wx_j}}$时，称为Softmax回归\n",
    "- 多项函数的对数似然为：$l(p;y) = \\sum_{j=1}^k y_j \\log p_j$, 其中$p = (p_1,\\dots,p_k) = \\rho(Wx)$\n",
    "- 损失函数为：$L(W) = n^{-1} \\sum_{i=1}^n l(\\rho(Wx_i);Y_i)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn_softmax(w, x, y):\n",
    "\n",
    "    rho = torch.softmax(torch.matmul(x,w.t()),dim=1)\n",
    "    l = torch.sum(torch.log(rho)*y)/y.shape[0]\n",
    "    return - l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lect4 前馈神经网络(数值稳定)、反向传播"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 前馈神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***激活函数***\n",
    "\n",
    "详见后面\n",
    "\n",
    "***数值稳定***\n",
    "\n",
    "[Sigmoid]\n",
    "\n",
    "$\\exp(x)$当$x$过大时发生溢出。可以上下同除以$\\exp(x)$得到：\n",
    "$$ \\sigma(x) = \\frac{1}{1+\\exp(-x)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid2(x):\n",
    "    sig  = 1 / ( 1 + torch.exp(-x) )\n",
    "    return sig\n",
    "# PyTorch 自带函数\n",
    "print(torch.sigmoid(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[tanh]\n",
    "\n",
    "当 $x>>0$时，分子$e^x$会发生溢出。可以上下同乘以$\\exp(-x)$得到：\n",
    "$$ \\sigma(x) = \\frac{1-\\exp(-2x)}{1+\\exp(-2x)}$$\n",
    "当 $x<<0$时，分母$e^{-x}$会发生溢出。可以上下同乘以$\\exp(x)$得到：\n",
    "$$ \\sigma(x) = \\frac{\\exp(2x)-1}{\\exp(2x)+1}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh2(x):\n",
    "    exp = torch.exp( -2 * torch.abs(x) ) # 此行代码灵感借鉴自ds2023s中hw4的参考答案\n",
    "    th = torch.where( x>0 , (1-exp)/(1+exp) , (exp-1)/(exp+1) ) \n",
    "    return th\n",
    "# PyTorch 自带函数\n",
    "print(torch.tanh(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[softplus]\n",
    "\n",
    "当x>>0时，$e^x$会发生溢出。可以有如下变形：\n",
    "$$ \\mathrm{softplus}(x) = \\log(1+e^x) = x + \\log(1+e^{-x})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softplus2(x):\n",
    "    lg = torch.log( torch.exp( -torch.abs(x)) + 1)\n",
    "    sp = torch.where(x>0, x + lg, lg )\n",
    "    return sp\n",
    "# PyTorch 自带函数\n",
    "print(torch.nn.functional.softplus(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[log sigmoid (logistics loss function)]\n",
    "\n",
    "$$\\log\\hat\\rho = \\log\\frac{e^{X\\beta}}{1+e^{X\\beta}} = X\\beta - \\log(1+e^{X\\beta}) = X\\beta  - \\mathrm{softplus}(X\\beta)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn_logistic(bhat, x, y):\n",
    "    xbhat = torch.matmul(x,bhat)\n",
    "    logrho1 = xbhat - softplus_fn(xbhat)\n",
    "    logrho2 = - softplus_fn (xbhat)\n",
    "    loss = -torch.sum( y*logrho1 + (1-y)* logrho2 ) / y.shape[0]\n",
    "    return loss  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***前馈网络***\n",
    "\n",
    "其中的$x_1,\\dots,x_p$为输入层，分别输入的是一个obs中的不同的features；不同的obs相当于一次丢进这个网络中\n",
    "\n",
    "每一层的网络都可以看做是一组features，是activated的状态；经过线性映射W^(l)被映射到下一层，归纳成了另外$M_l$个元素，而经过激活函数激活后，这些元素就是下一层的输入\n",
    "\n",
    "![](https://michael-1313341240.cos.ap-shanghai.myqcloud.com/202312222351646.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***通用近似定理***\n",
    "\n",
    "两层的网络几乎可以拟合任意函数"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
