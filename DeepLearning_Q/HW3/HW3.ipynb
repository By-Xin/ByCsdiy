{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 作业3：前馈神经网络+自动微分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 数值稳定的算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在编写激活函数或计算损失函数时，经常会遇到一些极端的取值，如果不对其进行适当的处理，很可能导致计算结果出现 `NaN` 或其他异常结果，影响程序的正常运行。本题将着重练习若干数值稳定的计算方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第1题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) 考虑 Sigmoid 函数 $$\\sigma(x)=\\frac{e^x}{1+e^x}$$\n",
    "\n",
    "请利用 PyTorch 编写一个函数 `sigmoid(x)`，令其可以接收一个 Tensor `x`，返回 Sigmoid 函数在 `x` 上的取值。不可直接调用 `torch.sigmoid()`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def sigmoid(x):\n",
    "    sig  = torch.exp(x) / ( 1 + torch.exp(x) )\n",
    "    return sig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一个简单的测试："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0000e+00, 0.0000e+00, 4.5398e-05, 5.0000e-01, 9.9995e-01, 1.0000e+00,\n",
      "        1.0000e+00])\n",
      "tensor([0.0000e+00, 3.7835e-44, 4.5398e-05, 5.0000e-01, 9.9995e-01,        nan,\n",
      "               nan])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([-1000.0, -100.0, -10.0, 0.0, 10.0, 100.0, 1000.0])\n",
    "\n",
    "# PyTorch 自带函数\n",
    "print(torch.sigmoid(x))\n",
    "\n",
    "# 上面编写的函数\n",
    "print(sigmoid(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) 如果出现异常取值，思考可能的原因是什么。（提示：Sigmoid 函数真实的取值范围是多少？分子和分母的取值范围又是什么？是否可以对 Sigmoid 函数的表达式进行某种等价变换？）请再次尝试编写 Sigmoid 函数。如果一切正常，可忽略此问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\exp(x)$当$x$过大时发生溢出。可以上下同除以$\\exp(x)$得到：\n",
    "$$ \\sigma(x) = \\frac{1}{1+\\exp(-x)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0000e+00, 0.0000e+00, 4.5398e-05, 5.0000e-01, 9.9995e-01, 1.0000e+00,\n",
      "        1.0000e+00])\n",
      "tensor([0.0000e+00, 0.0000e+00, 4.5398e-05, 5.0000e-01, 9.9995e-01, 1.0000e+00,\n",
      "        1.0000e+00])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def sigmoid2(x):\n",
    "    sig  = 1 / ( 1 + torch.exp(-x) )\n",
    "    return sig\n",
    "\n",
    "x = torch.tensor([-1000.0, -100.0, -10.0, 0.0, 10.0, 100.0, 1000.0])\n",
    "\n",
    "# PyTorch 自带函数\n",
    "print(torch.sigmoid(x))\n",
    "\n",
    "# 上面编写的函数\n",
    "print(sigmoid2(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第2题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) 考虑 Tanh 函数 $$\\sigma(x)=\\frac{e^x-e^{-x}}{e^x+e^{-x}}$$\n",
    "\n",
    "请利用 PyTorch 编写一个函数 `tanh(x)`，令其可以接收一个 Tensor `x`，返回 Tanh 函数在 `x` 上的取值。不可直接调用 `torch.tanh()`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def tanh(x):\n",
    "    ex = torch.exp(x)\n",
    "    enx = torch.exp(-x)\n",
    "    th = (ex - enx) / (ex + enx)\n",
    "    return th"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一个简单的测试："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1., -1., -1.,  0.,  1.,  1.,  1.])\n",
      "tensor([nan, nan, -1.,  0.,  1., nan, nan])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([-1000.0, -100.0, -10.0, 0.0, 10.0, 100.0, 1000.0])\n",
    "\n",
    "# PyTorch 自带函数\n",
    "print(torch.tanh(x))\n",
    "\n",
    "# 上面编写的函数\n",
    "print(tanh(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) 如果出现异常取值，思考可能的原因是什么。请再次尝试编写 Tanh 函数。如果一切正常，可忽略此问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当 $x>>0$时，分子$e^x$会发生溢出。可以上下同乘以$\\exp(-x)$得到：\n",
    "$$ \\sigma(x) = \\frac{1-\\exp(-2x)}{1+\\exp(-2x)}$$\n",
    "当 $x<<0$时，分母$e^{-x}$会发生溢出。可以上下同乘以$\\exp(x)$得到：\n",
    "$$ \\sigma(x) = \\frac{\\exp(2x)-1}{\\exp(2x)+1}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0)\n"
     ]
    }
   ],
   "source": [
    "x  = torch.tensor(2)\n",
    "print(torch.where(x>3,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1., -1., -1.,  0.,  1.,  1.,  1.])\n",
      "tensor([-1., -1., -1.,  0.,  1.,  1.,  1.])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def tanh2(x):\n",
    "    exp = torch.exp( -2 * torch.abs(x) ) # 此行代码灵感借鉴自ds2023s中hw4的参考答案\n",
    "    th = torch.where( x>0 , (1-exp)/(1+exp) , (exp-1)/(exp+1) ) \n",
    "    return th\n",
    "\n",
    "x = torch.tensor([-1000.0, -100.0, -10.0, 0.0, 10.0, 100.0, 1000.0])\n",
    "\n",
    "# PyTorch 自带函数\n",
    "print(torch.tanh(x))\n",
    "\n",
    "# 上面编写的函数\n",
    "print(tanh2(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第3题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) 考虑 Softplus 函数 $$\\mathrm{softplus}(x)=\\log(1+e^x)$$\n",
    "\n",
    "请利用 PyTorch 编写一个函数 `softplus(x)`，令其可以接收一个 Tensor `x`，返回 Softplus 函数在 `x` 上的取值。不可直接调用 `torch.nn.functional.softplus()`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def softplus(x):\n",
    "    sp = torch.log( (1 + torch.exp(x) ) )\n",
    "    return sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一个简单的测试："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0000e+00, 3.7835e-44, 4.5399e-05, 6.9315e-01, 1.0000e+01, 1.0000e+02,\n",
      "        1.0000e+03])\n",
      "tensor([0.0000e+00, 0.0000e+00, 4.5418e-05, 6.9315e-01, 1.0000e+01,        inf,\n",
      "               inf])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([-1000.0, -100.0, -10.0, 0.0, 10.0, 100.0, 1000.0])\n",
    "\n",
    "# PyTorch 自带函数\n",
    "print(torch.nn.functional.softplus(x))\n",
    "\n",
    "# 上面编写的函数\n",
    "print(softplus(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) 如果出现异常取值，思考可能的原因是什么。请再次尝试编写 Softplus 函数。如果一切正常，可忽略此问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当x>>0时，$e^x$会发生溢出。可以有如下变形：\n",
    "$$ \\mathrm{softplus}(x) = \\log(1+e^x) = x + \\log(1+e^{-x})$$\n",
    "\n",
    "> *Q: 当x<0时的计算精度可以通过一些简单的方法进行提升吗？*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0000e+00, 3.7835e-44, 4.5399e-05, 6.9315e-01, 1.0000e+01, 1.0000e+02,\n",
      "        1.0000e+03])\n",
      "tensor([0.0000e+00, 0.0000e+00, 4.5418e-05, 6.9315e-01, 1.0000e+01, 1.0000e+02,\n",
      "        1.0000e+03])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def softplus2(x):\n",
    "    lg = torch.log( torch.exp( -torch.abs(x)) + 1)\n",
    "    sp = torch.where(x>0, x + lg, lg )\n",
    "    return sp\n",
    "\n",
    "x = torch.tensor([-1000.0, -100.0, -10.0, 0.0, 10.0, 100.0, 1000.0])\n",
    "\n",
    "# PyTorch 自带函数\n",
    "print(torch.nn.functional.softplus(x))\n",
    "\n",
    "# 上面编写的函数\n",
    "print(softplus2(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第4题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在作业2第2题中，为了计算损失函数，我们通常先计算 $\\hat{\\rho}=\\mathrm{sigmoid}(X\\beta)$，然后再与 $y$ 计算 $l(y,\\hat{\\rho})=-y\\log \\hat{\\rho} - (1-y) \\cdot \\log(1-\\hat{\\rho})$。但当 $\\hat{\\rho}$ 非常接近0或1时，可能就会出现 $\\log(0)$ 错误。根据本次作业第1题和第2题的结果，请思考是否有更稳定的数值算法，并重新编写损失函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\log\\hat\\rho = \\log\\frac{e^{X\\beta}}{1+e^{X\\beta}} = X\\beta - \\log(1+e^{X\\beta}) = X\\beta  - \\mathrm{softplus}(X\\beta)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softplus_fn(X):\n",
    "    lg = torch.log( torch.exp( -torch.abs(X)) + 1)\n",
    "    sp = torch.where(X>0, X + lg, lg )\n",
    "    return sp\n",
    "\n",
    "def loss_fn_logistic(bhat, x, y):\n",
    "    \n",
    "    xbhat = torch.matmul(x,bhat)\n",
    "\n",
    "\n",
    "\n",
    "    logrho1 = xbhat - softplus_fn(xbhat)\n",
    "    logrho2 = - softplus_fn (xbhat)\n",
    "\n",
    "    loss = -torch.sum( y*logrho1 + (1-y)* logrho2 ) / y.shape[0]\n",
    "\n",
    "    return loss  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "借用上次的代码检验一下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9170, 0.3364, 0.8977, 0.5980, 0.4221, 0.6253, 0.9170, 0.2109, 0.3360,\n",
      "        0.9648, 0.7328, 0.0751, 0.9946, 0.0556, 0.1271, 0.2755, 0.7304, 0.8941,\n",
      "        0.5023, 0.4810, 0.1459, 0.5332, 0.3893, 0.6638, 0.9223, 0.7655, 0.9368,\n",
      "        0.9572, 0.5875, 0.7579, 0.0816, 0.2996, 0.7004, 0.9692, 0.4495, 0.2173,\n",
      "        0.4645, 0.7816, 0.6017, 0.9717, 0.2330, 0.7996, 0.5977, 0.7063, 0.7713,\n",
      "        0.0598, 0.9857, 0.6477, 0.8376, 0.3697, 0.9493, 0.1482, 0.8821, 0.4167,\n",
      "        0.5975, 0.2716, 0.9308, 0.3505, 0.1371, 0.7432, 0.8271, 0.3834, 0.3581,\n",
      "        0.2745, 0.7022, 0.9428, 0.7693, 0.4178, 0.3509, 0.6029, 0.9774, 0.8961,\n",
      "        0.4705, 0.9801, 0.4752, 0.0135, 0.7070, 0.7323, 0.9354, 0.9310, 0.8790,\n",
      "        0.4550, 0.1359, 0.5483, 0.8925, 0.9669, 0.6468, 0.8284, 0.8154, 0.3084,\n",
      "        0.6380, 0.9928, 0.9899, 0.8051, 0.9232, 0.5038, 0.3173, 0.3921, 0.5216,\n",
      "        0.8531, 0.1394, 0.9823, 0.1469, 0.9830, 0.6762, 0.6870, 0.2843, 0.9497,\n",
      "        0.7572, 0.4652, 0.9581, 0.2799, 0.9905, 0.2572, 0.5173, 0.4884, 0.4902,\n",
      "        0.9794, 0.4946, 0.1617, 0.0714, 0.8568, 0.7842, 0.9433, 0.4602, 0.9653,\n",
      "        0.0528, 0.8341, 0.3252, 0.1165, 0.6667, 0.3264, 0.1699, 0.3646, 0.9958,\n",
      "        0.5997, 0.1246, 0.6464, 0.6014, 0.6770, 0.7541, 0.2346, 0.9852, 0.9553,\n",
      "        0.6816, 0.5328, 0.9182, 0.1096, 0.5000, 0.5684])\n",
      "tensor(2.2795)\n",
      "tensor(2.2795)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(123456)\n",
    "torch.manual_seed(123456)\n",
    "import torch.distributions as D\n",
    "import math\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "n = 150\n",
    "p = 6\n",
    "norm = D.Normal(loc=torch.tensor([1.0]), scale=torch.tensor([math.sqrt(2.0)]))\n",
    "uni = D.Uniform(low = torch.tensor([-1.0]), high=torch.tensor([1.0]))\n",
    "x = norm.sample(sample_shape=(n,p)).view(n,p) #?\n",
    "beta = uni.sample(sample_shape=(p,)).view(p,)\n",
    "\n",
    "def sigmoid(x):\n",
    "    sig = torch.exp(x) / (1 + torch.exp(x))\n",
    "    return sig\n",
    "\n",
    "param = sigmoid (torch.matmul(x,beta))\n",
    "print(param)\n",
    "y = D.Bernoulli(probs = param).sample()\n",
    "\n",
    "bhat = torch.ones(p)\n",
    "rhohat = sigmoid(torch.matmul(x,bhat))\n",
    "\n",
    "bce_logistic = nn.BCELoss()\n",
    "loss1 = bce_logistic(rhohat, y)\n",
    "\n",
    "loss2 = loss_fn_logistic(bhat, x, y)\n",
    "\n",
    "print(loss1)\n",
    "print(loss2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 自动微分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第5题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们利用一个两层的前馈神经网络来模拟 XOR 函数。首先生成四个数据点："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[0.0, 0.0],\n",
    "                  [0.0, 1.0],\n",
    "                  [1.0, 0.0],\n",
    "                  [1.0, 1.0]])\n",
    "y = torch.tensor([[0.0],\n",
    "                  [1.0],\n",
    "                  [1.0],\n",
    "                  [0.0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看出输入的维度为 $p=2$，输出的维度为 $d=1$，样本量为 $n=4$。我们将构建一个两层的前馈神经网络，其中隐藏层的维度为 $r=5$。计算流程如下：\n",
    "\n",
    "![](img/model.png)\n",
    "\n",
    "$Z_1=XW_1+\\mathbf{1}_nb_1^T,\\quad A_1=\\mathrm{softplus}(Z_1)$\n",
    "\n",
    "$Z_2=A_1W_2+\\mathbf{1}_nb_2^T,\\quad A_2=\\mathrm{sigmoid}(Z_2)$\n",
    "\n",
    "其中 $\\mathbf{1}_n$ 为元素全为1的 $n\\times 1$ 向量，$W_1$ 为 $p\\times r$ 矩阵，$b_1$ 为 $r\\times 1$ 向量，$W_2$ 为 $r\\times d$ 矩阵，$b_2$ 为 $d\\times 1$ 向量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$X: n \\times p,~ Z_1: n\\times r,~ A_1 : n\\times r,~Z_2: n\\times d$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先创建适当大小的 `w1`，`b1`，`w2` 和 `b2`，用标准正态分布填充，**并附加上梯度**（`requires_grad = True`）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = x.shape[0]  # 样本量\n",
    "p = x.shape[1]  # 输入维度\n",
    "d = y.shape[1]  # 输出维度\n",
    "r = 5           # 隐藏层维度\n",
    "\n",
    "import torch.distributions as D\n",
    "norm = D.Normal(loc=torch.tensor([0.0]), scale=torch.tensor(1.0))\n",
    "torch.manual_seed(123456)\n",
    "# 完成此处程序\n",
    "w1 = norm.sample(sample_shape=(p,r)).view(p,r)\n",
    "b1 = norm.sample(sample_shape=(r,)).view(r,1)\n",
    "w2 = norm.sample(sample_shape=(r,d)).view(r,d)\n",
    "b2 = norm.sample(sample_shape=(d,)).view(d,1)\n",
    "\n",
    "w1.requires_grad = True\n",
    "b1.requires_grad = True\n",
    "w2.requires_grad = True\n",
    "b2.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算 `z1`，并确保结果的维度是正确的："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.0585, -0.5059,  0.2100,  0.1418,  0.0596],\n",
      "        [ 0.3973, -2.1847,  1.1466,  0.5208,  0.3862],\n",
      "        [ 2.9229, -0.0988, -0.9871,  0.4907, -1.0841],\n",
      "        [ 2.2618, -1.7776, -0.0505,  0.8697, -0.7575]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 完成此处程序\n",
    "onevect = torch.ones(size=(n,)).view(n,1)\n",
    "z1 = torch.matmul(x,w1) + torch.matmul(onevect,b1.t()) \n",
    "print(z1)\n",
    "assert z1.shape == (n, r), \"z1 维度不正确\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "利用前面编写的 `softplus` 函数计算 `a1`："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.3563, 0.4719, 0.8037, 0.7665, 0.7234],\n",
      "        [0.9114, 0.1066, 1.4225, 0.9871, 0.9048],\n",
      "        [2.9753, 0.6450, 0.3167, 0.9683, 0.2913],\n",
      "        [2.3609, 0.1562, 0.6682, 1.2197, 0.3845]], grad_fn=<WhereBackward0>)\n"
     ]
    }
   ],
   "source": [
    "a1 = softplus2(z1)\n",
    "print(a1)\n",
    "assert a1.shape == (n, r), \"a1 维度不正确\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "继续计算 `z2` 和 `a2`："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Z_2=A_1W_2+\\mathbf{1}_nb_2^T,\\quad A_2=\\mathrm{sigmoid}(Z_2)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z2:tensor([[ 0.2095],\n",
      "        [-0.6404],\n",
      "        [ 1.1079],\n",
      "        [ 0.6916]], grad_fn=<AddBackward0>),\n",
      "A2:tensor([[0.5522],\n",
      "        [0.3452],\n",
      "        [0.7517],\n",
      "        [0.6663]], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 完成此处程序\n",
    "z2 = torch.matmul(a1,w2) + torch.matmul(onevect,b2.t()) \n",
    "a2 = sigmoid2(z2)\n",
    "print(f\"Z2:{z2},\\nA2:{a2}\")\n",
    "assert z2.shape == (n, d), \"z2 维度不正确\"\n",
    "assert a2.shape == (n, d), \"a2 维度不正确\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "利用 Logistic 回归的损失函数，计算 $l(y,a_2)=-y\\log(a_2)-(1-y)\\log(1-a_2)$："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8125, grad_fn=<NegBackward0>)\n"
     ]
    }
   ],
   "source": [
    "loss = -torch.mean(y * torch.log(a2) + (1.0 - y) * torch.log(1.0 - a2))\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "利用反向传播计算出 `loss` 对 `w1`，`b1`，`w2` 和 `b2` 的梯度并打印出来："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "loss.backward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w1.grad:tensor([[ 0.0081, -0.0007, -0.1257,  0.0921,  0.0395],\n",
      "        [ 0.0047,  0.0010,  0.0841,  0.0172, -0.0467]]),\n",
      "b1.grad:tensor([[ 0.0085],\n",
      "        [ 0.0041],\n",
      "        [-0.0319],\n",
      "        [ 0.0585],\n",
      "        [ 0.0117]]),\n",
      "w2.grad:tensor([[ 0.2466],\n",
      "        [ 0.0337],\n",
      "        [-0.0303],\n",
      "        [ 0.0873],\n",
      "        [-0.0023]]),\n",
      "b2.grad:tensor([[0.0789]])\n"
     ]
    }
   ],
   "source": [
    "print(f\"w1.grad:{w1.grad},\\nb1.grad:{b1.grad},\\nw2.grad:{w2.grad},\\nb2.grad:{b2.grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 前馈神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第6题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "利用模块化编程（参考 `lec5-module.ipynb`），搭建一个第3题中的前馈神经网络，取隐藏层维度 $r=3$，拟合 XOR 函数。注意设置合适的步长和迭代次数，并打印出最终在 `x` 上的拟合值，与 `y` 的真值进行比较。模型类可以参考如下结构："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[ 1.8645,  0.4071, -1.1971],\n",
      "        [ 0.3489, -1.1437, -0.6611]], requires_grad=True), Parameter containing:\n",
      "tensor([[0.5524],\n",
      "        [0.0060],\n",
      "        [0.1053]], requires_grad=True), Parameter containing:\n",
      "tensor([[ 0.8593],\n",
      "        [-0.3097],\n",
      "        [-0.9248]], requires_grad=True), Parameter containing:\n",
      "tensor([[0.9038]], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class XOR(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(XOR, self).__init__()\n",
    "        self.w1 = nn.Parameter(torch.randn(input_dim, hidden_dim))\n",
    "        self.b1 = nn.Parameter(torch.rand(hidden_dim, 1))\n",
    "        self.w2 = nn.Parameter(torch.randn(hidden_dim, output_dim))\n",
    "        self.b2 = nn.Parameter(torch.rand(output_dim, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        n = x.shape[0]  # 样本量\n",
    "        onevect = torch.ones(size=(n,)).view(n,1)\n",
    "\n",
    "        z1 = torch.matmul(x,self.w1) + torch.matmul(onevect,self.b1.t()) \n",
    "        a1 = softplus2(z1)\n",
    "        z2 = torch.matmul(a1,self.w2) + torch.matmul(onevect,self.b2.t()) \n",
    "        a2 = sigmoid2(z2)\n",
    "\n",
    "        return a2\n",
    "    \n",
    "torch.random.manual_seed(123456)\n",
    "\n",
    "model = XOR(input_dim=2, hidden_dim=3, output_dim=1)\n",
    "print(list(model.parameters()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.0012975949794054031: 100%|██████████| 10000/10000 [00:48<00:00, 207.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[7.4539e-04],\n",
      "        [9.9768e-01],\n",
      "        [9.9907e-01],\n",
      "        [1.1904e-03]], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "# 迭代次数\n",
    "nepoch = 10000\n",
    "# 学习率，即步长\n",
    "learning_rate = 0.1\n",
    "# 记录损失函数值\n",
    "losses = []\n",
    "\n",
    "opt = torch.optim.SGD(model.parameters(), lr=learning_rate) # pytorch提供了一个优化器，对于model这个对象里的所有para进行优化\n",
    "\n",
    "pbar = tqdm(range(nepoch))\n",
    "for i in pbar:\n",
    "    a2 = model(x)\n",
    "    loss = -torch.mean(y * torch.log(a2) + (1.0 - y) * torch.log(1.0 - a2))\n",
    "\n",
    "    opt.zero_grad() # 梯度清零\n",
    "    loss.backward() # 反向传播\n",
    "    opt.step() \n",
    "\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    pbar.set_description(\"loss: %s\" % loss.item())\n",
    "\n",
    "print(a2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x13df61150>]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pylab as plt\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[7.4539e-04],\n",
      "        [9.9768e-01],\n",
      "        [9.9907e-01],\n",
      "        [1.1904e-03]], grad_fn=<MulBackward0>)\n",
      "tensor([[0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.]])\n"
     ]
    }
   ],
   "source": [
    "print(a2)\n",
    "print(y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
