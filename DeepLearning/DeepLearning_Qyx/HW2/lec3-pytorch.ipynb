{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch基础+线性模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. PyTorch 基础"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如遇到不清楚的函数或主题，可以查阅[官方文档](https://pytorch.org/docs/stable/index.html)或利用搜索引擎寻求帮助。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 必备代码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch 通常会与 Numpy 包共同使用，因此导入 PyTorch 的时候按惯例也把 Numpy 加载进来。更重要的是，在**每个** PyTorch 程序的最开头都应设置好随机数种子，从而让结果可重复。**作业中也应在每题的代码开头设置随机数种子**。由于 PyTorch 和 Numpy 使用了两套随机数生成机制，因此还需分别设置。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1165d2410>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "np.random.seed(123456)\n",
    "torch.manual_seed(123456)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch 中最为重要的数据结构是张量（Tensor），可以简单理解为多维数组，我们一般使用的向量和矩阵都是张量的特例。本次作业中我们只用到一维和二维 Tensor，分别对应向量和矩阵。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensor 可以通过 `torch.tensor()` 加上给定的数据创建，注意矩阵是按行创建的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 5.])\n"
     ]
    }
   ],
   "source": [
    "vec = torch.tensor([1.0, 2.0, 5.0])\n",
    "print(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 2.0000, 2.0000],\n",
      "        [3.0000, 5.0000, 4.5000]])\n"
     ]
    }
   ],
   "source": [
    "mat = torch.tensor([[1.0, 2.0, 2.0], [3.0, 5.0, 4.5]])\n",
    "print(mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一些常用的 Tensor 有专用的创建方法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones(3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3.0000,  4.7500,  6.5000,  8.2500, 10.0000])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.linspace(3, 10, steps=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "也可以给定形状生成随机数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.8645,  0.4071, -1.1971],\n",
       "        [ 0.3489, -1.1437, -0.6611]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(2, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensor 的形状可以通过 `shape` 属性来获取："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3])\n",
      "torch.Size([2, 3])\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "print(vec.shape)\n",
    "print(mat.shape)\n",
    "n = mat.shape[0]\n",
    "p = mat.shape[1]\n",
    "print(n)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将 Tensor 变形："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.],\n",
      "        [2.],\n",
      "        [5.]])\n",
      "tensor([[1.0000, 2.0000],\n",
      "        [2.0000, 3.0000],\n",
      "        [5.0000, 4.5000]])\n",
      "tensor([[1.],\n",
      "        [2.],\n",
      "        [5.]]) \n",
      " tensor([1., 2., 5.])\n"
     ]
    }
   ],
   "source": [
    "print(vec.view(3, 1))\n",
    "print(mat.view(3, 2))\n",
    "\n",
    "newvec = vec.view(3,1)\n",
    "print(newvec,'\\n',vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 矩阵向量运算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在Pytorch中，当一个torch方法的函数是含有下划线的，如`torch.exp_(vec)`，则会对这个对象进行修改；而一般不加下划线的则是返回一个新的对象，对原对象不作处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensor 可以很直观地与标量进行运算："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 4., 7.])\n",
      "tensor([[1.2000, 2.4000, 2.4000],\n",
      "        [3.6000, 6.0000, 5.4000]])\n"
     ]
    }
   ],
   "source": [
    "print(vec + 2.0)\n",
    "print(mat * 1.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "逐元素运算的数学函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  2.7183,   7.3891, 148.4132])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.8415,  0.9093,  0.9093],\n",
       "        [ 0.1411, -0.9589, -0.9775]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sin(mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "汇总运算："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.6667)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(17.5000)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "按坐标轴汇总："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 2.0000, 2.0000],\n",
      "        [3.0000, 5.0000, 4.5000]])\n",
      "tensor([4.0000, 7.0000, 6.5000])\n",
      "tensor([ 5.0000, 12.5000])\n"
     ]
    }
   ],
   "source": [
    "print(mat)\n",
    "print(torch.sum(mat, dim=0))\n",
    "print(torch.sum(mat, dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "矩阵乘法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([15.0000, 35.5000])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matmul(mat, vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "搭配转置："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10.0000, 17.0000, 15.5000],\n",
       "        [17.0000, 29.0000, 26.5000],\n",
       "        [15.5000, 26.5000, 24.2500]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matmul(torch.t(mat), mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 3.0000],\n",
       "        [2.0000, 5.0000],\n",
       "        [2.0000, 4.5000]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat.t()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 统计分布"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch 包含了许多常见的统计分布，参见[说明文档](https://pytorch.org/docs/stable/distributions.html)。大部分分布都可以计算 p.d.f. 和 c.d.f. 等函数，以及进行随机模拟抽样。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "建立一个正态分布 $N(1, 3)$ 对象：\n",
    "\n",
    "**注意这里的`scale`是标准差**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.distributions as D\n",
    "import math\n",
    "\n",
    "norm = D.Normal(loc=torch.tensor([1.0]), scale=torch.tensor([math.sqrt(3.0)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算 $x = 1,2,3$ 处的对数密度函数值："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.4682, -1.6349, -2.1349])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm.log_prob(torch.tensor([1.0, 2.0, 3.0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "生成5个随机数（注意这里形状的写法）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.9078],\n",
       "        [ 2.6222],\n",
       "        [ 1.6566],\n",
       "        [ 1.5657],\n",
       "        [ 2.8333]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm.sample(sample_shape=(5,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分布的参数可以是一个向量，例如三个分别以0.1, 0.5和0.9为参数的 Bernoulli 分布："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "bern = D.Bernoulli(probs=torch.tensor([0.1, 0.5, 0.9]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "各自生成一个随机数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 1.])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bern.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其他常见的操作可以参考[官方教程](https://pytorch.org/tutorials/beginner/basics/tensorqs_tutorial.html)，完整的函数列表可以查看[官方 API 文档](https://pytorch.org/docs/stable/torch.html)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 练习题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第1题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) 生成一个大小为 $[n \\times p] = [200 \\times 10]$ 的数据矩阵 `x`，用正态分布 N(0, 2) 填充。随机数种子设为123456。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.5865, -1.6086,  2.4507,  ...,  1.0598,  1.5384,  1.0760],\n",
      "        [ 1.7137,  0.2947,  0.3083,  ...,  0.8538,  1.1678,  0.6129],\n",
      "        [ 2.6316,  2.9672,  2.5178,  ...,  2.1425,  1.2001,  0.2178],\n",
      "        ...,\n",
      "        [ 1.2433, -1.6048,  2.6921,  ...,  1.2950, -0.0972,  2.4354],\n",
      "        [-0.6391,  0.8272,  0.6263,  ..., -2.5582,  2.0212,  0.2053],\n",
      "        [ 1.9554,  0.0333, -0.3287,  ...,  2.3684,  0.2466, -0.2890]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123456)\n",
    "\n",
    "n = 200\n",
    "p = 10\n",
    "\n",
    "norm = D.Normal(loc=torch.tensor([1.0]), scale=torch.tensor([math.sqrt(2.0)]))\n",
    "\n",
    "x = norm.sample(sample_shape=(n,p)).view(n,p) #?\n",
    "#x = norm.sample(sample_shape=(n,p))\n",
    "print(x)\n",
    "\n",
    "# 检查 x 的大小，方便 debug\n",
    "assert x.shape == (n, p), \"x 形状有误\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) 生成一个长度为 $p$ 的向量 `beta`，每个元素服从均匀分布 Uniform(-1, 1)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni = D.Uniform(low = torch.tensor([-1.0]), high=torch.tensor([1.0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10])\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "\n",
    "beta = uni.sample(sample_shape=(p,)).view(p,)\n",
    "print(beta.shape)\n",
    "print(p)\n",
    "# 检查 beta 的长度，方便 debug\n",
    "assert beta.shape == (p,), \"beta 长度有误\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) 生成一个长度为 $n$ 的向量 `eps`，每个元素服从独立正态 $N(0, 0.1)$。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = D.Normal(loc=torch.tensor([1.0]), scale=torch.tensor([math.sqrt(.1)]))\n",
    "eps = norm.sample(sample_shape=(n,)).view(n,)\n",
    "\n",
    "\n",
    "# 检查 eps 的长度，方便 debug\n",
    "assert eps.shape == (n,), \"eps 长度有误\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d) 创建向量 `y`，令其在数学上等于 $y=X\\beta+\\epsilon$。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.matmul(x, beta) + eps\n",
    "\n",
    "# 检查 y 的长度，方便 debug\n",
    "assert y.shape == (n,), \"y 长度有误\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(e) 回归问题：给定数据 `x` 和 `y`，估计 `beta` 的取值。以 MSE 为损失函数，编写 Python 函数 `loss_fn_reg(bhat, x, y)`，用来返回任意 $\\hat{\\beta}$ 下的目标函数值。请用基础的矩阵和向量运算实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn_reg(bhat, x, y):\n",
    "    yhat = torch.matmul(x,bhat)\n",
    "    err = yhat - y\n",
    "    n = y.shape[0]\n",
    "    # print(err)\n",
    "    mse = torch.sum(err**2) / n\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(f) Pytorch 中也提供了 MSE 损失函数，参见[其文档](https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html)。其用法是先建立一个损失函数对象，然后将 $\\hat{y}$ 和 $y$ 作为参数传入。请利用这种方法计算如下给定 $\\hat{\\beta}$ 后的损失函数值，并与你自己的函数结果进行对比。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(77.3495)\n",
      "tensor(77.3495)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "bhat = torch.ones(p)\n",
    "yhat = torch.matmul(x,bhat)\n",
    "\n",
    "mse_reg = nn.MSELoss()\n",
    "loss1 = mse_reg(yhat, y)\n",
    "\n",
    "loss2 = loss_fn_reg(bhat, x, y)\n",
    "\n",
    "print(loss1)\n",
    "print(loss2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "【说明文字】"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第2题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) 与第1题类似创建变量 `x` 和 `beta`，但使用不同的 `n` 和 `p`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123456)\n",
    "torch.manual_seed(123456)\n",
    "import torch.distributions as D\n",
    "import math\n",
    "\n",
    "\n",
    "n = 150\n",
    "p = 6\n",
    "norm = D.Normal(loc=torch.tensor([1.0]), scale=torch.tensor([math.sqrt(2.0)]))\n",
    "uni = D.Uniform(low = torch.tensor([-1.0]), high=torch.tensor([1.0]))\n",
    "x = norm.sample(sample_shape=(n,p)).view(n,p) #?\n",
    "beta = uni.sample(sample_shape=(p,)).view(p,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) 定义函数 `sigmoid(x)`，其中 `x` 是一个 Tensor，$\\mathrm{sigmoid}(x)=e^x/(1+e^x)$。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    sig = torch.exp(x) / (1 + torch.exp(x))\n",
    "    return sig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) 根据分布 $Y|X\\sim Bernoulli(\\mathrm{sigmoid}(X\\beta))$，生成 $Y$ 的随机数。提示：参照1.4节的方法，先计算 Bernoulli 分布的参数**向量**，然后生成随机数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9170, 0.3364, 0.8977, 0.5980, 0.4221, 0.6253, 0.9170, 0.2109, 0.3360,\n",
      "        0.9648, 0.7328, 0.0751, 0.9946, 0.0556, 0.1271, 0.2755, 0.7304, 0.8941,\n",
      "        0.5023, 0.4810, 0.1459, 0.5332, 0.3893, 0.6638, 0.9223, 0.7655, 0.9368,\n",
      "        0.9572, 0.5875, 0.7579, 0.0816, 0.2996, 0.7004, 0.9692, 0.4495, 0.2173,\n",
      "        0.4645, 0.7816, 0.6017, 0.9717, 0.2330, 0.7996, 0.5977, 0.7063, 0.7713,\n",
      "        0.0598, 0.9857, 0.6477, 0.8376, 0.3697, 0.9493, 0.1482, 0.8821, 0.4167,\n",
      "        0.5975, 0.2716, 0.9308, 0.3505, 0.1371, 0.7432, 0.8271, 0.3834, 0.3581,\n",
      "        0.2745, 0.7022, 0.9428, 0.7693, 0.4178, 0.3509, 0.6029, 0.9774, 0.8961,\n",
      "        0.4705, 0.9801, 0.4752, 0.0135, 0.7070, 0.7323, 0.9354, 0.9310, 0.8790,\n",
      "        0.4550, 0.1359, 0.5483, 0.8925, 0.9669, 0.6468, 0.8284, 0.8154, 0.3084,\n",
      "        0.6380, 0.9928, 0.9899, 0.8051, 0.9232, 0.5038, 0.3173, 0.3921, 0.5216,\n",
      "        0.8531, 0.1394, 0.9823, 0.1469, 0.9830, 0.6762, 0.6870, 0.2843, 0.9497,\n",
      "        0.7572, 0.4652, 0.9581, 0.2799, 0.9905, 0.2572, 0.5173, 0.4884, 0.4902,\n",
      "        0.9794, 0.4946, 0.1617, 0.0714, 0.8568, 0.7842, 0.9433, 0.4602, 0.9653,\n",
      "        0.0528, 0.8341, 0.3252, 0.1165, 0.6667, 0.3264, 0.1699, 0.3646, 0.9958,\n",
      "        0.5997, 0.1246, 0.6464, 0.6014, 0.6770, 0.7541, 0.2346, 0.9852, 0.9553,\n",
      "        0.6816, 0.5328, 0.9182, 0.1096, 0.5000, 0.5684])\n"
     ]
    }
   ],
   "source": [
    "param = sigmoid (torch.matmul(x,beta))\n",
    "print(param)\n",
    "y = D.Bernoulli(probs = param).sample()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d) 已知 $Bernoulli(\\rho)$ 分布的对数密度函数为 $\\log p(y;\\rho)=y\\log \\rho + (1-y) \\cdot \\log(1-\\rho)$。根据此信息，推导出给定 $\\hat{\\beta}$ 时的对数似然函数，并编写损失函数 `loss_fn_logistic(bhat, x, y)`，返回**负**对数似然值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "【说明文字】"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn_logistic(bhat, x, y):\n",
    "    rhohat = sigmoid(torch.matmul(x,bhat))\n",
    "    loss = -torch.sum(y*torch.log(rhohat) + (1-y) * torch.log(1-rhohat))/y.shape[0]\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(e) Pytorch 中也提供了 BCELoss 损失函数，参见[其文档](https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html)。其用法是先建立一个损失函数对象，然后将 $\\hat{\\rho}$ 和 $y$ 作为参数传入。请利用这种方法计算如下给定 $\\hat{\\beta}$ 后的损失函数值，并与你自己的函数结果进行对比。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.2795)\n",
      "tensor(2.2795)\n"
     ]
    }
   ],
   "source": [
    "bhat = torch.ones(p)\n",
    "rhohat = sigmoid(torch.matmul(x,bhat))\n",
    "\n",
    "bce_logistic = nn.BCELoss()\n",
    "loss1 = bce_logistic(rhohat, y)\n",
    "\n",
    "loss2 = loss_fn_logistic(bhat, x, y)\n",
    "\n",
    "print(loss1)\n",
    "print(loss2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "【说明文字】"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第3题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) 多分类问题的数据通常包括数据阵 $X$ 和标签向量 $l$，其中标签为整数。在计算损失函数时，我们需要先将 $l$ 转换成多项分布的0-1数据，即所谓 One-hot 编码。运行并观察下面的代码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 2, 1, 0, 3, 3, 3, 3, 0, 3, 0, 0, 2, 2, 0, 3, 0, 3, 3])\n",
      "torch.Size([200, 4])\n",
      "tensor([[0, 1, 0, 0],\n",
      "        [0, 0, 1, 0],\n",
      "        [0, 0, 1, 0],\n",
      "        [0, 1, 0, 0],\n",
      "        [1, 0, 0, 0],\n",
      "        [0, 0, 0, 1],\n",
      "        [0, 0, 0, 1],\n",
      "        [0, 0, 0, 1],\n",
      "        [0, 0, 0, 1],\n",
      "        [1, 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(123456)\n",
    "torch.manual_seed(123456)\n",
    "n = 200  # 样本量\n",
    "p = 10   # 变量数\n",
    "k = 4    # 类别数\n",
    "x = torch.randn(n, p)\n",
    "l = torch.tensor(np.random.choice(range(4), size=n, replace=True), dtype=int)\n",
    "print(l[:20])\n",
    "\n",
    "y = torch.nn.functional.one_hot(l)\n",
    "print(y.shape)\n",
    "print(y[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) 创建矩阵 `W`，大小为 $k \\times p$，用 N(0, 1) 填充其取值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "norm = D.Normal(loc=torch.tensor([1.0]), scale=torch.tensor([math.sqrt(1.0)]))\n",
    "w = norm.sample(sample_shape=(k,p)).view(k,p) #?\n",
    "# 检查 w 的形状，方便 debug\n",
    "assert w.shape == (k, p), \"w 形状有误\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) 接下来计算对 $Y$ 的概率预测值，其中每个 $Y_i$ 观测对应一个等长的概率向量 $p_i$，而 $p_i=\\mathrm{Softmax}(Wx_i)$。首先计算 $Wx_i$，其中 $x_i$ 是第 $i$ 个观测。由于 $X$ 是把 $x_i$ 按行组合，因此矩阵形式表达为 $U=XW'$，其中 $U$ 的第 $i$ 行即为 $Wx_i$。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = torch.matmul(x, w.t())\n",
    "\n",
    "# 检查 u 的形状，方便 debug\n",
    "assert u.shape == (n, k), \"u 形状有误\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们先测试一下 $\\mathrm{Softmax}(Wx_{100})$ 的结果，观察其元素和是否为1。代码中的 `dim=0` 意思是对第一个下标方向计算 Softmax，由于 `u[99]` 是一个向量，因此第一个下标方向就是该向量本身。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.7236, 0.1425, 0.1051, 0.0287])\n",
      "tensor(1.0000)\n"
     ]
    }
   ],
   "source": [
    "print(torch.softmax(u[99], dim=0))\n",
    "print(sum(torch.softmax(u[99], dim=0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "而为了对 $U$ 的每一行都计算 Softmax，我们可以直接对整个 `u` 矩阵用 `torch.softmax`，其中 `dim` 需指定为1，意思是对第二个下标方向求 Softmax，即对 $U$ 的每一行。原理类似于1.3节的按坐标轴汇总。请完成该计算，得到矩阵 $P$，其中 $P$ 的第 $i$ 行即为 $p_i$。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[7.6269e-05, 2.8724e-03, 9.9677e-01, 2.8114e-04],\n",
      "        [5.9896e-01, 3.0678e-01, 7.4194e-02, 2.0060e-02],\n",
      "        [5.7304e-01, 8.0425e-02, 6.5251e-04, 3.4588e-01],\n",
      "        [9.1996e-01, 7.2885e-02, 7.1390e-03, 1.7064e-05],\n",
      "        [1.3924e-01, 1.9466e-01, 1.9528e-02, 6.4658e-01],\n",
      "        [9.1579e-01, 1.2557e-03, 8.2951e-02, 2.6220e-06],\n",
      "        [9.8585e-01, 7.7156e-04, 1.3343e-02, 3.5518e-05],\n",
      "        [9.8902e-01, 7.1037e-03, 3.1069e-03, 7.7161e-04],\n",
      "        [1.8780e-01, 7.6740e-01, 2.5136e-04, 4.4552e-02],\n",
      "        [5.3485e-01, 7.2683e-02, 7.2678e-02, 3.1979e-01],\n",
      "        [2.9731e-02, 7.2813e-01, 1.5023e-02, 2.2711e-01],\n",
      "        [9.9897e-01, 7.5668e-04, 2.3033e-04, 4.6076e-05],\n",
      "        [2.9977e-04, 3.9224e-05, 9.9929e-01, 3.7325e-04],\n",
      "        [4.2689e-02, 4.8078e-01, 1.8308e-02, 4.5822e-01],\n",
      "        [2.8122e-01, 3.5670e-03, 7.1329e-01, 1.9160e-03],\n",
      "        [3.3673e-04, 2.4343e-03, 9.7221e-01, 2.5021e-02],\n",
      "        [9.9047e-01, 1.8729e-04, 1.1907e-03, 8.1471e-03],\n",
      "        [8.1385e-02, 1.7990e-01, 2.2205e-01, 5.1666e-01],\n",
      "        [9.9451e-01, 1.7241e-03, 2.9190e-03, 8.4240e-04],\n",
      "        [1.0873e-04, 4.4123e-01, 2.9874e-02, 5.2879e-01],\n",
      "        [9.9658e-02, 1.6033e-02, 7.9041e-01, 9.3900e-02],\n",
      "        [1.3607e-03, 3.0753e-05, 9.8512e-01, 1.3491e-02],\n",
      "        [9.7100e-01, 2.4060e-03, 2.6494e-02, 1.0017e-04],\n",
      "        [8.6105e-01, 4.6543e-03, 1.3377e-01, 5.2200e-04],\n",
      "        [9.9943e-01, 1.2904e-05, 3.1880e-04, 2.3475e-04],\n",
      "        [3.5191e-01, 6.2189e-01, 1.9287e-03, 2.4273e-02],\n",
      "        [5.2337e-01, 4.6276e-01, 1.3367e-02, 5.0855e-04],\n",
      "        [1.1499e-03, 2.2357e-01, 1.3027e-02, 7.6225e-01],\n",
      "        [4.5427e-05, 4.0621e-02, 3.9001e-03, 9.5543e-01],\n",
      "        [5.5109e-03, 9.6893e-01, 5.1748e-04, 2.5045e-02],\n",
      "        [6.0769e-01, 2.3219e-01, 3.9399e-02, 1.2072e-01],\n",
      "        [2.3370e-02, 1.3621e-02, 9.2321e-01, 3.9798e-02],\n",
      "        [2.4563e-01, 5.4739e-01, 5.2756e-03, 2.0171e-01],\n",
      "        [2.6790e-04, 1.3324e-02, 9.8503e-01, 1.3807e-03],\n",
      "        [6.1455e-03, 1.6207e-01, 8.2215e-03, 8.2357e-01],\n",
      "        [1.5496e-02, 9.7239e-01, 1.1964e-02, 1.4547e-04],\n",
      "        [2.8235e-01, 3.6923e-01, 3.4521e-01, 3.2055e-03],\n",
      "        [9.8516e-02, 1.2762e-01, 3.3244e-01, 4.4142e-01],\n",
      "        [4.6181e-03, 6.2731e-01, 2.9825e-02, 3.3825e-01],\n",
      "        [9.1732e-01, 2.3142e-02, 4.8921e-02, 1.0621e-02],\n",
      "        [5.3999e-05, 3.9573e-02, 5.1087e-05, 9.6032e-01],\n",
      "        [9.8442e-01, 8.0638e-03, 7.4216e-03, 9.2635e-05],\n",
      "        [8.3745e-02, 8.6933e-01, 1.9520e-02, 2.7402e-02],\n",
      "        [3.5408e-04, 7.9050e-03, 6.0897e-04, 9.9113e-01],\n",
      "        [1.1365e-04, 7.9705e-01, 1.0021e-01, 1.0263e-01],\n",
      "        [3.3150e-01, 6.0433e-04, 6.3225e-01, 3.5648e-02],\n",
      "        [3.9663e-02, 8.9897e-01, 3.1737e-03, 5.8190e-02],\n",
      "        [3.9384e-04, 3.4147e-01, 6.2502e-01, 3.3115e-02],\n",
      "        [7.6283e-02, 9.0611e-01, 6.3367e-03, 1.1274e-02],\n",
      "        [1.5744e-02, 7.5620e-02, 6.0448e-01, 3.0415e-01],\n",
      "        [9.6279e-01, 6.3627e-03, 2.2174e-02, 8.6687e-03],\n",
      "        [2.5256e-03, 3.1919e-01, 3.0145e-03, 6.7527e-01],\n",
      "        [9.6522e-01, 2.3234e-02, 1.1498e-02, 4.7047e-05],\n",
      "        [5.0717e-04, 2.2703e-01, 6.0603e-02, 7.1186e-01],\n",
      "        [1.1002e-02, 1.2561e-01, 5.3004e-02, 8.1039e-01],\n",
      "        [9.0593e-01, 5.2483e-05, 7.6936e-02, 1.7078e-02],\n",
      "        [9.9550e-01, 4.2856e-05, 4.1502e-03, 3.1137e-04],\n",
      "        [5.7862e-02, 4.8261e-02, 7.8185e-01, 1.1202e-01],\n",
      "        [3.4786e-04, 9.4808e-01, 6.0701e-04, 5.0964e-02],\n",
      "        [1.5103e-01, 1.5776e-01, 4.4491e-01, 2.4630e-01],\n",
      "        [3.7814e-03, 1.8033e-01, 2.7375e-03, 8.1315e-01],\n",
      "        [8.4530e-03, 8.2021e-02, 1.0643e-01, 8.0310e-01],\n",
      "        [7.3317e-03, 6.1589e-01, 2.7280e-01, 1.0398e-01],\n",
      "        [1.2326e-01, 2.2029e-01, 4.3358e-01, 2.2287e-01],\n",
      "        [3.5559e-01, 6.1899e-01, 2.4188e-02, 1.2381e-03],\n",
      "        [2.7837e-01, 3.6934e-02, 6.3149e-01, 5.3203e-02],\n",
      "        [1.1187e-01, 1.6436e-01, 7.2090e-01, 2.8652e-03],\n",
      "        [9.9839e-01, 3.8669e-04, 1.2244e-03, 2.9296e-06],\n",
      "        [1.8839e-02, 9.5298e-01, 8.6478e-03, 1.9536e-02],\n",
      "        [1.4410e-01, 3.5894e-03, 8.5166e-01, 6.4466e-04],\n",
      "        [9.8909e-01, 4.5079e-03, 1.3590e-04, 6.2666e-03],\n",
      "        [8.7941e-05, 1.7278e-02, 3.4294e-03, 9.7921e-01],\n",
      "        [8.3806e-05, 2.1395e-04, 8.6736e-01, 1.3234e-01],\n",
      "        [6.5777e-01, 1.3494e-01, 1.8178e-03, 2.0547e-01],\n",
      "        [1.7747e-02, 1.4403e-02, 6.0486e-01, 3.6299e-01],\n",
      "        [1.7418e-03, 4.9103e-04, 9.9386e-01, 3.9041e-03],\n",
      "        [1.1316e-02, 9.7519e-01, 3.3367e-04, 1.3155e-02],\n",
      "        [7.9048e-02, 8.5177e-01, 4.0395e-02, 2.8784e-02],\n",
      "        [9.4123e-01, 2.0483e-03, 5.5048e-02, 1.6724e-03],\n",
      "        [1.6963e-04, 9.9927e-01, 3.3688e-05, 5.2293e-04],\n",
      "        [8.1638e-01, 1.8781e-03, 1.5710e-01, 2.4642e-02],\n",
      "        [8.9475e-02, 2.1760e-01, 2.3899e-01, 4.5394e-01],\n",
      "        [7.3802e-02, 2.0709e-02, 8.7660e-01, 2.8891e-02],\n",
      "        [7.5235e-01, 2.4087e-01, 1.4431e-03, 5.3327e-03],\n",
      "        [8.4966e-01, 5.1497e-02, 2.2815e-02, 7.6032e-02],\n",
      "        [1.4038e-03, 1.9625e-01, 6.1056e-01, 1.9179e-01],\n",
      "        [1.4864e-01, 2.7911e-01, 4.3228e-03, 5.6792e-01],\n",
      "        [1.0519e-03, 9.1774e-02, 9.0349e-01, 3.6853e-03],\n",
      "        [2.0799e-04, 6.5271e-03, 3.7852e-05, 9.9323e-01],\n",
      "        [1.5192e-03, 2.2390e-04, 8.7937e-01, 1.1889e-01],\n",
      "        [1.3527e-02, 8.2351e-02, 9.0205e-01, 2.0738e-03],\n",
      "        [8.8787e-01, 5.4838e-02, 3.6774e-02, 2.0514e-02],\n",
      "        [3.7150e-02, 9.5840e-01, 5.9634e-05, 4.3916e-03],\n",
      "        [7.8233e-01, 1.9526e-02, 1.9015e-01, 7.9928e-03],\n",
      "        [4.4618e-02, 7.3337e-01, 1.1686e-02, 2.1033e-01],\n",
      "        [2.3250e-04, 2.9803e-01, 7.0450e-03, 6.9469e-01],\n",
      "        [4.4785e-01, 1.5601e-02, 4.7122e-01, 6.5330e-02],\n",
      "        [2.0539e-03, 6.5695e-03, 2.8236e-03, 9.8855e-01],\n",
      "        [3.1492e-02, 2.7601e-02, 4.6769e-02, 8.9414e-01],\n",
      "        [7.2364e-01, 1.4254e-01, 1.0513e-01, 2.8685e-02],\n",
      "        [4.9351e-03, 3.2345e-04, 9.4408e-01, 5.0659e-02],\n",
      "        [4.5956e-01, 1.8298e-01, 1.9068e-01, 1.6679e-01],\n",
      "        [3.2998e-04, 9.6345e-01, 1.8391e-02, 1.7830e-02],\n",
      "        [7.6093e-01, 1.3574e-01, 1.9622e-02, 8.3709e-02],\n",
      "        [5.0869e-04, 9.8697e-01, 1.1811e-03, 1.1336e-02],\n",
      "        [1.3967e-01, 3.3513e-01, 1.2286e-01, 4.0234e-01],\n",
      "        [9.8861e-01, 9.9510e-03, 4.6860e-04, 9.7055e-04],\n",
      "        [2.2216e-02, 3.4046e-01, 1.7246e-01, 4.6487e-01],\n",
      "        [8.2788e-01, 1.3253e-01, 1.3238e-02, 2.6353e-02],\n",
      "        [7.3774e-02, 3.4500e-01, 3.5003e-05, 5.8119e-01],\n",
      "        [3.8319e-04, 7.3603e-01, 2.9420e-02, 2.3416e-01],\n",
      "        [1.2569e-01, 8.0370e-01, 4.7899e-02, 2.2713e-02],\n",
      "        [7.3862e-02, 8.8020e-01, 1.1856e-03, 4.4747e-02],\n",
      "        [1.8019e-08, 6.5208e-03, 5.5515e-05, 9.9342e-01],\n",
      "        [8.7421e-01, 4.0768e-03, 9.5924e-02, 2.5789e-02],\n",
      "        [9.5705e-01, 1.2472e-03, 4.1589e-02, 1.1428e-04],\n",
      "        [4.1539e-02, 7.7304e-01, 3.4107e-06, 1.8541e-01],\n",
      "        [3.4506e-03, 3.7378e-02, 4.5452e-02, 9.1372e-01],\n",
      "        [8.7291e-06, 3.2779e-01, 3.4189e-04, 6.7186e-01],\n",
      "        [4.5689e-02, 1.3880e-01, 7.6167e-01, 5.3835e-02],\n",
      "        [2.7215e-01, 5.0128e-02, 6.6456e-01, 1.3160e-02],\n",
      "        [3.8683e-02, 8.8967e-01, 2.2864e-02, 4.8787e-02],\n",
      "        [9.6590e-01, 2.6954e-04, 3.3830e-02, 1.2866e-06],\n",
      "        [2.0512e-01, 2.8769e-01, 2.6110e-02, 4.8108e-01],\n",
      "        [2.4578e-01, 6.4342e-01, 3.6312e-02, 7.4483e-02],\n",
      "        [7.9201e-02, 6.2262e-01, 2.0747e-01, 9.0713e-02],\n",
      "        [7.0208e-01, 3.6816e-02, 2.2834e-01, 3.2769e-02],\n",
      "        [1.7772e-02, 8.3435e-01, 4.9641e-03, 1.4291e-01],\n",
      "        [1.8438e-02, 5.5769e-01, 3.9784e-01, 2.6033e-02],\n",
      "        [8.3427e-01, 3.3947e-03, 1.4658e-01, 1.5751e-02],\n",
      "        [1.5261e-04, 1.2768e-01, 3.9067e-03, 8.6826e-01],\n",
      "        [1.5626e-02, 7.6013e-01, 6.4477e-03, 2.1780e-01],\n",
      "        [6.7522e-03, 2.6652e-02, 5.9018e-01, 3.7642e-01],\n",
      "        [8.6431e-05, 5.2657e-02, 8.3656e-03, 9.3889e-01],\n",
      "        [2.4288e-01, 1.0242e-03, 6.6863e-01, 8.7470e-02],\n",
      "        [8.9089e-01, 2.2749e-03, 1.0672e-01, 1.1551e-04],\n",
      "        [3.2860e-05, 4.5512e-05, 9.9659e-01, 3.3275e-03],\n",
      "        [4.7047e-02, 1.0733e-01, 3.6293e-01, 4.8269e-01],\n",
      "        [1.2565e-01, 8.6463e-01, 6.3314e-04, 9.0874e-03],\n",
      "        [9.5489e-01, 3.4333e-02, 8.7326e-03, 2.0479e-03],\n",
      "        [9.7287e-01, 1.5227e-04, 2.6963e-02, 1.5755e-05],\n",
      "        [2.0024e-04, 9.9695e-04, 6.8449e-03, 9.9196e-01],\n",
      "        [2.7839e-04, 8.2149e-01, 1.4959e-02, 1.6328e-01],\n",
      "        [2.3238e-02, 2.8951e-02, 4.0340e-01, 5.4441e-01],\n",
      "        [4.5808e-02, 1.0803e-03, 9.3729e-01, 1.5819e-02],\n",
      "        [9.9760e-01, 5.2751e-04, 1.2636e-03, 6.1340e-04],\n",
      "        [3.8783e-04, 1.3736e-02, 9.7675e-01, 9.1305e-03],\n",
      "        [1.2387e-02, 6.7930e-03, 9.6613e-01, 1.4689e-02],\n",
      "        [8.8639e-01, 7.1493e-03, 1.0599e-01, 4.6869e-04],\n",
      "        [5.8610e-03, 5.8399e-01, 2.2868e-01, 1.8147e-01],\n",
      "        [2.0258e-01, 1.0978e-01, 9.2405e-02, 5.9523e-01],\n",
      "        [4.7705e-02, 8.0945e-01, 1.4218e-01, 6.7185e-04],\n",
      "        [8.5673e-01, 1.3898e-01, 2.4330e-03, 1.8597e-03],\n",
      "        [9.7010e-01, 1.6140e-02, 2.6014e-03, 1.1156e-02],\n",
      "        [6.0206e-01, 7.7977e-02, 1.4803e-01, 1.7193e-01],\n",
      "        [9.8769e-01, 3.0619e-03, 7.2272e-04, 8.5235e-03],\n",
      "        [8.8270e-01, 4.4738e-04, 1.1646e-01, 3.8653e-04],\n",
      "        [2.5602e-01, 4.5389e-01, 3.0476e-02, 2.5962e-01],\n",
      "        [1.9372e-01, 7.3218e-04, 7.9808e-01, 7.4618e-03],\n",
      "        [1.2415e-01, 1.0832e-02, 8.7792e-04, 8.6414e-01],\n",
      "        [6.3475e-03, 9.8525e-01, 3.2922e-03, 5.1127e-03],\n",
      "        [7.9445e-01, 1.3618e-01, 6.9328e-02, 4.0487e-05],\n",
      "        [2.1163e-01, 7.5899e-04, 7.8572e-01, 1.8911e-03],\n",
      "        [9.1465e-01, 9.6950e-03, 3.1133e-03, 7.2540e-02],\n",
      "        [6.5709e-01, 3.0797e-01, 3.4705e-02, 2.3256e-04],\n",
      "        [1.6081e-01, 7.4233e-01, 7.9613e-04, 9.6065e-02],\n",
      "        [5.6216e-01, 2.5833e-03, 4.3242e-01, 2.8304e-03],\n",
      "        [1.2305e-02, 4.9510e-03, 2.6903e-01, 7.1371e-01],\n",
      "        [4.6754e-03, 1.1015e-02, 1.9880e-01, 7.8551e-01],\n",
      "        [4.9177e-02, 2.3692e-01, 1.9193e-01, 5.2198e-01],\n",
      "        [2.3691e-04, 4.2420e-01, 5.4263e-01, 3.2931e-02],\n",
      "        [2.5620e-01, 4.4431e-01, 2.6509e-01, 3.4400e-02],\n",
      "        [1.3356e-01, 1.2555e-02, 8.3615e-01, 1.7734e-02],\n",
      "        [1.8018e-01, 2.7846e-01, 8.5955e-02, 4.5541e-01],\n",
      "        [1.2341e-02, 1.0370e-03, 1.3637e-01, 8.5025e-01],\n",
      "        [6.6054e-01, 1.2127e-01, 1.1920e-01, 9.8993e-02],\n",
      "        [2.7244e-02, 9.6696e-01, 9.8149e-04, 4.8135e-03],\n",
      "        [9.8870e-01, 8.7468e-04, 7.6140e-03, 2.8114e-03],\n",
      "        [4.0676e-04, 9.9426e-01, 4.7386e-04, 4.8559e-03],\n",
      "        [2.5680e-02, 3.0448e-02, 6.9961e-02, 8.7391e-01],\n",
      "        [1.5011e-03, 8.2133e-01, 2.4638e-03, 1.7470e-01],\n",
      "        [2.9807e-01, 6.9483e-01, 1.8289e-03, 5.2709e-03],\n",
      "        [3.0166e-01, 5.2009e-01, 1.7789e-01, 3.5608e-04],\n",
      "        [7.6189e-02, 7.3751e-01, 1.4311e-01, 4.3192e-02],\n",
      "        [1.3928e-01, 1.0935e-02, 7.8569e-01, 6.4097e-02],\n",
      "        [6.2909e-02, 6.8628e-01, 1.2767e-02, 2.3804e-01],\n",
      "        [1.0992e-01, 5.9082e-02, 6.1152e-03, 8.2489e-01],\n",
      "        [2.1071e-02, 2.6787e-02, 8.8884e-02, 8.6326e-01],\n",
      "        [4.8145e-03, 8.8388e-01, 7.7296e-03, 1.0357e-01],\n",
      "        [2.3566e-01, 4.5036e-01, 2.9966e-01, 1.4315e-02],\n",
      "        [5.0998e-01, 1.8038e-03, 4.5846e-01, 2.9759e-02],\n",
      "        [3.0072e-02, 9.4701e-01, 1.1413e-04, 2.2804e-02],\n",
      "        [1.7356e-02, 5.5164e-03, 9.7470e-01, 2.4298e-03],\n",
      "        [1.6506e-05, 1.6585e-01, 6.4206e-04, 8.3349e-01],\n",
      "        [1.2168e-03, 5.4262e-04, 9.7973e-01, 1.8514e-02],\n",
      "        [8.1279e-01, 4.7767e-02, 1.0420e-01, 3.5239e-02],\n",
      "        [1.8913e-01, 7.2485e-01, 5.4485e-02, 3.1534e-02],\n",
      "        [4.2389e-05, 5.5938e-01, 1.2834e-02, 4.2774e-01],\n",
      "        [3.0661e-04, 5.0177e-04, 9.9803e-01, 1.1611e-03],\n",
      "        [5.7578e-01, 3.1139e-01, 6.9295e-02, 4.3529e-02]])\n"
     ]
    }
   ],
   "source": [
    "p  = torch.softmax( u, dim = 1 )\n",
    "assert p.shape == (n, k), \"p 形状有误\"\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d) 根据 `y` 和 `p` 两个矩阵，即可根据公式得到对数似然函数值。总结上述步骤，编写损失函数 `loss_fn_softmax(w, x, y)`，返回**负**对数似然值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn_softmax(w, x, y):\n",
    "\n",
    "    rho = torch.softmax(torch.matmul(x,w.t()),dim=1)\n",
    "    l = torch.sum(torch.log(rho)*y)/y.shape[0]\n",
    "    return - l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(e) Pytorch 中也提供了 CrossEntropyLoss 损失函数，参见[其文档](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)。其用法是先建立一个损失函数对象，然后将 $U$ 和 $l$ 作为参数传入（注意 $U$ 是经过 Softmax **之前**的矩阵，$l$ 是**原始**的标签）。请利用这种方法计算损失函数值，并与你自己的函数结果进行对比。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.6297)\n",
      "tensor(3.6297)\n"
     ]
    }
   ],
   "source": [
    "ce_softmax = nn.CrossEntropyLoss()\n",
    "loss1 = ce_softmax(u, l)\n",
    "\n",
    "loss2 = loss_fn_softmax(w, x, y)\n",
    "\n",
    "print(loss1)\n",
    "print(loss2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "【说明文字】"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
