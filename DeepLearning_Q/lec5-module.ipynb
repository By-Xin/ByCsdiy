{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自动微分与优化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有了自动微分以后，对一大类机器学习的求解会非常方便。首先考虑一个简单的线性模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "np.random.seed(123456)\n",
    "torch.manual_seed(123456)\n",
    "n = 100\n",
    "p = 5\n",
    "\n",
    "beta = torch.randn(p)\n",
    "x = torch.randn(n, p)\n",
    "y = torch.matmul(x, beta) + torch.randn(n) * math.sqrt(0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们将利用简单的梯度下降对 $\\beta$ 进行优化。首先创建一个初始值并声明需要梯度："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bhat = torch.zeros(p)\n",
    "bhat.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "编写损失函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(bhat, x, y):\n",
    "    yhat = torch.matmul(x, bhat)\n",
    "    return torch.mean(torch.square(y - yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "建立一个循环用来不断更新参数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0, loss = 6.666600227355957, error = 1.300956130027771\n",
      "iteration 50, loss = 0.889494001865387, error = 0.17852994799613953\n",
      "iteration 100, loss = 0.20490001142024994, error = 0.031567562371492386\n",
      "iteration 150, loss = 0.11406505852937698, error = 0.007916359230875969\n",
      "iteration 200, loss = 0.09939470887184143, error = 0.0029288791120052338\n",
      "iteration 250, loss = 0.09633126854896545, error = 0.001613738015294075\n",
      "iteration 300, loss = 0.09553375095129013, error = 0.0012249459978193045\n",
      "iteration 350, loss = 0.09529763460159302, error = 0.0011070544132962823\n",
      "iteration 400, loss = 0.09522352367639542, error = 0.001072814455255866\n",
      "iteration 450, loss = 0.09519965946674347, error = 0.0010642611887305975\n",
      "iteration 500, loss = 0.09519195556640625, error = 0.0010630962206050754\n",
      "iteration 550, loss = 0.09518943727016449, error = 0.0010637094965204597\n",
      "iteration 600, loss = 0.09518859535455704, error = 0.0010645021684467793\n",
      "iteration 650, loss = 0.09518834948539734, error = 0.0010651161428540945\n",
      "iteration 700, loss = 0.09518824517726898, error = 0.001065518124960363\n",
      "iteration 750, loss = 0.09518824517726898, error = 0.0010657599195837975\n",
      "iteration 800, loss = 0.09518823027610779, error = 0.0010659333784133196\n",
      "iteration 850, loss = 0.0951882153749466, error = 0.0010660113766789436\n",
      "iteration 900, loss = 0.095188207924366, error = 0.0010660934494808316\n",
      "iteration 950, loss = 0.095188207924366, error = 0.0010660995030775666\n",
      "iteration 1000, loss = 0.0951882153749466, error = 0.0010661024134606123\n",
      "iteration 1050, loss = 0.0951882153749466, error = 0.0010661041596904397\n",
      "iteration 1100, loss = 0.09518823772668839, error = 0.0010661020642146468\n",
      "iteration 1150, loss = 0.09518823772668839, error = 0.0010661020642146468\n",
      "iteration 1200, loss = 0.09518823772668839, error = 0.0010661020642146468\n",
      "iteration 1250, loss = 0.09518823772668839, error = 0.0010661020642146468\n",
      "iteration 1300, loss = 0.09518823772668839, error = 0.0010661020642146468\n",
      "iteration 1350, loss = 0.09518823772668839, error = 0.0010661020642146468\n",
      "iteration 1400, loss = 0.09518823772668839, error = 0.0010661020642146468\n",
      "iteration 1450, loss = 0.09518823772668839, error = 0.0010661020642146468\n",
      "iteration 1500, loss = 0.09518823772668839, error = 0.0010661020642146468\n",
      "iteration 1550, loss = 0.09518823772668839, error = 0.0010661020642146468\n",
      "iteration 1600, loss = 0.09518823772668839, error = 0.0010661020642146468\n",
      "iteration 1650, loss = 0.09518823772668839, error = 0.0010661020642146468\n",
      "iteration 1700, loss = 0.09518823772668839, error = 0.0010661020642146468\n",
      "iteration 1750, loss = 0.09518823772668839, error = 0.0010661020642146468\n",
      "iteration 1800, loss = 0.09518823772668839, error = 0.0010661020642146468\n",
      "iteration 1850, loss = 0.09518823772668839, error = 0.0010661020642146468\n",
      "iteration 1900, loss = 0.09518823772668839, error = 0.0010661020642146468\n",
      "iteration 1950, loss = 0.09518823772668839, error = 0.0010661020642146468\n",
      "iteration 2000, loss = 0.09518823772668839, error = 0.0010661020642146468\n",
      "iteration 2050, loss = 0.09518823772668839, error = 0.0010661020642146468\n",
      "iteration 2100, loss = 0.09518823772668839, error = 0.0010661020642146468\n",
      "iteration 2150, loss = 0.09518823772668839, error = 0.0010661020642146468\n",
      "iteration 2200, loss = 0.09518823772668839, error = 0.0010661020642146468\n",
      "iteration 2250, loss = 0.09518823772668839, error = 0.0010661020642146468\n",
      "iteration 2300, loss = 0.09518823772668839, error = 0.0010661020642146468\n",
      "iteration 2350, loss = 0.09518823772668839, error = 0.0010661020642146468\n",
      "iteration 2400, loss = 0.09518823772668839, error = 0.0010661020642146468\n",
      "iteration 2450, loss = 0.09518823772668839, error = 0.0010661020642146468\n",
      "iteration 2500, loss = 0.09518823772668839, error = 0.0010661020642146468\n",
      "iteration 2550, loss = 0.09518823772668839, error = 0.0010661020642146468\n",
      "iteration 2600, loss = 0.09518823772668839, error = 0.0010661020642146468\n",
      "iteration 2650, loss = 0.09518823772668839, error = 0.0010661020642146468\n",
      "iteration 2700, loss = 0.09518823772668839, error = 0.0010661020642146468\n",
      "iteration 2750, loss = 0.09518823772668839, error = 0.0010661020642146468\n",
      "iteration 2800, loss = 0.09518823772668839, error = 0.0010661020642146468\n",
      "iteration 2850, loss = 0.09518823772668839, error = 0.0010661020642146468\n",
      "iteration 2900, loss = 0.09518823772668839, error = 0.0010661020642146468\n",
      "iteration 2950, loss = 0.09518823772668839, error = 0.0010661020642146468\n",
      "iteration 3000, loss = 0.09518823772668839, error = 0.0010661020642146468\n",
      "iteration 3050, loss = 0.09518823772668839, error = 0.0010661020642146468\n",
      "iteration 3100, loss = 0.09518823772668839, error = 0.0010661020642146468\n",
      "iteration 3150, loss = 0.09518823772668839, error = 0.0010661020642146468\n",
      "iteration 3200, loss = 0.09518823772668839, error = 0.0010661020642146468\n",
      "iteration 3250, loss = 0.09518823772668839, error = 0.0010661020642146468\n",
      "iteration 3300, loss = 0.09518823772668839, error = 0.0010661020642146468\n",
      "iteration 3350, loss = 0.09518823772668839, error = 0.0010661020642146468\n",
      "iteration 3400, loss = 0.09518823772668839, error = 0.0010661020642146468\n",
      "iteration 3450, loss = 0.09518823772668839, error = 0.0010661020642146468\n",
      "iteration 3500, loss = 0.09518823772668839, error = 0.0010661020642146468\n",
      "iteration 3550, loss = 0.09518823772668839, error = 0.0010661020642146468\n",
      "iteration 3600, loss = 0.09518823772668839, error = 0.0010661020642146468\n",
      "iteration 3650, loss = 0.09518823772668839, error = 0.0010661020642146468\n",
      "iteration 3700, loss = 0.09518823772668839, error = 0.0010661020642146468\n",
      "iteration 3750, loss = 0.09518823772668839, error = 0.0010661020642146468\n",
      "iteration 3800, loss = 0.09518823772668839, error = 0.0010661020642146468\n",
      "iteration 3850, loss = 0.09518823772668839, error = 0.0010661020642146468\n",
      "iteration 3900, loss = 0.09518823772668839, error = 0.0010661020642146468\n",
      "iteration 3950, loss = 0.09518823772668839, error = 0.0010661020642146468\n",
      "iteration 4000, loss = 0.09518823772668839, error = 0.0010661020642146468\n",
      "iteration 4050, loss = 0.09518823772668839, error = 0.0010661020642146468\n",
      "iteration 4100, loss = 0.09518823772668839, error = 0.0010661020642146468\n",
      "iteration 4150, loss = 0.09518823772668839, error = 0.0010661020642146468\n",
      "iteration 4200, loss = 0.09518823772668839, error = 0.0010661020642146468\n",
      "iteration 4250, loss = 0.09518823772668839, error = 0.0010661020642146468\n",
      "iteration 4300, loss = 0.09518823772668839, error = 0.0010661020642146468\n",
      "iteration 4350, loss = 0.09518823772668839, error = 0.0010661020642146468\n",
      "iteration 4400, loss = 0.09518823772668839, error = 0.0010661020642146468\n",
      "iteration 4450, loss = 0.09518823772668839, error = 0.0010661020642146468\n",
      "iteration 4500, loss = 0.09518823772668839, error = 0.0010661020642146468\n",
      "iteration 4550, loss = 0.09518823772668839, error = 0.0010661020642146468\n",
      "iteration 4600, loss = 0.09518823772668839, error = 0.0010661020642146468\n",
      "iteration 4650, loss = 0.09518823772668839, error = 0.0010661020642146468\n",
      "iteration 4700, loss = 0.09518823772668839, error = 0.0010661020642146468\n",
      "iteration 4750, loss = 0.09518823772668839, error = 0.0010661020642146468\n",
      "iteration 4800, loss = 0.09518823772668839, error = 0.0010661020642146468\n",
      "iteration 4850, loss = 0.09518823772668839, error = 0.0010661020642146468\n",
      "iteration 4900, loss = 0.09518823772668839, error = 0.0010661020642146468\n",
      "iteration 4950, loss = 0.09518823772668839, error = 0.0010661020642146468\n"
     ]
    }
   ],
   "source": [
    "# 迭代次数\n",
    "nepoch = 5000\n",
    "# 学习率，即步长\n",
    "learning_rate = 0.01\n",
    "# 记录损失函数值\n",
    "losses = []\n",
    "\n",
    "for i in range(nepoch):\n",
    "    loss = loss_fn(bhat, x, y)\n",
    "\n",
    "    loss.backward()\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    if i % 50 == 0:\n",
    "        print(f\"iteration {i}, loss = {loss.item()}, error = {torch.mean(torch.square(bhat - beta))}\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        bhat -= learning_rate * bhat.grad\n",
    "        \n",
    "        # 清空梯度项\n",
    "        bhat.grad = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "画出损失函数值的图像："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x132e914d0>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pylab as plt\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "比较真实和估计的 $\\beta$："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.8645,  0.4071, -1.1971,  0.3489, -1.1437])\n",
      "tensor([ 1.8306,  0.4251, -1.2020,  0.3770, -1.0885], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(beta)\n",
    "print(bhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模块化编程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch 中一种更常用的模型搭建和求解的方法是利用模块化编程，即将所有的参数装进一个类中，然后通过定义 `forward()` 函数来计算预测值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0.], requires_grad=True), Parameter containing:\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MyModel(nn.Module): # 从nn.Module中继承一些方法\n",
    "    def __init__(self, beta_dim, w_dim): # 初始一些参数，需要传入beta的维度\n",
    "        super(MyModel, self).__init__()\n",
    "        self.bhat = nn.Parameter(torch.zeros(beta_dim)) # 所有加入nn.Para中的内容都令其为torch.zeros()\n",
    "        self.w = nn.Parameter(torch.ones(w_dim))\n",
    "        \n",
    "        # nn.Para可以自动检测class中设置的参数内容\n",
    "\n",
    "    def forward(self, x): # x是数据\n",
    "        yhat = torch.matmul(x, self.bhat) # 这里是线性模型\n",
    "        return yhat\n",
    "\n",
    "np.random.seed(123456)\n",
    "torch.random.manual_seed(123456)\n",
    "\n",
    "model = MyModel(beta_dim=p,w_dim=(5,3)) # 根据MyModel创建了一个对象，并且传入了一些参数\n",
    "print(list(model.parameters())) # 可以看到nn.Para自动将所有参数初始化了 #？这个model.parameters()是从哪里得到的？nn.Parameter吗还是nn.Module？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0, loss = 0.09518823772668839\n",
      "iteration 50, loss = 0.09518823772668839\n",
      "iteration 100, loss = 0.09518823772668839\n",
      "iteration 150, loss = 0.09518823772668839\n",
      "iteration 200, loss = 0.09518823772668839\n",
      "iteration 250, loss = 0.09518823772668839\n",
      "iteration 300, loss = 0.09518823772668839\n",
      "iteration 350, loss = 0.09518823772668839\n",
      "iteration 400, loss = 0.09518823772668839\n",
      "iteration 450, loss = 0.09518823772668839\n",
      "iteration 500, loss = 0.09518823772668839\n",
      "iteration 550, loss = 0.09518823772668839\n",
      "iteration 600, loss = 0.09518823772668839\n",
      "iteration 650, loss = 0.09518823772668839\n",
      "iteration 700, loss = 0.09518823772668839\n",
      "iteration 750, loss = 0.09518823772668839\n",
      "iteration 800, loss = 0.09518823772668839\n",
      "iteration 850, loss = 0.09518823772668839\n",
      "iteration 900, loss = 0.09518823772668839\n",
      "iteration 950, loss = 0.09518823772668839\n",
      "iteration 1000, loss = 0.09518823772668839\n",
      "iteration 1050, loss = 0.09518823772668839\n",
      "iteration 1100, loss = 0.09518823772668839\n",
      "iteration 1150, loss = 0.09518823772668839\n",
      "iteration 1200, loss = 0.09518823772668839\n",
      "iteration 1250, loss = 0.09518823772668839\n",
      "iteration 1300, loss = 0.09518823772668839\n",
      "iteration 1350, loss = 0.09518823772668839\n",
      "iteration 1400, loss = 0.09518823772668839\n",
      "iteration 1450, loss = 0.09518823772668839\n",
      "iteration 1500, loss = 0.09518823772668839\n",
      "iteration 1550, loss = 0.09518823772668839\n",
      "iteration 1600, loss = 0.09518823772668839\n",
      "iteration 1650, loss = 0.09518823772668839\n",
      "iteration 1700, loss = 0.09518823772668839\n",
      "iteration 1750, loss = 0.09518823772668839\n",
      "iteration 1800, loss = 0.09518823772668839\n",
      "iteration 1850, loss = 0.09518823772668839\n",
      "iteration 1900, loss = 0.09518823772668839\n",
      "iteration 1950, loss = 0.09518823772668839\n",
      "iteration 2000, loss = 0.09518823772668839\n",
      "iteration 2050, loss = 0.09518823772668839\n",
      "iteration 2100, loss = 0.09518823772668839\n",
      "iteration 2150, loss = 0.09518823772668839\n",
      "iteration 2200, loss = 0.09518823772668839\n",
      "iteration 2250, loss = 0.09518823772668839\n",
      "iteration 2300, loss = 0.09518823772668839\n",
      "iteration 2350, loss = 0.09518823772668839\n",
      "iteration 2400, loss = 0.09518823772668839\n",
      "iteration 2450, loss = 0.09518823772668839\n",
      "iteration 2500, loss = 0.09518823772668839\n",
      "iteration 2550, loss = 0.09518823772668839\n",
      "iteration 2600, loss = 0.09518823772668839\n",
      "iteration 2650, loss = 0.09518823772668839\n",
      "iteration 2700, loss = 0.09518823772668839\n",
      "iteration 2750, loss = 0.09518823772668839\n",
      "iteration 2800, loss = 0.09518823772668839\n",
      "iteration 2850, loss = 0.09518823772668839\n",
      "iteration 2900, loss = 0.09518823772668839\n",
      "iteration 2950, loss = 0.09518823772668839\n",
      "iteration 3000, loss = 0.09518823772668839\n",
      "iteration 3050, loss = 0.09518823772668839\n",
      "iteration 3100, loss = 0.09518823772668839\n",
      "iteration 3150, loss = 0.09518823772668839\n",
      "iteration 3200, loss = 0.09518823772668839\n",
      "iteration 3250, loss = 0.09518823772668839\n",
      "iteration 3300, loss = 0.09518823772668839\n",
      "iteration 3350, loss = 0.09518823772668839\n",
      "iteration 3400, loss = 0.09518823772668839\n",
      "iteration 3450, loss = 0.09518823772668839\n",
      "iteration 3500, loss = 0.09518823772668839\n",
      "iteration 3550, loss = 0.09518823772668839\n",
      "iteration 3600, loss = 0.09518823772668839\n",
      "iteration 3650, loss = 0.09518823772668839\n",
      "iteration 3700, loss = 0.09518823772668839\n",
      "iteration 3750, loss = 0.09518823772668839\n",
      "iteration 3800, loss = 0.09518823772668839\n",
      "iteration 3850, loss = 0.09518823772668839\n",
      "iteration 3900, loss = 0.09518823772668839\n",
      "iteration 3950, loss = 0.09518823772668839\n",
      "iteration 4000, loss = 0.09518823772668839\n",
      "iteration 4050, loss = 0.09518823772668839\n",
      "iteration 4100, loss = 0.09518823772668839\n",
      "iteration 4150, loss = 0.09518823772668839\n",
      "iteration 4200, loss = 0.09518823772668839\n",
      "iteration 4250, loss = 0.09518823772668839\n",
      "iteration 4300, loss = 0.09518823772668839\n",
      "iteration 4350, loss = 0.09518823772668839\n",
      "iteration 4400, loss = 0.09518823772668839\n",
      "iteration 4450, loss = 0.09518823772668839\n",
      "iteration 4500, loss = 0.09518823772668839\n",
      "iteration 4550, loss = 0.09518823772668839\n",
      "iteration 4600, loss = 0.09518823772668839\n",
      "iteration 4650, loss = 0.09518823772668839\n",
      "iteration 4700, loss = 0.09518823772668839\n",
      "iteration 4750, loss = 0.09518823772668839\n",
      "iteration 4800, loss = 0.09518823772668839\n",
      "iteration 4850, loss = 0.09518823772668839\n",
      "iteration 4900, loss = 0.09518823772668839\n",
      "iteration 4950, loss = 0.09518823772668839\n",
      "iteration 5000, loss = 0.09518823772668839\n",
      "iteration 5050, loss = 0.09518823772668839\n",
      "iteration 5100, loss = 0.09518823772668839\n",
      "iteration 5150, loss = 0.09518823772668839\n",
      "iteration 5200, loss = 0.09518823772668839\n",
      "iteration 5250, loss = 0.09518823772668839\n",
      "iteration 5300, loss = 0.09518823772668839\n",
      "iteration 5350, loss = 0.09518823772668839\n",
      "iteration 5400, loss = 0.09518823772668839\n",
      "iteration 5450, loss = 0.09518823772668839\n",
      "iteration 5500, loss = 0.09518823772668839\n",
      "iteration 5550, loss = 0.09518823772668839\n",
      "iteration 5600, loss = 0.09518823772668839\n",
      "iteration 5650, loss = 0.09518823772668839\n",
      "iteration 5700, loss = 0.09518823772668839\n",
      "iteration 5750, loss = 0.09518823772668839\n",
      "iteration 5800, loss = 0.09518823772668839\n",
      "iteration 5850, loss = 0.09518823772668839\n",
      "iteration 5900, loss = 0.09518823772668839\n",
      "iteration 5950, loss = 0.09518823772668839\n",
      "iteration 6000, loss = 0.09518823772668839\n",
      "iteration 6050, loss = 0.09518823772668839\n",
      "iteration 6100, loss = 0.09518823772668839\n",
      "iteration 6150, loss = 0.09518823772668839\n",
      "iteration 6200, loss = 0.09518823772668839\n",
      "iteration 6250, loss = 0.09518823772668839\n",
      "iteration 6300, loss = 0.09518823772668839\n",
      "iteration 6350, loss = 0.09518823772668839\n",
      "iteration 6400, loss = 0.09518823772668839\n",
      "iteration 6450, loss = 0.09518823772668839\n",
      "iteration 6500, loss = 0.09518823772668839\n",
      "iteration 6550, loss = 0.09518823772668839\n",
      "iteration 6600, loss = 0.09518823772668839\n",
      "iteration 6650, loss = 0.09518823772668839\n",
      "iteration 6700, loss = 0.09518823772668839\n",
      "iteration 6750, loss = 0.09518823772668839\n",
      "iteration 6800, loss = 0.09518823772668839\n",
      "iteration 6850, loss = 0.09518823772668839\n",
      "iteration 6900, loss = 0.09518823772668839\n",
      "iteration 6950, loss = 0.09518823772668839\n",
      "iteration 7000, loss = 0.09518823772668839\n",
      "iteration 7050, loss = 0.09518823772668839\n",
      "iteration 7100, loss = 0.09518823772668839\n",
      "iteration 7150, loss = 0.09518823772668839\n",
      "iteration 7200, loss = 0.09518823772668839\n",
      "iteration 7250, loss = 0.09518823772668839\n",
      "iteration 7300, loss = 0.09518823772668839\n",
      "iteration 7350, loss = 0.09518823772668839\n",
      "iteration 7400, loss = 0.09518823772668839\n",
      "iteration 7450, loss = 0.09518823772668839\n",
      "iteration 7500, loss = 0.09518823772668839\n",
      "iteration 7550, loss = 0.09518823772668839\n",
      "iteration 7600, loss = 0.09518823772668839\n",
      "iteration 7650, loss = 0.09518823772668839\n",
      "iteration 7700, loss = 0.09518823772668839\n",
      "iteration 7750, loss = 0.09518823772668839\n",
      "iteration 7800, loss = 0.09518823772668839\n",
      "iteration 7850, loss = 0.09518823772668839\n",
      "iteration 7900, loss = 0.09518823772668839\n",
      "iteration 7950, loss = 0.09518823772668839\n",
      "iteration 8000, loss = 0.09518823772668839\n",
      "iteration 8050, loss = 0.09518823772668839\n",
      "iteration 8100, loss = 0.09518823772668839\n",
      "iteration 8150, loss = 0.09518823772668839\n",
      "iteration 8200, loss = 0.09518823772668839\n",
      "iteration 8250, loss = 0.09518823772668839\n",
      "iteration 8300, loss = 0.09518823772668839\n",
      "iteration 8350, loss = 0.09518823772668839\n",
      "iteration 8400, loss = 0.09518823772668839\n",
      "iteration 8450, loss = 0.09518823772668839\n",
      "iteration 8500, loss = 0.09518823772668839\n",
      "iteration 8550, loss = 0.09518823772668839\n",
      "iteration 8600, loss = 0.09518823772668839\n",
      "iteration 8650, loss = 0.09518823772668839\n",
      "iteration 8700, loss = 0.09518823772668839\n",
      "iteration 8750, loss = 0.09518823772668839\n",
      "iteration 8800, loss = 0.09518823772668839\n",
      "iteration 8850, loss = 0.09518823772668839\n",
      "iteration 8900, loss = 0.09518823772668839\n",
      "iteration 8950, loss = 0.09518823772668839\n",
      "iteration 9000, loss = 0.09518823772668839\n",
      "iteration 9050, loss = 0.09518823772668839\n",
      "iteration 9100, loss = 0.09518823772668839\n",
      "iteration 9150, loss = 0.09518823772668839\n",
      "iteration 9200, loss = 0.09518823772668839\n",
      "iteration 9250, loss = 0.09518823772668839\n",
      "iteration 9300, loss = 0.09518823772668839\n",
      "iteration 9350, loss = 0.09518823772668839\n",
      "iteration 9400, loss = 0.09518823772668839\n",
      "iteration 9450, loss = 0.09518823772668839\n",
      "iteration 9500, loss = 0.09518823772668839\n",
      "iteration 9550, loss = 0.09518823772668839\n",
      "iteration 9600, loss = 0.09518823772668839\n",
      "iteration 9650, loss = 0.09518823772668839\n",
      "iteration 9700, loss = 0.09518823772668839\n",
      "iteration 9750, loss = 0.09518823772668839\n",
      "iteration 9800, loss = 0.09518823772668839\n",
      "iteration 9850, loss = 0.09518823772668839\n",
      "iteration 9900, loss = 0.09518823772668839\n",
      "iteration 9950, loss = 0.09518823772668839\n",
      "iteration 10000, loss = 0.09518823772668839\n",
      "iteration 10050, loss = 0.09518823772668839\n",
      "iteration 10100, loss = 0.09518823772668839\n",
      "iteration 10150, loss = 0.09518823772668839\n",
      "iteration 10200, loss = 0.09518823772668839\n",
      "iteration 10250, loss = 0.09518823772668839\n",
      "iteration 10300, loss = 0.09518823772668839\n",
      "iteration 10350, loss = 0.09518823772668839\n",
      "iteration 10400, loss = 0.09518823772668839\n",
      "iteration 10450, loss = 0.09518823772668839\n",
      "iteration 10500, loss = 0.09518823772668839\n",
      "iteration 10550, loss = 0.09518823772668839\n",
      "iteration 10600, loss = 0.09518823772668839\n",
      "iteration 10650, loss = 0.09518823772668839\n",
      "iteration 10700, loss = 0.09518823772668839\n",
      "iteration 10750, loss = 0.09518823772668839\n",
      "iteration 10800, loss = 0.09518823772668839\n",
      "iteration 10850, loss = 0.09518823772668839\n",
      "iteration 10900, loss = 0.09518823772668839\n",
      "iteration 10950, loss = 0.09518823772668839\n",
      "iteration 11000, loss = 0.09518823772668839\n",
      "iteration 11050, loss = 0.09518823772668839\n",
      "iteration 11100, loss = 0.09518823772668839\n",
      "iteration 11150, loss = 0.09518823772668839\n",
      "iteration 11200, loss = 0.09518823772668839\n",
      "iteration 11250, loss = 0.09518823772668839\n",
      "iteration 11300, loss = 0.09518823772668839\n",
      "iteration 11350, loss = 0.09518823772668839\n",
      "iteration 11400, loss = 0.09518823772668839\n",
      "iteration 11450, loss = 0.09518823772668839\n",
      "iteration 11500, loss = 0.09518823772668839\n",
      "iteration 11550, loss = 0.09518823772668839\n",
      "iteration 11600, loss = 0.09518823772668839\n",
      "iteration 11650, loss = 0.09518823772668839\n",
      "iteration 11700, loss = 0.09518823772668839\n",
      "iteration 11750, loss = 0.09518823772668839\n",
      "iteration 11800, loss = 0.09518823772668839\n",
      "iteration 11850, loss = 0.09518823772668839\n",
      "iteration 11900, loss = 0.09518823772668839\n",
      "iteration 11950, loss = 0.09518823772668839\n",
      "iteration 12000, loss = 0.09518823772668839\n",
      "iteration 12050, loss = 0.09518823772668839\n",
      "iteration 12100, loss = 0.09518823772668839\n",
      "iteration 12150, loss = 0.09518823772668839\n",
      "iteration 12200, loss = 0.09518823772668839\n",
      "iteration 12250, loss = 0.09518823772668839\n",
      "iteration 12300, loss = 0.09518823772668839\n",
      "iteration 12350, loss = 0.09518823772668839\n",
      "iteration 12400, loss = 0.09518823772668839\n",
      "iteration 12450, loss = 0.09518823772668839\n",
      "iteration 12500, loss = 0.09518823772668839\n",
      "iteration 12550, loss = 0.09518823772668839\n",
      "iteration 12600, loss = 0.09518823772668839\n",
      "iteration 12650, loss = 0.09518823772668839\n",
      "iteration 12700, loss = 0.09518823772668839\n",
      "iteration 12750, loss = 0.09518823772668839\n",
      "iteration 12800, loss = 0.09518823772668839\n",
      "iteration 12850, loss = 0.09518823772668839\n",
      "iteration 12900, loss = 0.09518823772668839\n",
      "iteration 12950, loss = 0.09518823772668839\n",
      "iteration 13000, loss = 0.09518823772668839\n",
      "iteration 13050, loss = 0.09518823772668839\n",
      "iteration 13100, loss = 0.09518823772668839\n",
      "iteration 13150, loss = 0.09518823772668839\n",
      "iteration 13200, loss = 0.09518823772668839\n",
      "iteration 13250, loss = 0.09518823772668839\n",
      "iteration 13300, loss = 0.09518823772668839\n",
      "iteration 13350, loss = 0.09518823772668839\n",
      "iteration 13400, loss = 0.09518823772668839\n",
      "iteration 13450, loss = 0.09518823772668839\n",
      "iteration 13500, loss = 0.09518823772668839\n",
      "iteration 13550, loss = 0.09518823772668839\n",
      "iteration 13600, loss = 0.09518823772668839\n",
      "iteration 13650, loss = 0.09518823772668839\n",
      "iteration 13700, loss = 0.09518823772668839\n",
      "iteration 13750, loss = 0.09518823772668839\n",
      "iteration 13800, loss = 0.09518823772668839\n",
      "iteration 13850, loss = 0.09518823772668839\n",
      "iteration 13900, loss = 0.09518823772668839\n",
      "iteration 13950, loss = 0.09518823772668839\n",
      "iteration 14000, loss = 0.09518823772668839\n",
      "iteration 14050, loss = 0.09518823772668839\n",
      "iteration 14100, loss = 0.09518823772668839\n",
      "iteration 14150, loss = 0.09518823772668839\n",
      "iteration 14200, loss = 0.09518823772668839\n",
      "iteration 14250, loss = 0.09518823772668839\n",
      "iteration 14300, loss = 0.09518823772668839\n",
      "iteration 14350, loss = 0.09518823772668839\n",
      "iteration 14400, loss = 0.09518823772668839\n",
      "iteration 14450, loss = 0.09518823772668839\n",
      "iteration 14500, loss = 0.09518823772668839\n",
      "iteration 14550, loss = 0.09518823772668839\n",
      "iteration 14600, loss = 0.09518823772668839\n",
      "iteration 14650, loss = 0.09518823772668839\n",
      "iteration 14700, loss = 0.09518823772668839\n",
      "iteration 14750, loss = 0.09518823772668839\n",
      "iteration 14800, loss = 0.09518823772668839\n",
      "iteration 14850, loss = 0.09518823772668839\n",
      "iteration 14900, loss = 0.09518823772668839\n",
      "iteration 14950, loss = 0.09518823772668839\n",
      "iteration 15000, loss = 0.09518823772668839\n",
      "iteration 15050, loss = 0.09518823772668839\n",
      "iteration 15100, loss = 0.09518823772668839\n",
      "iteration 15150, loss = 0.09518823772668839\n",
      "iteration 15200, loss = 0.09518823772668839\n",
      "iteration 15250, loss = 0.09518823772668839\n",
      "iteration 15300, loss = 0.09518823772668839\n",
      "iteration 15350, loss = 0.09518823772668839\n",
      "iteration 15400, loss = 0.09518823772668839\n",
      "iteration 15450, loss = 0.09518823772668839\n",
      "iteration 15500, loss = 0.09518823772668839\n",
      "iteration 15550, loss = 0.09518823772668839\n",
      "iteration 15600, loss = 0.09518823772668839\n",
      "iteration 15650, loss = 0.09518823772668839\n",
      "iteration 15700, loss = 0.09518823772668839\n",
      "iteration 15750, loss = 0.09518823772668839\n",
      "iteration 15800, loss = 0.09518823772668839\n",
      "iteration 15850, loss = 0.09518823772668839\n",
      "iteration 15900, loss = 0.09518823772668839\n",
      "iteration 15950, loss = 0.09518823772668839\n",
      "iteration 16000, loss = 0.09518823772668839\n",
      "iteration 16050, loss = 0.09518823772668839\n",
      "iteration 16100, loss = 0.09518823772668839\n",
      "iteration 16150, loss = 0.09518823772668839\n",
      "iteration 16200, loss = 0.09518823772668839\n",
      "iteration 16250, loss = 0.09518823772668839\n",
      "iteration 16300, loss = 0.09518823772668839\n",
      "iteration 16350, loss = 0.09518823772668839\n",
      "iteration 16400, loss = 0.09518823772668839\n",
      "iteration 16450, loss = 0.09518823772668839\n",
      "iteration 16500, loss = 0.09518823772668839\n",
      "iteration 16550, loss = 0.09518823772668839\n",
      "iteration 16600, loss = 0.09518823772668839\n",
      "iteration 16650, loss = 0.09518823772668839\n",
      "iteration 16700, loss = 0.09518823772668839\n",
      "iteration 16750, loss = 0.09518823772668839\n",
      "iteration 16800, loss = 0.09518823772668839\n",
      "iteration 16850, loss = 0.09518823772668839\n",
      "iteration 16900, loss = 0.09518823772668839\n",
      "iteration 16950, loss = 0.09518823772668839\n",
      "iteration 17000, loss = 0.09518823772668839\n",
      "iteration 17050, loss = 0.09518823772668839\n",
      "iteration 17100, loss = 0.09518823772668839\n",
      "iteration 17150, loss = 0.09518823772668839\n",
      "iteration 17200, loss = 0.09518823772668839\n",
      "iteration 17250, loss = 0.09518823772668839\n",
      "iteration 17300, loss = 0.09518823772668839\n",
      "iteration 17350, loss = 0.09518823772668839\n",
      "iteration 17400, loss = 0.09518823772668839\n",
      "iteration 17450, loss = 0.09518823772668839\n",
      "iteration 17500, loss = 0.09518823772668839\n",
      "iteration 17550, loss = 0.09518823772668839\n",
      "iteration 17600, loss = 0.09518823772668839\n",
      "iteration 17650, loss = 0.09518823772668839\n",
      "iteration 17700, loss = 0.09518823772668839\n",
      "iteration 17750, loss = 0.09518823772668839\n",
      "iteration 17800, loss = 0.09518823772668839\n",
      "iteration 17850, loss = 0.09518823772668839\n",
      "iteration 17900, loss = 0.09518823772668839\n",
      "iteration 17950, loss = 0.09518823772668839\n",
      "iteration 18000, loss = 0.09518823772668839\n",
      "iteration 18050, loss = 0.09518823772668839\n",
      "iteration 18100, loss = 0.09518823772668839\n",
      "iteration 18150, loss = 0.09518823772668839\n",
      "iteration 18200, loss = 0.09518823772668839\n",
      "iteration 18250, loss = 0.09518823772668839\n",
      "iteration 18300, loss = 0.09518823772668839\n",
      "iteration 18350, loss = 0.09518823772668839\n",
      "iteration 18400, loss = 0.09518823772668839\n",
      "iteration 18450, loss = 0.09518823772668839\n",
      "iteration 18500, loss = 0.09518823772668839\n",
      "iteration 18550, loss = 0.09518823772668839\n",
      "iteration 18600, loss = 0.09518823772668839\n",
      "iteration 18650, loss = 0.09518823772668839\n",
      "iteration 18700, loss = 0.09518823772668839\n",
      "iteration 18750, loss = 0.09518823772668839\n",
      "iteration 18800, loss = 0.09518823772668839\n",
      "iteration 18850, loss = 0.09518823772668839\n",
      "iteration 18900, loss = 0.09518823772668839\n",
      "iteration 18950, loss = 0.09518823772668839\n",
      "iteration 19000, loss = 0.09518823772668839\n",
      "iteration 19050, loss = 0.09518823772668839\n",
      "iteration 19100, loss = 0.09518823772668839\n",
      "iteration 19150, loss = 0.09518823772668839\n",
      "iteration 19200, loss = 0.09518823772668839\n",
      "iteration 19250, loss = 0.09518823772668839\n",
      "iteration 19300, loss = 0.09518823772668839\n",
      "iteration 19350, loss = 0.09518823772668839\n",
      "iteration 19400, loss = 0.09518823772668839\n",
      "iteration 19450, loss = 0.09518823772668839\n",
      "iteration 19500, loss = 0.09518823772668839\n",
      "iteration 19550, loss = 0.09518823772668839\n",
      "iteration 19600, loss = 0.09518823772668839\n",
      "iteration 19650, loss = 0.09518823772668839\n",
      "iteration 19700, loss = 0.09518823772668839\n",
      "iteration 19750, loss = 0.09518823772668839\n",
      "iteration 19800, loss = 0.09518823772668839\n",
      "iteration 19850, loss = 0.09518823772668839\n",
      "iteration 19900, loss = 0.09518823772668839\n",
      "iteration 19950, loss = 0.09518823772668839\n",
      "iteration 20000, loss = 0.09518823772668839\n",
      "iteration 20050, loss = 0.09518823772668839\n",
      "iteration 20100, loss = 0.09518823772668839\n",
      "iteration 20150, loss = 0.09518823772668839\n",
      "iteration 20200, loss = 0.09518823772668839\n",
      "iteration 20250, loss = 0.09518823772668839\n",
      "iteration 20300, loss = 0.09518823772668839\n",
      "iteration 20350, loss = 0.09518823772668839\n",
      "iteration 20400, loss = 0.09518823772668839\n",
      "iteration 20450, loss = 0.09518823772668839\n",
      "iteration 20500, loss = 0.09518823772668839\n",
      "iteration 20550, loss = 0.09518823772668839\n",
      "iteration 20600, loss = 0.09518823772668839\n",
      "iteration 20650, loss = 0.09518823772668839\n",
      "iteration 20700, loss = 0.09518823772668839\n",
      "iteration 20750, loss = 0.09518823772668839\n",
      "iteration 20800, loss = 0.09518823772668839\n",
      "iteration 20850, loss = 0.09518823772668839\n",
      "iteration 20900, loss = 0.09518823772668839\n",
      "iteration 20950, loss = 0.09518823772668839\n",
      "iteration 21000, loss = 0.09518823772668839\n",
      "iteration 21050, loss = 0.09518823772668839\n",
      "iteration 21100, loss = 0.09518823772668839\n",
      "iteration 21150, loss = 0.09518823772668839\n",
      "iteration 21200, loss = 0.09518823772668839\n",
      "iteration 21250, loss = 0.09518823772668839\n",
      "iteration 21300, loss = 0.09518823772668839\n",
      "iteration 21350, loss = 0.09518823772668839\n",
      "iteration 21400, loss = 0.09518823772668839\n",
      "iteration 21450, loss = 0.09518823772668839\n",
      "iteration 21500, loss = 0.09518823772668839\n",
      "iteration 21550, loss = 0.09518823772668839\n",
      "iteration 21600, loss = 0.09518823772668839\n",
      "iteration 21650, loss = 0.09518823772668839\n",
      "iteration 21700, loss = 0.09518823772668839\n",
      "iteration 21750, loss = 0.09518823772668839\n",
      "iteration 21800, loss = 0.09518823772668839\n",
      "iteration 21850, loss = 0.09518823772668839\n",
      "iteration 21900, loss = 0.09518823772668839\n",
      "iteration 21950, loss = 0.09518823772668839\n",
      "iteration 22000, loss = 0.09518823772668839\n",
      "iteration 22050, loss = 0.09518823772668839\n",
      "iteration 22100, loss = 0.09518823772668839\n",
      "iteration 22150, loss = 0.09518823772668839\n",
      "iteration 22200, loss = 0.09518823772668839\n",
      "iteration 22250, loss = 0.09518823772668839\n",
      "iteration 22300, loss = 0.09518823772668839\n",
      "iteration 22350, loss = 0.09518823772668839\n",
      "iteration 22400, loss = 0.09518823772668839\n",
      "iteration 22450, loss = 0.09518823772668839\n",
      "iteration 22500, loss = 0.09518823772668839\n",
      "iteration 22550, loss = 0.09518823772668839\n",
      "iteration 22600, loss = 0.09518823772668839\n",
      "iteration 22650, loss = 0.09518823772668839\n",
      "iteration 22700, loss = 0.09518823772668839\n",
      "iteration 22750, loss = 0.09518823772668839\n",
      "iteration 22800, loss = 0.09518823772668839\n",
      "iteration 22850, loss = 0.09518823772668839\n",
      "iteration 22900, loss = 0.09518823772668839\n",
      "iteration 22950, loss = 0.09518823772668839\n",
      "iteration 23000, loss = 0.09518823772668839\n",
      "iteration 23050, loss = 0.09518823772668839\n",
      "iteration 23100, loss = 0.09518823772668839\n",
      "iteration 23150, loss = 0.09518823772668839\n",
      "iteration 23200, loss = 0.09518823772668839\n",
      "iteration 23250, loss = 0.09518823772668839\n",
      "iteration 23300, loss = 0.09518823772668839\n",
      "iteration 23350, loss = 0.09518823772668839\n",
      "iteration 23400, loss = 0.09518823772668839\n",
      "iteration 23450, loss = 0.09518823772668839\n",
      "iteration 23500, loss = 0.09518823772668839\n",
      "iteration 23550, loss = 0.09518823772668839\n",
      "iteration 23600, loss = 0.09518823772668839\n",
      "iteration 23650, loss = 0.09518823772668839\n",
      "iteration 23700, loss = 0.09518823772668839\n",
      "iteration 23750, loss = 0.09518823772668839\n",
      "iteration 23800, loss = 0.09518823772668839\n",
      "iteration 23850, loss = 0.09518823772668839\n",
      "iteration 23900, loss = 0.09518823772668839\n",
      "iteration 23950, loss = 0.09518823772668839\n",
      "iteration 24000, loss = 0.09518823772668839\n",
      "iteration 24050, loss = 0.09518823772668839\n",
      "iteration 24100, loss = 0.09518823772668839\n",
      "iteration 24150, loss = 0.09518823772668839\n",
      "iteration 24200, loss = 0.09518823772668839\n",
      "iteration 24250, loss = 0.09518823772668839\n",
      "iteration 24300, loss = 0.09518823772668839\n",
      "iteration 24350, loss = 0.09518823772668839\n",
      "iteration 24400, loss = 0.09518823772668839\n",
      "iteration 24450, loss = 0.09518823772668839\n",
      "iteration 24500, loss = 0.09518823772668839\n",
      "iteration 24550, loss = 0.09518823772668839\n",
      "iteration 24600, loss = 0.09518823772668839\n",
      "iteration 24650, loss = 0.09518823772668839\n",
      "iteration 24700, loss = 0.09518823772668839\n",
      "iteration 24750, loss = 0.09518823772668839\n",
      "iteration 24800, loss = 0.09518823772668839\n",
      "iteration 24850, loss = 0.09518823772668839\n",
      "iteration 24900, loss = 0.09518823772668839\n",
      "iteration 24950, loss = 0.09518823772668839\n",
      "iteration 25000, loss = 0.09518823772668839\n",
      "iteration 25050, loss = 0.09518823772668839\n",
      "iteration 25100, loss = 0.09518823772668839\n",
      "iteration 25150, loss = 0.09518823772668839\n",
      "iteration 25200, loss = 0.09518823772668839\n",
      "iteration 25250, loss = 0.09518823772668839\n",
      "iteration 25300, loss = 0.09518823772668839\n",
      "iteration 25350, loss = 0.09518823772668839\n",
      "iteration 25400, loss = 0.09518823772668839\n",
      "iteration 25450, loss = 0.09518823772668839\n",
      "iteration 25500, loss = 0.09518823772668839\n",
      "iteration 25550, loss = 0.09518823772668839\n",
      "iteration 25600, loss = 0.09518823772668839\n",
      "iteration 25650, loss = 0.09518823772668839\n",
      "iteration 25700, loss = 0.09518823772668839\n",
      "iteration 25750, loss = 0.09518823772668839\n",
      "iteration 25800, loss = 0.09518823772668839\n",
      "iteration 25850, loss = 0.09518823772668839\n",
      "iteration 25900, loss = 0.09518823772668839\n",
      "iteration 25950, loss = 0.09518823772668839\n",
      "iteration 26000, loss = 0.09518823772668839\n",
      "iteration 26050, loss = 0.09518823772668839\n",
      "iteration 26100, loss = 0.09518823772668839\n",
      "iteration 26150, loss = 0.09518823772668839\n",
      "iteration 26200, loss = 0.09518823772668839\n",
      "iteration 26250, loss = 0.09518823772668839\n",
      "iteration 26300, loss = 0.09518823772668839\n",
      "iteration 26350, loss = 0.09518823772668839\n",
      "iteration 26400, loss = 0.09518823772668839\n",
      "iteration 26450, loss = 0.09518823772668839\n",
      "iteration 26500, loss = 0.09518823772668839\n",
      "iteration 26550, loss = 0.09518823772668839\n",
      "iteration 26600, loss = 0.09518823772668839\n",
      "iteration 26650, loss = 0.09518823772668839\n",
      "iteration 26700, loss = 0.09518823772668839\n",
      "iteration 26750, loss = 0.09518823772668839\n",
      "iteration 26800, loss = 0.09518823772668839\n",
      "iteration 26850, loss = 0.09518823772668839\n",
      "iteration 26900, loss = 0.09518823772668839\n",
      "iteration 26950, loss = 0.09518823772668839\n",
      "iteration 27000, loss = 0.09518823772668839\n",
      "iteration 27050, loss = 0.09518823772668839\n",
      "iteration 27100, loss = 0.09518823772668839\n",
      "iteration 27150, loss = 0.09518823772668839\n",
      "iteration 27200, loss = 0.09518823772668839\n",
      "iteration 27250, loss = 0.09518823772668839\n",
      "iteration 27300, loss = 0.09518823772668839\n",
      "iteration 27350, loss = 0.09518823772668839\n",
      "iteration 27400, loss = 0.09518823772668839\n",
      "iteration 27450, loss = 0.09518823772668839\n",
      "iteration 27500, loss = 0.09518823772668839\n",
      "iteration 27550, loss = 0.09518823772668839\n",
      "iteration 27600, loss = 0.09518823772668839\n",
      "iteration 27650, loss = 0.09518823772668839\n",
      "iteration 27700, loss = 0.09518823772668839\n",
      "iteration 27750, loss = 0.09518823772668839\n",
      "iteration 27800, loss = 0.09518823772668839\n",
      "iteration 27850, loss = 0.09518823772668839\n",
      "iteration 27900, loss = 0.09518823772668839\n",
      "iteration 27950, loss = 0.09518823772668839\n",
      "iteration 28000, loss = 0.09518823772668839\n",
      "iteration 28050, loss = 0.09518823772668839\n",
      "iteration 28100, loss = 0.09518823772668839\n",
      "iteration 28150, loss = 0.09518823772668839\n",
      "iteration 28200, loss = 0.09518823772668839\n",
      "iteration 28250, loss = 0.09518823772668839\n",
      "iteration 28300, loss = 0.09518823772668839\n",
      "iteration 28350, loss = 0.09518823772668839\n",
      "iteration 28400, loss = 0.09518823772668839\n",
      "iteration 28450, loss = 0.09518823772668839\n",
      "iteration 28500, loss = 0.09518823772668839\n",
      "iteration 28550, loss = 0.09518823772668839\n",
      "iteration 28600, loss = 0.09518823772668839\n",
      "iteration 28650, loss = 0.09518823772668839\n",
      "iteration 28700, loss = 0.09518823772668839\n",
      "iteration 28750, loss = 0.09518823772668839\n",
      "iteration 28800, loss = 0.09518823772668839\n",
      "iteration 28850, loss = 0.09518823772668839\n",
      "iteration 28900, loss = 0.09518823772668839\n",
      "iteration 28950, loss = 0.09518823772668839\n",
      "iteration 29000, loss = 0.09518823772668839\n",
      "iteration 29050, loss = 0.09518823772668839\n",
      "iteration 29100, loss = 0.09518823772668839\n",
      "iteration 29150, loss = 0.09518823772668839\n",
      "iteration 29200, loss = 0.09518823772668839\n",
      "iteration 29250, loss = 0.09518823772668839\n",
      "iteration 29300, loss = 0.09518823772668839\n",
      "iteration 29350, loss = 0.09518823772668839\n",
      "iteration 29400, loss = 0.09518823772668839\n",
      "iteration 29450, loss = 0.09518823772668839\n",
      "iteration 29500, loss = 0.09518823772668839\n",
      "iteration 29550, loss = 0.09518823772668839\n",
      "iteration 29600, loss = 0.09518823772668839\n",
      "iteration 29650, loss = 0.09518823772668839\n",
      "iteration 29700, loss = 0.09518823772668839\n",
      "iteration 29750, loss = 0.09518823772668839\n",
      "iteration 29800, loss = 0.09518823772668839\n",
      "iteration 29850, loss = 0.09518823772668839\n",
      "iteration 29900, loss = 0.09518823772668839\n",
      "iteration 29950, loss = 0.09518823772668839\n",
      "iteration 30000, loss = 0.09518823772668839\n",
      "iteration 30050, loss = 0.09518823772668839\n",
      "iteration 30100, loss = 0.09518823772668839\n",
      "iteration 30150, loss = 0.09518823772668839\n",
      "iteration 30200, loss = 0.09518823772668839\n",
      "iteration 30250, loss = 0.09518823772668839\n",
      "iteration 30300, loss = 0.09518823772668839\n",
      "iteration 30350, loss = 0.09518823772668839\n",
      "iteration 30400, loss = 0.09518823772668839\n",
      "iteration 30450, loss = 0.09518823772668839\n",
      "iteration 30500, loss = 0.09518823772668839\n",
      "iteration 30550, loss = 0.09518823772668839\n",
      "iteration 30600, loss = 0.09518823772668839\n",
      "iteration 30650, loss = 0.09518823772668839\n",
      "iteration 30700, loss = 0.09518823772668839\n",
      "iteration 30750, loss = 0.09518823772668839\n",
      "iteration 30800, loss = 0.09518823772668839\n",
      "iteration 30850, loss = 0.09518823772668839\n",
      "iteration 30900, loss = 0.09518823772668839\n",
      "iteration 30950, loss = 0.09518823772668839\n",
      "iteration 31000, loss = 0.09518823772668839\n",
      "iteration 31050, loss = 0.09518823772668839\n",
      "iteration 31100, loss = 0.09518823772668839\n",
      "iteration 31150, loss = 0.09518823772668839\n",
      "iteration 31200, loss = 0.09518823772668839\n",
      "iteration 31250, loss = 0.09518823772668839\n",
      "iteration 31300, loss = 0.09518823772668839\n",
      "iteration 31350, loss = 0.09518823772668839\n",
      "iteration 31400, loss = 0.09518823772668839\n",
      "iteration 31450, loss = 0.09518823772668839\n",
      "iteration 31500, loss = 0.09518823772668839\n",
      "iteration 31550, loss = 0.09518823772668839\n",
      "iteration 31600, loss = 0.09518823772668839\n",
      "iteration 31650, loss = 0.09518823772668839\n",
      "iteration 31700, loss = 0.09518823772668839\n",
      "iteration 31750, loss = 0.09518823772668839\n",
      "iteration 31800, loss = 0.09518823772668839\n",
      "iteration 31850, loss = 0.09518823772668839\n",
      "iteration 31900, loss = 0.09518823772668839\n",
      "iteration 31950, loss = 0.09518823772668839\n",
      "iteration 32000, loss = 0.09518823772668839\n",
      "iteration 32050, loss = 0.09518823772668839\n",
      "iteration 32100, loss = 0.09518823772668839\n",
      "iteration 32150, loss = 0.09518823772668839\n",
      "iteration 32200, loss = 0.09518823772668839\n",
      "iteration 32250, loss = 0.09518823772668839\n",
      "iteration 32300, loss = 0.09518823772668839\n",
      "iteration 32350, loss = 0.09518823772668839\n",
      "iteration 32400, loss = 0.09518823772668839\n",
      "iteration 32450, loss = 0.09518823772668839\n",
      "iteration 32500, loss = 0.09518823772668839\n",
      "iteration 32550, loss = 0.09518823772668839\n",
      "iteration 32600, loss = 0.09518823772668839\n",
      "iteration 32650, loss = 0.09518823772668839\n",
      "iteration 32700, loss = 0.09518823772668839\n",
      "iteration 32750, loss = 0.09518823772668839\n",
      "iteration 32800, loss = 0.09518823772668839\n",
      "iteration 32850, loss = 0.09518823772668839\n",
      "iteration 32900, loss = 0.09518823772668839\n",
      "iteration 32950, loss = 0.09518823772668839\n",
      "iteration 33000, loss = 0.09518823772668839\n",
      "iteration 33050, loss = 0.09518823772668839\n",
      "iteration 33100, loss = 0.09518823772668839\n",
      "iteration 33150, loss = 0.09518823772668839\n",
      "iteration 33200, loss = 0.09518823772668839\n",
      "iteration 33250, loss = 0.09518823772668839\n",
      "iteration 33300, loss = 0.09518823772668839\n",
      "iteration 33350, loss = 0.09518823772668839\n",
      "iteration 33400, loss = 0.09518823772668839\n",
      "iteration 33450, loss = 0.09518823772668839\n",
      "iteration 33500, loss = 0.09518823772668839\n",
      "iteration 33550, loss = 0.09518823772668839\n",
      "iteration 33600, loss = 0.09518823772668839\n",
      "iteration 33650, loss = 0.09518823772668839\n",
      "iteration 33700, loss = 0.09518823772668839\n",
      "iteration 33750, loss = 0.09518823772668839\n",
      "iteration 33800, loss = 0.09518823772668839\n",
      "iteration 33850, loss = 0.09518823772668839\n",
      "iteration 33900, loss = 0.09518823772668839\n",
      "iteration 33950, loss = 0.09518823772668839\n",
      "iteration 34000, loss = 0.09518823772668839\n",
      "iteration 34050, loss = 0.09518823772668839\n",
      "iteration 34100, loss = 0.09518823772668839\n",
      "iteration 34150, loss = 0.09518823772668839\n",
      "iteration 34200, loss = 0.09518823772668839\n",
      "iteration 34250, loss = 0.09518823772668839\n",
      "iteration 34300, loss = 0.09518823772668839\n",
      "iteration 34350, loss = 0.09518823772668839\n",
      "iteration 34400, loss = 0.09518823772668839\n",
      "iteration 34450, loss = 0.09518823772668839\n",
      "iteration 34500, loss = 0.09518823772668839\n",
      "iteration 34550, loss = 0.09518823772668839\n",
      "iteration 34600, loss = 0.09518823772668839\n",
      "iteration 34650, loss = 0.09518823772668839\n",
      "iteration 34700, loss = 0.09518823772668839\n",
      "iteration 34750, loss = 0.09518823772668839\n",
      "iteration 34800, loss = 0.09518823772668839\n",
      "iteration 34850, loss = 0.09518823772668839\n",
      "iteration 34900, loss = 0.09518823772668839\n",
      "iteration 34950, loss = 0.09518823772668839\n",
      "iteration 35000, loss = 0.09518823772668839\n",
      "iteration 35050, loss = 0.09518823772668839\n",
      "iteration 35100, loss = 0.09518823772668839\n",
      "iteration 35150, loss = 0.09518823772668839\n",
      "iteration 35200, loss = 0.09518823772668839\n",
      "iteration 35250, loss = 0.09518823772668839\n",
      "iteration 35300, loss = 0.09518823772668839\n",
      "iteration 35350, loss = 0.09518823772668839\n",
      "iteration 35400, loss = 0.09518823772668839\n",
      "iteration 35450, loss = 0.09518823772668839\n",
      "iteration 35500, loss = 0.09518823772668839\n",
      "iteration 35550, loss = 0.09518823772668839\n",
      "iteration 35600, loss = 0.09518823772668839\n",
      "iteration 35650, loss = 0.09518823772668839\n",
      "iteration 35700, loss = 0.09518823772668839\n",
      "iteration 35750, loss = 0.09518823772668839\n",
      "iteration 35800, loss = 0.09518823772668839\n",
      "iteration 35850, loss = 0.09518823772668839\n",
      "iteration 35900, loss = 0.09518823772668839\n",
      "iteration 35950, loss = 0.09518823772668839\n",
      "iteration 36000, loss = 0.09518823772668839\n",
      "iteration 36050, loss = 0.09518823772668839\n",
      "iteration 36100, loss = 0.09518823772668839\n",
      "iteration 36150, loss = 0.09518823772668839\n",
      "iteration 36200, loss = 0.09518823772668839\n",
      "iteration 36250, loss = 0.09518823772668839\n",
      "iteration 36300, loss = 0.09518823772668839\n",
      "iteration 36350, loss = 0.09518823772668839\n",
      "iteration 36400, loss = 0.09518823772668839\n",
      "iteration 36450, loss = 0.09518823772668839\n",
      "iteration 36500, loss = 0.09518823772668839\n",
      "iteration 36550, loss = 0.09518823772668839\n",
      "iteration 36600, loss = 0.09518823772668839\n",
      "iteration 36650, loss = 0.09518823772668839\n",
      "iteration 36700, loss = 0.09518823772668839\n",
      "iteration 36750, loss = 0.09518823772668839\n",
      "iteration 36800, loss = 0.09518823772668839\n",
      "iteration 36850, loss = 0.09518823772668839\n",
      "iteration 36900, loss = 0.09518823772668839\n",
      "iteration 36950, loss = 0.09518823772668839\n",
      "iteration 37000, loss = 0.09518823772668839\n",
      "iteration 37050, loss = 0.09518823772668839\n",
      "iteration 37100, loss = 0.09518823772668839\n",
      "iteration 37150, loss = 0.09518823772668839\n",
      "iteration 37200, loss = 0.09518823772668839\n",
      "iteration 37250, loss = 0.09518823772668839\n",
      "iteration 37300, loss = 0.09518823772668839\n",
      "iteration 37350, loss = 0.09518823772668839\n",
      "iteration 37400, loss = 0.09518823772668839\n",
      "iteration 37450, loss = 0.09518823772668839\n",
      "iteration 37500, loss = 0.09518823772668839\n",
      "iteration 37550, loss = 0.09518823772668839\n",
      "iteration 37600, loss = 0.09518823772668839\n",
      "iteration 37650, loss = 0.09518823772668839\n",
      "iteration 37700, loss = 0.09518823772668839\n",
      "iteration 37750, loss = 0.09518823772668839\n",
      "iteration 37800, loss = 0.09518823772668839\n",
      "iteration 37850, loss = 0.09518823772668839\n",
      "iteration 37900, loss = 0.09518823772668839\n",
      "iteration 37950, loss = 0.09518823772668839\n",
      "iteration 38000, loss = 0.09518823772668839\n",
      "iteration 38050, loss = 0.09518823772668839\n",
      "iteration 38100, loss = 0.09518823772668839\n",
      "iteration 38150, loss = 0.09518823772668839\n",
      "iteration 38200, loss = 0.09518823772668839\n",
      "iteration 38250, loss = 0.09518823772668839\n",
      "iteration 38300, loss = 0.09518823772668839\n",
      "iteration 38350, loss = 0.09518823772668839\n",
      "iteration 38400, loss = 0.09518823772668839\n",
      "iteration 38450, loss = 0.09518823772668839\n",
      "iteration 38500, loss = 0.09518823772668839\n",
      "iteration 38550, loss = 0.09518823772668839\n",
      "iteration 38600, loss = 0.09518823772668839\n",
      "iteration 38650, loss = 0.09518823772668839\n",
      "iteration 38700, loss = 0.09518823772668839\n",
      "iteration 38750, loss = 0.09518823772668839\n",
      "iteration 38800, loss = 0.09518823772668839\n",
      "iteration 38850, loss = 0.09518823772668839\n",
      "iteration 38900, loss = 0.09518823772668839\n",
      "iteration 38950, loss = 0.09518823772668839\n",
      "iteration 39000, loss = 0.09518823772668839\n",
      "iteration 39050, loss = 0.09518823772668839\n",
      "iteration 39100, loss = 0.09518823772668839\n",
      "iteration 39150, loss = 0.09518823772668839\n",
      "iteration 39200, loss = 0.09518823772668839\n",
      "iteration 39250, loss = 0.09518823772668839\n",
      "iteration 39300, loss = 0.09518823772668839\n",
      "iteration 39350, loss = 0.09518823772668839\n",
      "iteration 39400, loss = 0.09518823772668839\n",
      "iteration 39450, loss = 0.09518823772668839\n",
      "iteration 39500, loss = 0.09518823772668839\n",
      "iteration 39550, loss = 0.09518823772668839\n",
      "iteration 39600, loss = 0.09518823772668839\n",
      "iteration 39650, loss = 0.09518823772668839\n",
      "iteration 39700, loss = 0.09518823772668839\n",
      "iteration 39750, loss = 0.09518823772668839\n",
      "iteration 39800, loss = 0.09518823772668839\n",
      "iteration 39850, loss = 0.09518823772668839\n",
      "iteration 39900, loss = 0.09518823772668839\n",
      "iteration 39950, loss = 0.09518823772668839\n",
      "iteration 40000, loss = 0.09518823772668839\n",
      "iteration 40050, loss = 0.09518823772668839\n",
      "iteration 40100, loss = 0.09518823772668839\n",
      "iteration 40150, loss = 0.09518823772668839\n",
      "iteration 40200, loss = 0.09518823772668839\n",
      "iteration 40250, loss = 0.09518823772668839\n",
      "iteration 40300, loss = 0.09518823772668839\n",
      "iteration 40350, loss = 0.09518823772668839\n",
      "iteration 40400, loss = 0.09518823772668839\n",
      "iteration 40450, loss = 0.09518823772668839\n",
      "iteration 40500, loss = 0.09518823772668839\n",
      "iteration 40550, loss = 0.09518823772668839\n",
      "iteration 40600, loss = 0.09518823772668839\n",
      "iteration 40650, loss = 0.09518823772668839\n",
      "iteration 40700, loss = 0.09518823772668839\n",
      "iteration 40750, loss = 0.09518823772668839\n",
      "iteration 40800, loss = 0.09518823772668839\n",
      "iteration 40850, loss = 0.09518823772668839\n",
      "iteration 40900, loss = 0.09518823772668839\n",
      "iteration 40950, loss = 0.09518823772668839\n",
      "iteration 41000, loss = 0.09518823772668839\n",
      "iteration 41050, loss = 0.09518823772668839\n",
      "iteration 41100, loss = 0.09518823772668839\n",
      "iteration 41150, loss = 0.09518823772668839\n",
      "iteration 41200, loss = 0.09518823772668839\n",
      "iteration 41250, loss = 0.09518823772668839\n",
      "iteration 41300, loss = 0.09518823772668839\n",
      "iteration 41350, loss = 0.09518823772668839\n",
      "iteration 41400, loss = 0.09518823772668839\n",
      "iteration 41450, loss = 0.09518823772668839\n",
      "iteration 41500, loss = 0.09518823772668839\n",
      "iteration 41550, loss = 0.09518823772668839\n",
      "iteration 41600, loss = 0.09518823772668839\n",
      "iteration 41650, loss = 0.09518823772668839\n",
      "iteration 41700, loss = 0.09518823772668839\n",
      "iteration 41750, loss = 0.09518823772668839\n",
      "iteration 41800, loss = 0.09518823772668839\n",
      "iteration 41850, loss = 0.09518823772668839\n",
      "iteration 41900, loss = 0.09518823772668839\n",
      "iteration 41950, loss = 0.09518823772668839\n",
      "iteration 42000, loss = 0.09518823772668839\n",
      "iteration 42050, loss = 0.09518823772668839\n",
      "iteration 42100, loss = 0.09518823772668839\n",
      "iteration 42150, loss = 0.09518823772668839\n",
      "iteration 42200, loss = 0.09518823772668839\n",
      "iteration 42250, loss = 0.09518823772668839\n",
      "iteration 42300, loss = 0.09518823772668839\n",
      "iteration 42350, loss = 0.09518823772668839\n",
      "iteration 42400, loss = 0.09518823772668839\n",
      "iteration 42450, loss = 0.09518823772668839\n",
      "iteration 42500, loss = 0.09518823772668839\n",
      "iteration 42550, loss = 0.09518823772668839\n",
      "iteration 42600, loss = 0.09518823772668839\n",
      "iteration 42650, loss = 0.09518823772668839\n",
      "iteration 42700, loss = 0.09518823772668839\n",
      "iteration 42750, loss = 0.09518823772668839\n",
      "iteration 42800, loss = 0.09518823772668839\n",
      "iteration 42850, loss = 0.09518823772668839\n",
      "iteration 42900, loss = 0.09518823772668839\n",
      "iteration 42950, loss = 0.09518823772668839\n",
      "iteration 43000, loss = 0.09518823772668839\n",
      "iteration 43050, loss = 0.09518823772668839\n",
      "iteration 43100, loss = 0.09518823772668839\n",
      "iteration 43150, loss = 0.09518823772668839\n",
      "iteration 43200, loss = 0.09518823772668839\n",
      "iteration 43250, loss = 0.09518823772668839\n",
      "iteration 43300, loss = 0.09518823772668839\n",
      "iteration 43350, loss = 0.09518823772668839\n",
      "iteration 43400, loss = 0.09518823772668839\n",
      "iteration 43450, loss = 0.09518823772668839\n",
      "iteration 43500, loss = 0.09518823772668839\n",
      "iteration 43550, loss = 0.09518823772668839\n",
      "iteration 43600, loss = 0.09518823772668839\n",
      "iteration 43650, loss = 0.09518823772668839\n",
      "iteration 43700, loss = 0.09518823772668839\n",
      "iteration 43750, loss = 0.09518823772668839\n",
      "iteration 43800, loss = 0.09518823772668839\n",
      "iteration 43850, loss = 0.09518823772668839\n",
      "iteration 43900, loss = 0.09518823772668839\n",
      "iteration 43950, loss = 0.09518823772668839\n",
      "iteration 44000, loss = 0.09518823772668839\n",
      "iteration 44050, loss = 0.09518823772668839\n",
      "iteration 44100, loss = 0.09518823772668839\n",
      "iteration 44150, loss = 0.09518823772668839\n",
      "iteration 44200, loss = 0.09518823772668839\n",
      "iteration 44250, loss = 0.09518823772668839\n",
      "iteration 44300, loss = 0.09518823772668839\n",
      "iteration 44350, loss = 0.09518823772668839\n",
      "iteration 44400, loss = 0.09518823772668839\n",
      "iteration 44450, loss = 0.09518823772668839\n",
      "iteration 44500, loss = 0.09518823772668839\n",
      "iteration 44550, loss = 0.09518823772668839\n",
      "iteration 44600, loss = 0.09518823772668839\n",
      "iteration 44650, loss = 0.09518823772668839\n",
      "iteration 44700, loss = 0.09518823772668839\n",
      "iteration 44750, loss = 0.09518823772668839\n",
      "iteration 44800, loss = 0.09518823772668839\n",
      "iteration 44850, loss = 0.09518823772668839\n",
      "iteration 44900, loss = 0.09518823772668839\n",
      "iteration 44950, loss = 0.09518823772668839\n",
      "iteration 45000, loss = 0.09518823772668839\n",
      "iteration 45050, loss = 0.09518823772668839\n",
      "iteration 45100, loss = 0.09518823772668839\n",
      "iteration 45150, loss = 0.09518823772668839\n",
      "iteration 45200, loss = 0.09518823772668839\n",
      "iteration 45250, loss = 0.09518823772668839\n",
      "iteration 45300, loss = 0.09518823772668839\n",
      "iteration 45350, loss = 0.09518823772668839\n",
      "iteration 45400, loss = 0.09518823772668839\n",
      "iteration 45450, loss = 0.09518823772668839\n",
      "iteration 45500, loss = 0.09518823772668839\n",
      "iteration 45550, loss = 0.09518823772668839\n",
      "iteration 45600, loss = 0.09518823772668839\n",
      "iteration 45650, loss = 0.09518823772668839\n",
      "iteration 45700, loss = 0.09518823772668839\n",
      "iteration 45750, loss = 0.09518823772668839\n",
      "iteration 45800, loss = 0.09518823772668839\n",
      "iteration 45850, loss = 0.09518823772668839\n",
      "iteration 45900, loss = 0.09518823772668839\n",
      "iteration 45950, loss = 0.09518823772668839\n",
      "iteration 46000, loss = 0.09518823772668839\n",
      "iteration 46050, loss = 0.09518823772668839\n",
      "iteration 46100, loss = 0.09518823772668839\n",
      "iteration 46150, loss = 0.09518823772668839\n",
      "iteration 46200, loss = 0.09518823772668839\n",
      "iteration 46250, loss = 0.09518823772668839\n",
      "iteration 46300, loss = 0.09518823772668839\n",
      "iteration 46350, loss = 0.09518823772668839\n",
      "iteration 46400, loss = 0.09518823772668839\n",
      "iteration 46450, loss = 0.09518823772668839\n",
      "iteration 46500, loss = 0.09518823772668839\n",
      "iteration 46550, loss = 0.09518823772668839\n",
      "iteration 46600, loss = 0.09518823772668839\n",
      "iteration 46650, loss = 0.09518823772668839\n",
      "iteration 46700, loss = 0.09518823772668839\n",
      "iteration 46750, loss = 0.09518823772668839\n",
      "iteration 46800, loss = 0.09518823772668839\n",
      "iteration 46850, loss = 0.09518823772668839\n",
      "iteration 46900, loss = 0.09518823772668839\n",
      "iteration 46950, loss = 0.09518823772668839\n",
      "iteration 47000, loss = 0.09518823772668839\n",
      "iteration 47050, loss = 0.09518823772668839\n",
      "iteration 47100, loss = 0.09518823772668839\n",
      "iteration 47150, loss = 0.09518823772668839\n",
      "iteration 47200, loss = 0.09518823772668839\n",
      "iteration 47250, loss = 0.09518823772668839\n",
      "iteration 47300, loss = 0.09518823772668839\n",
      "iteration 47350, loss = 0.09518823772668839\n",
      "iteration 47400, loss = 0.09518823772668839\n",
      "iteration 47450, loss = 0.09518823772668839\n",
      "iteration 47500, loss = 0.09518823772668839\n",
      "iteration 47550, loss = 0.09518823772668839\n",
      "iteration 47600, loss = 0.09518823772668839\n",
      "iteration 47650, loss = 0.09518823772668839\n",
      "iteration 47700, loss = 0.09518823772668839\n",
      "iteration 47750, loss = 0.09518823772668839\n",
      "iteration 47800, loss = 0.09518823772668839\n",
      "iteration 47850, loss = 0.09518823772668839\n",
      "iteration 47900, loss = 0.09518823772668839\n",
      "iteration 47950, loss = 0.09518823772668839\n",
      "iteration 48000, loss = 0.09518823772668839\n",
      "iteration 48050, loss = 0.09518823772668839\n",
      "iteration 48100, loss = 0.09518823772668839\n",
      "iteration 48150, loss = 0.09518823772668839\n",
      "iteration 48200, loss = 0.09518823772668839\n",
      "iteration 48250, loss = 0.09518823772668839\n",
      "iteration 48300, loss = 0.09518823772668839\n",
      "iteration 48350, loss = 0.09518823772668839\n",
      "iteration 48400, loss = 0.09518823772668839\n",
      "iteration 48450, loss = 0.09518823772668839\n",
      "iteration 48500, loss = 0.09518823772668839\n",
      "iteration 48550, loss = 0.09518823772668839\n",
      "iteration 48600, loss = 0.09518823772668839\n",
      "iteration 48650, loss = 0.09518823772668839\n",
      "iteration 48700, loss = 0.09518823772668839\n",
      "iteration 48750, loss = 0.09518823772668839\n",
      "iteration 48800, loss = 0.09518823772668839\n",
      "iteration 48850, loss = 0.09518823772668839\n",
      "iteration 48900, loss = 0.09518823772668839\n",
      "iteration 48950, loss = 0.09518823772668839\n",
      "iteration 49000, loss = 0.09518823772668839\n",
      "iteration 49050, loss = 0.09518823772668839\n",
      "iteration 49100, loss = 0.09518823772668839\n",
      "iteration 49150, loss = 0.09518823772668839\n",
      "iteration 49200, loss = 0.09518823772668839\n",
      "iteration 49250, loss = 0.09518823772668839\n",
      "iteration 49300, loss = 0.09518823772668839\n",
      "iteration 49350, loss = 0.09518823772668839\n",
      "iteration 49400, loss = 0.09518823772668839\n",
      "iteration 49450, loss = 0.09518823772668839\n",
      "iteration 49500, loss = 0.09518823772668839\n",
      "iteration 49550, loss = 0.09518823772668839\n",
      "iteration 49600, loss = 0.09518823772668839\n",
      "iteration 49650, loss = 0.09518823772668839\n",
      "iteration 49700, loss = 0.09518823772668839\n",
      "iteration 49750, loss = 0.09518823772668839\n",
      "iteration 49800, loss = 0.09518823772668839\n",
      "iteration 49850, loss = 0.09518823772668839\n",
      "iteration 49900, loss = 0.09518823772668839\n",
      "iteration 49950, loss = 0.09518823772668839\n",
      "iteration 50000, loss = 0.09518823772668839\n",
      "iteration 50050, loss = 0.09518823772668839\n",
      "iteration 50100, loss = 0.09518823772668839\n",
      "iteration 50150, loss = 0.09518823772668839\n",
      "iteration 50200, loss = 0.09518823772668839\n",
      "iteration 50250, loss = 0.09518823772668839\n",
      "iteration 50300, loss = 0.09518823772668839\n",
      "iteration 50350, loss = 0.09518823772668839\n",
      "iteration 50400, loss = 0.09518823772668839\n",
      "iteration 50450, loss = 0.09518823772668839\n",
      "iteration 50500, loss = 0.09518823772668839\n",
      "iteration 50550, loss = 0.09518823772668839\n",
      "iteration 50600, loss = 0.09518823772668839\n",
      "iteration 50650, loss = 0.09518823772668839\n",
      "iteration 50700, loss = 0.09518823772668839\n",
      "iteration 50750, loss = 0.09518823772668839\n",
      "iteration 50800, loss = 0.09518823772668839\n",
      "iteration 50850, loss = 0.09518823772668839\n",
      "iteration 50900, loss = 0.09518823772668839\n",
      "iteration 50950, loss = 0.09518823772668839\n",
      "iteration 51000, loss = 0.09518823772668839\n",
      "iteration 51050, loss = 0.09518823772668839\n",
      "iteration 51100, loss = 0.09518823772668839\n",
      "iteration 51150, loss = 0.09518823772668839\n",
      "iteration 51200, loss = 0.09518823772668839\n",
      "iteration 51250, loss = 0.09518823772668839\n",
      "iteration 51300, loss = 0.09518823772668839\n",
      "iteration 51350, loss = 0.09518823772668839\n",
      "iteration 51400, loss = 0.09518823772668839\n",
      "iteration 51450, loss = 0.09518823772668839\n",
      "iteration 51500, loss = 0.09518823772668839\n",
      "iteration 51550, loss = 0.09518823772668839\n",
      "iteration 51600, loss = 0.09518823772668839\n",
      "iteration 51650, loss = 0.09518823772668839\n",
      "iteration 51700, loss = 0.09518823772668839\n",
      "iteration 51750, loss = 0.09518823772668839\n",
      "iteration 51800, loss = 0.09518823772668839\n",
      "iteration 51850, loss = 0.09518823772668839\n",
      "iteration 51900, loss = 0.09518823772668839\n",
      "iteration 51950, loss = 0.09518823772668839\n",
      "iteration 52000, loss = 0.09518823772668839\n",
      "iteration 52050, loss = 0.09518823772668839\n",
      "iteration 52100, loss = 0.09518823772668839\n",
      "iteration 52150, loss = 0.09518823772668839\n",
      "iteration 52200, loss = 0.09518823772668839\n",
      "iteration 52250, loss = 0.09518823772668839\n",
      "iteration 52300, loss = 0.09518823772668839\n",
      "iteration 52350, loss = 0.09518823772668839\n",
      "iteration 52400, loss = 0.09518823772668839\n",
      "iteration 52450, loss = 0.09518823772668839\n",
      "iteration 52500, loss = 0.09518823772668839\n",
      "iteration 52550, loss = 0.09518823772668839\n",
      "iteration 52600, loss = 0.09518823772668839\n",
      "iteration 52650, loss = 0.09518823772668839\n",
      "iteration 52700, loss = 0.09518823772668839\n",
      "iteration 52750, loss = 0.09518823772668839\n",
      "iteration 52800, loss = 0.09518823772668839\n",
      "iteration 52850, loss = 0.09518823772668839\n",
      "iteration 52900, loss = 0.09518823772668839\n",
      "iteration 52950, loss = 0.09518823772668839\n",
      "iteration 53000, loss = 0.09518823772668839\n",
      "iteration 53050, loss = 0.09518823772668839\n",
      "iteration 53100, loss = 0.09518823772668839\n",
      "iteration 53150, loss = 0.09518823772668839\n",
      "iteration 53200, loss = 0.09518823772668839\n",
      "iteration 53250, loss = 0.09518823772668839\n",
      "iteration 53300, loss = 0.09518823772668839\n",
      "iteration 53350, loss = 0.09518823772668839\n",
      "iteration 53400, loss = 0.09518823772668839\n",
      "iteration 53450, loss = 0.09518823772668839\n",
      "iteration 53500, loss = 0.09518823772668839\n",
      "iteration 53550, loss = 0.09518823772668839\n",
      "iteration 53600, loss = 0.09518823772668839\n",
      "iteration 53650, loss = 0.09518823772668839\n",
      "iteration 53700, loss = 0.09518823772668839\n",
      "iteration 53750, loss = 0.09518823772668839\n",
      "iteration 53800, loss = 0.09518823772668839\n",
      "iteration 53850, loss = 0.09518823772668839\n",
      "iteration 53900, loss = 0.09518823772668839\n",
      "iteration 53950, loss = 0.09518823772668839\n",
      "iteration 54000, loss = 0.09518823772668839\n",
      "iteration 54050, loss = 0.09518823772668839\n",
      "iteration 54100, loss = 0.09518823772668839\n",
      "iteration 54150, loss = 0.09518823772668839\n",
      "iteration 54200, loss = 0.09518823772668839\n",
      "iteration 54250, loss = 0.09518823772668839\n",
      "iteration 54300, loss = 0.09518823772668839\n",
      "iteration 54350, loss = 0.09518823772668839\n",
      "iteration 54400, loss = 0.09518823772668839\n",
      "iteration 54450, loss = 0.09518823772668839\n",
      "iteration 54500, loss = 0.09518823772668839\n",
      "iteration 54550, loss = 0.09518823772668839\n",
      "iteration 54600, loss = 0.09518823772668839\n",
      "iteration 54650, loss = 0.09518823772668839\n",
      "iteration 54700, loss = 0.09518823772668839\n",
      "iteration 54750, loss = 0.09518823772668839\n",
      "iteration 54800, loss = 0.09518823772668839\n",
      "iteration 54850, loss = 0.09518823772668839\n",
      "iteration 54900, loss = 0.09518823772668839\n",
      "iteration 54950, loss = 0.09518823772668839\n",
      "iteration 55000, loss = 0.09518823772668839\n",
      "iteration 55050, loss = 0.09518823772668839\n",
      "iteration 55100, loss = 0.09518823772668839\n",
      "iteration 55150, loss = 0.09518823772668839\n",
      "iteration 55200, loss = 0.09518823772668839\n",
      "iteration 55250, loss = 0.09518823772668839\n",
      "iteration 55300, loss = 0.09518823772668839\n",
      "iteration 55350, loss = 0.09518823772668839\n",
      "iteration 55400, loss = 0.09518823772668839\n",
      "iteration 55450, loss = 0.09518823772668839\n",
      "iteration 55500, loss = 0.09518823772668839\n",
      "iteration 55550, loss = 0.09518823772668839\n",
      "iteration 55600, loss = 0.09518823772668839\n",
      "iteration 55650, loss = 0.09518823772668839\n",
      "iteration 55700, loss = 0.09518823772668839\n",
      "iteration 55750, loss = 0.09518823772668839\n",
      "iteration 55800, loss = 0.09518823772668839\n",
      "iteration 55850, loss = 0.09518823772668839\n",
      "iteration 55900, loss = 0.09518823772668839\n",
      "iteration 55950, loss = 0.09518823772668839\n",
      "iteration 56000, loss = 0.09518823772668839\n",
      "iteration 56050, loss = 0.09518823772668839\n",
      "iteration 56100, loss = 0.09518823772668839\n",
      "iteration 56150, loss = 0.09518823772668839\n",
      "iteration 56200, loss = 0.09518823772668839\n",
      "iteration 56250, loss = 0.09518823772668839\n",
      "iteration 56300, loss = 0.09518823772668839\n",
      "iteration 56350, loss = 0.09518823772668839\n",
      "iteration 56400, loss = 0.09518823772668839\n",
      "iteration 56450, loss = 0.09518823772668839\n",
      "iteration 56500, loss = 0.09518823772668839\n",
      "iteration 56550, loss = 0.09518823772668839\n",
      "iteration 56600, loss = 0.09518823772668839\n",
      "iteration 56650, loss = 0.09518823772668839\n",
      "iteration 56700, loss = 0.09518823772668839\n",
      "iteration 56750, loss = 0.09518823772668839\n",
      "iteration 56800, loss = 0.09518823772668839\n",
      "iteration 56850, loss = 0.09518823772668839\n",
      "iteration 56900, loss = 0.09518823772668839\n",
      "iteration 56950, loss = 0.09518823772668839\n",
      "iteration 57000, loss = 0.09518823772668839\n",
      "iteration 57050, loss = 0.09518823772668839\n",
      "iteration 57100, loss = 0.09518823772668839\n",
      "iteration 57150, loss = 0.09518823772668839\n",
      "iteration 57200, loss = 0.09518823772668839\n",
      "iteration 57250, loss = 0.09518823772668839\n",
      "iteration 57300, loss = 0.09518823772668839\n",
      "iteration 57350, loss = 0.09518823772668839\n",
      "iteration 57400, loss = 0.09518823772668839\n",
      "iteration 57450, loss = 0.09518823772668839\n",
      "iteration 57500, loss = 0.09518823772668839\n",
      "iteration 57550, loss = 0.09518823772668839\n",
      "iteration 57600, loss = 0.09518823772668839\n",
      "iteration 57650, loss = 0.09518823772668839\n",
      "iteration 57700, loss = 0.09518823772668839\n",
      "iteration 57750, loss = 0.09518823772668839\n",
      "iteration 57800, loss = 0.09518823772668839\n",
      "iteration 57850, loss = 0.09518823772668839\n",
      "iteration 57900, loss = 0.09518823772668839\n",
      "iteration 57950, loss = 0.09518823772668839\n",
      "iteration 58000, loss = 0.09518823772668839\n",
      "iteration 58050, loss = 0.09518823772668839\n",
      "iteration 58100, loss = 0.09518823772668839\n",
      "iteration 58150, loss = 0.09518823772668839\n",
      "iteration 58200, loss = 0.09518823772668839\n",
      "iteration 58250, loss = 0.09518823772668839\n",
      "iteration 58300, loss = 0.09518823772668839\n",
      "iteration 58350, loss = 0.09518823772668839\n",
      "iteration 58400, loss = 0.09518823772668839\n",
      "iteration 58450, loss = 0.09518823772668839\n",
      "iteration 58500, loss = 0.09518823772668839\n",
      "iteration 58550, loss = 0.09518823772668839\n",
      "iteration 58600, loss = 0.09518823772668839\n",
      "iteration 58650, loss = 0.09518823772668839\n",
      "iteration 58700, loss = 0.09518823772668839\n",
      "iteration 58750, loss = 0.09518823772668839\n",
      "iteration 58800, loss = 0.09518823772668839\n",
      "iteration 58850, loss = 0.09518823772668839\n",
      "iteration 58900, loss = 0.09518823772668839\n",
      "iteration 58950, loss = 0.09518823772668839\n",
      "iteration 59000, loss = 0.09518823772668839\n",
      "iteration 59050, loss = 0.09518823772668839\n",
      "iteration 59100, loss = 0.09518823772668839\n",
      "iteration 59150, loss = 0.09518823772668839\n",
      "iteration 59200, loss = 0.09518823772668839\n",
      "iteration 59250, loss = 0.09518823772668839\n",
      "iteration 59300, loss = 0.09518823772668839\n",
      "iteration 59350, loss = 0.09518823772668839\n",
      "iteration 59400, loss = 0.09518823772668839\n",
      "iteration 59450, loss = 0.09518823772668839\n",
      "iteration 59500, loss = 0.09518823772668839\n",
      "iteration 59550, loss = 0.09518823772668839\n",
      "iteration 59600, loss = 0.09518823772668839\n",
      "iteration 59650, loss = 0.09518823772668839\n",
      "iteration 59700, loss = 0.09518823772668839\n",
      "iteration 59750, loss = 0.09518823772668839\n",
      "iteration 59800, loss = 0.09518823772668839\n",
      "iteration 59850, loss = 0.09518823772668839\n",
      "iteration 59900, loss = 0.09518823772668839\n",
      "iteration 59950, loss = 0.09518823772668839\n",
      "iteration 60000, loss = 0.09518823772668839\n",
      "iteration 60050, loss = 0.09518823772668839\n",
      "iteration 60100, loss = 0.09518823772668839\n",
      "iteration 60150, loss = 0.09518823772668839\n",
      "iteration 60200, loss = 0.09518823772668839\n",
      "iteration 60250, loss = 0.09518823772668839\n",
      "iteration 60300, loss = 0.09518823772668839\n",
      "iteration 60350, loss = 0.09518823772668839\n",
      "iteration 60400, loss = 0.09518823772668839\n",
      "iteration 60450, loss = 0.09518823772668839\n",
      "iteration 60500, loss = 0.09518823772668839\n",
      "iteration 60550, loss = 0.09518823772668839\n",
      "iteration 60600, loss = 0.09518823772668839\n",
      "iteration 60650, loss = 0.09518823772668839\n",
      "iteration 60700, loss = 0.09518823772668839\n",
      "iteration 60750, loss = 0.09518823772668839\n",
      "iteration 60800, loss = 0.09518823772668839\n",
      "iteration 60850, loss = 0.09518823772668839\n",
      "iteration 60900, loss = 0.09518823772668839\n",
      "iteration 60950, loss = 0.09518823772668839\n",
      "iteration 61000, loss = 0.09518823772668839\n",
      "iteration 61050, loss = 0.09518823772668839\n",
      "iteration 61100, loss = 0.09518823772668839\n",
      "iteration 61150, loss = 0.09518823772668839\n",
      "iteration 61200, loss = 0.09518823772668839\n",
      "iteration 61250, loss = 0.09518823772668839\n",
      "iteration 61300, loss = 0.09518823772668839\n",
      "iteration 61350, loss = 0.09518823772668839\n",
      "iteration 61400, loss = 0.09518823772668839\n",
      "iteration 61450, loss = 0.09518823772668839\n",
      "iteration 61500, loss = 0.09518823772668839\n",
      "iteration 61550, loss = 0.09518823772668839\n",
      "iteration 61600, loss = 0.09518823772668839\n",
      "iteration 61650, loss = 0.09518823772668839\n",
      "iteration 61700, loss = 0.09518823772668839\n",
      "iteration 61750, loss = 0.09518823772668839\n",
      "iteration 61800, loss = 0.09518823772668839\n",
      "iteration 61850, loss = 0.09518823772668839\n",
      "iteration 61900, loss = 0.09518823772668839\n",
      "iteration 61950, loss = 0.09518823772668839\n",
      "iteration 62000, loss = 0.09518823772668839\n",
      "iteration 62050, loss = 0.09518823772668839\n",
      "iteration 62100, loss = 0.09518823772668839\n",
      "iteration 62150, loss = 0.09518823772668839\n",
      "iteration 62200, loss = 0.09518823772668839\n",
      "iteration 62250, loss = 0.09518823772668839\n",
      "iteration 62300, loss = 0.09518823772668839\n",
      "iteration 62350, loss = 0.09518823772668839\n",
      "iteration 62400, loss = 0.09518823772668839\n",
      "iteration 62450, loss = 0.09518823772668839\n",
      "iteration 62500, loss = 0.09518823772668839\n",
      "iteration 62550, loss = 0.09518823772668839\n",
      "iteration 62600, loss = 0.09518823772668839\n",
      "iteration 62650, loss = 0.09518823772668839\n",
      "iteration 62700, loss = 0.09518823772668839\n",
      "iteration 62750, loss = 0.09518823772668839\n",
      "iteration 62800, loss = 0.09518823772668839\n",
      "iteration 62850, loss = 0.09518823772668839\n",
      "iteration 62900, loss = 0.09518823772668839\n",
      "iteration 62950, loss = 0.09518823772668839\n",
      "iteration 63000, loss = 0.09518823772668839\n",
      "iteration 63050, loss = 0.09518823772668839\n",
      "iteration 63100, loss = 0.09518823772668839\n",
      "iteration 63150, loss = 0.09518823772668839\n",
      "iteration 63200, loss = 0.09518823772668839\n",
      "iteration 63250, loss = 0.09518823772668839\n",
      "iteration 63300, loss = 0.09518823772668839\n",
      "iteration 63350, loss = 0.09518823772668839\n",
      "iteration 63400, loss = 0.09518823772668839\n",
      "iteration 63450, loss = 0.09518823772668839\n",
      "iteration 63500, loss = 0.09518823772668839\n",
      "iteration 63550, loss = 0.09518823772668839\n",
      "iteration 63600, loss = 0.09518823772668839\n",
      "iteration 63650, loss = 0.09518823772668839\n",
      "iteration 63700, loss = 0.09518823772668839\n",
      "iteration 63750, loss = 0.09518823772668839\n",
      "iteration 63800, loss = 0.09518823772668839\n",
      "iteration 63850, loss = 0.09518823772668839\n",
      "iteration 63900, loss = 0.09518823772668839\n",
      "iteration 63950, loss = 0.09518823772668839\n",
      "iteration 64000, loss = 0.09518823772668839\n",
      "iteration 64050, loss = 0.09518823772668839\n",
      "iteration 64100, loss = 0.09518823772668839\n",
      "iteration 64150, loss = 0.09518823772668839\n",
      "iteration 64200, loss = 0.09518823772668839\n",
      "iteration 64250, loss = 0.09518823772668839\n",
      "iteration 64300, loss = 0.09518823772668839\n",
      "iteration 64350, loss = 0.09518823772668839\n",
      "iteration 64400, loss = 0.09518823772668839\n",
      "iteration 64450, loss = 0.09518823772668839\n",
      "iteration 64500, loss = 0.09518823772668839\n",
      "iteration 64550, loss = 0.09518823772668839\n",
      "iteration 64600, loss = 0.09518823772668839\n",
      "iteration 64650, loss = 0.09518823772668839\n",
      "iteration 64700, loss = 0.09518823772668839\n",
      "iteration 64750, loss = 0.09518823772668839\n",
      "iteration 64800, loss = 0.09518823772668839\n",
      "iteration 64850, loss = 0.09518823772668839\n",
      "iteration 64900, loss = 0.09518823772668839\n",
      "iteration 64950, loss = 0.09518823772668839\n",
      "iteration 65000, loss = 0.09518823772668839\n",
      "iteration 65050, loss = 0.09518823772668839\n",
      "iteration 65100, loss = 0.09518823772668839\n",
      "iteration 65150, loss = 0.09518823772668839\n",
      "iteration 65200, loss = 0.09518823772668839\n",
      "iteration 65250, loss = 0.09518823772668839\n",
      "iteration 65300, loss = 0.09518823772668839\n",
      "iteration 65350, loss = 0.09518823772668839\n",
      "iteration 65400, loss = 0.09518823772668839\n",
      "iteration 65450, loss = 0.09518823772668839\n",
      "iteration 65500, loss = 0.09518823772668839\n",
      "iteration 65550, loss = 0.09518823772668839\n",
      "iteration 65600, loss = 0.09518823772668839\n",
      "iteration 65650, loss = 0.09518823772668839\n",
      "iteration 65700, loss = 0.09518823772668839\n",
      "iteration 65750, loss = 0.09518823772668839\n",
      "iteration 65800, loss = 0.09518823772668839\n",
      "iteration 65850, loss = 0.09518823772668839\n",
      "iteration 65900, loss = 0.09518823772668839\n",
      "iteration 65950, loss = 0.09518823772668839\n",
      "iteration 66000, loss = 0.09518823772668839\n",
      "iteration 66050, loss = 0.09518823772668839\n",
      "iteration 66100, loss = 0.09518823772668839\n",
      "iteration 66150, loss = 0.09518823772668839\n",
      "iteration 66200, loss = 0.09518823772668839\n",
      "iteration 66250, loss = 0.09518823772668839\n",
      "iteration 66300, loss = 0.09518823772668839\n",
      "iteration 66350, loss = 0.09518823772668839\n",
      "iteration 66400, loss = 0.09518823772668839\n",
      "iteration 66450, loss = 0.09518823772668839\n",
      "iteration 66500, loss = 0.09518823772668839\n",
      "iteration 66550, loss = 0.09518823772668839\n",
      "iteration 66600, loss = 0.09518823772668839\n",
      "iteration 66650, loss = 0.09518823772668839\n",
      "iteration 66700, loss = 0.09518823772668839\n",
      "iteration 66750, loss = 0.09518823772668839\n",
      "iteration 66800, loss = 0.09518823772668839\n",
      "iteration 66850, loss = 0.09518823772668839\n",
      "iteration 66900, loss = 0.09518823772668839\n",
      "iteration 66950, loss = 0.09518823772668839\n",
      "iteration 67000, loss = 0.09518823772668839\n",
      "iteration 67050, loss = 0.09518823772668839\n",
      "iteration 67100, loss = 0.09518823772668839\n",
      "iteration 67150, loss = 0.09518823772668839\n",
      "iteration 67200, loss = 0.09518823772668839\n",
      "iteration 67250, loss = 0.09518823772668839\n",
      "iteration 67300, loss = 0.09518823772668839\n",
      "iteration 67350, loss = 0.09518823772668839\n",
      "iteration 67400, loss = 0.09518823772668839\n",
      "iteration 67450, loss = 0.09518823772668839\n",
      "iteration 67500, loss = 0.09518823772668839\n",
      "iteration 67550, loss = 0.09518823772668839\n",
      "iteration 67600, loss = 0.09518823772668839\n",
      "iteration 67650, loss = 0.09518823772668839\n",
      "iteration 67700, loss = 0.09518823772668839\n",
      "iteration 67750, loss = 0.09518823772668839\n",
      "iteration 67800, loss = 0.09518823772668839\n",
      "iteration 67850, loss = 0.09518823772668839\n",
      "iteration 67900, loss = 0.09518823772668839\n",
      "iteration 67950, loss = 0.09518823772668839\n",
      "iteration 68000, loss = 0.09518823772668839\n",
      "iteration 68050, loss = 0.09518823772668839\n",
      "iteration 68100, loss = 0.09518823772668839\n",
      "iteration 68150, loss = 0.09518823772668839\n",
      "iteration 68200, loss = 0.09518823772668839\n",
      "iteration 68250, loss = 0.09518823772668839\n",
      "iteration 68300, loss = 0.09518823772668839\n",
      "iteration 68350, loss = 0.09518823772668839\n",
      "iteration 68400, loss = 0.09518823772668839\n",
      "iteration 68450, loss = 0.09518823772668839\n",
      "iteration 68500, loss = 0.09518823772668839\n",
      "iteration 68550, loss = 0.09518823772668839\n",
      "iteration 68600, loss = 0.09518823772668839\n",
      "iteration 68650, loss = 0.09518823772668839\n",
      "iteration 68700, loss = 0.09518823772668839\n",
      "iteration 68750, loss = 0.09518823772668839\n",
      "iteration 68800, loss = 0.09518823772668839\n",
      "iteration 68850, loss = 0.09518823772668839\n",
      "iteration 68900, loss = 0.09518823772668839\n",
      "iteration 68950, loss = 0.09518823772668839\n",
      "iteration 69000, loss = 0.09518823772668839\n",
      "iteration 69050, loss = 0.09518823772668839\n",
      "iteration 69100, loss = 0.09518823772668839\n",
      "iteration 69150, loss = 0.09518823772668839\n",
      "iteration 69200, loss = 0.09518823772668839\n",
      "iteration 69250, loss = 0.09518823772668839\n",
      "iteration 69300, loss = 0.09518823772668839\n",
      "iteration 69350, loss = 0.09518823772668839\n",
      "iteration 69400, loss = 0.09518823772668839\n",
      "iteration 69450, loss = 0.09518823772668839\n",
      "iteration 69500, loss = 0.09518823772668839\n",
      "iteration 69550, loss = 0.09518823772668839\n",
      "iteration 69600, loss = 0.09518823772668839\n",
      "iteration 69650, loss = 0.09518823772668839\n",
      "iteration 69700, loss = 0.09518823772668839\n",
      "iteration 69750, loss = 0.09518823772668839\n",
      "iteration 69800, loss = 0.09518823772668839\n",
      "iteration 69850, loss = 0.09518823772668839\n",
      "iteration 69900, loss = 0.09518823772668839\n",
      "iteration 69950, loss = 0.09518823772668839\n",
      "iteration 70000, loss = 0.09518823772668839\n",
      "iteration 70050, loss = 0.09518823772668839\n",
      "iteration 70100, loss = 0.09518823772668839\n",
      "iteration 70150, loss = 0.09518823772668839\n",
      "iteration 70200, loss = 0.09518823772668839\n",
      "iteration 70250, loss = 0.09518823772668839\n",
      "iteration 70300, loss = 0.09518823772668839\n",
      "iteration 70350, loss = 0.09518823772668839\n",
      "iteration 70400, loss = 0.09518823772668839\n",
      "iteration 70450, loss = 0.09518823772668839\n",
      "iteration 70500, loss = 0.09518823772668839\n",
      "iteration 70550, loss = 0.09518823772668839\n",
      "iteration 70600, loss = 0.09518823772668839\n",
      "iteration 70650, loss = 0.09518823772668839\n",
      "iteration 70700, loss = 0.09518823772668839\n",
      "iteration 70750, loss = 0.09518823772668839\n",
      "iteration 70800, loss = 0.09518823772668839\n",
      "iteration 70850, loss = 0.09518823772668839\n",
      "iteration 70900, loss = 0.09518823772668839\n",
      "iteration 70950, loss = 0.09518823772668839\n",
      "iteration 71000, loss = 0.09518823772668839\n",
      "iteration 71050, loss = 0.09518823772668839\n",
      "iteration 71100, loss = 0.09518823772668839\n",
      "iteration 71150, loss = 0.09518823772668839\n",
      "iteration 71200, loss = 0.09518823772668839\n",
      "iteration 71250, loss = 0.09518823772668839\n",
      "iteration 71300, loss = 0.09518823772668839\n",
      "iteration 71350, loss = 0.09518823772668839\n",
      "iteration 71400, loss = 0.09518823772668839\n",
      "iteration 71450, loss = 0.09518823772668839\n",
      "iteration 71500, loss = 0.09518823772668839\n",
      "iteration 71550, loss = 0.09518823772668839\n",
      "iteration 71600, loss = 0.09518823772668839\n",
      "iteration 71650, loss = 0.09518823772668839\n",
      "iteration 71700, loss = 0.09518823772668839\n",
      "iteration 71750, loss = 0.09518823772668839\n",
      "iteration 71800, loss = 0.09518823772668839\n",
      "iteration 71850, loss = 0.09518823772668839\n",
      "iteration 71900, loss = 0.09518823772668839\n",
      "iteration 71950, loss = 0.09518823772668839\n",
      "iteration 72000, loss = 0.09518823772668839\n",
      "iteration 72050, loss = 0.09518823772668839\n",
      "iteration 72100, loss = 0.09518823772668839\n",
      "iteration 72150, loss = 0.09518823772668839\n",
      "iteration 72200, loss = 0.09518823772668839\n",
      "iteration 72250, loss = 0.09518823772668839\n",
      "iteration 72300, loss = 0.09518823772668839\n",
      "iteration 72350, loss = 0.09518823772668839\n",
      "iteration 72400, loss = 0.09518823772668839\n",
      "iteration 72450, loss = 0.09518823772668839\n",
      "iteration 72500, loss = 0.09518823772668839\n",
      "iteration 72550, loss = 0.09518823772668839\n",
      "iteration 72600, loss = 0.09518823772668839\n",
      "iteration 72650, loss = 0.09518823772668839\n",
      "iteration 72700, loss = 0.09518823772668839\n",
      "iteration 72750, loss = 0.09518823772668839\n",
      "iteration 72800, loss = 0.09518823772668839\n",
      "iteration 72850, loss = 0.09518823772668839\n",
      "iteration 72900, loss = 0.09518823772668839\n",
      "iteration 72950, loss = 0.09518823772668839\n",
      "iteration 73000, loss = 0.09518823772668839\n",
      "iteration 73050, loss = 0.09518823772668839\n",
      "iteration 73100, loss = 0.09518823772668839\n",
      "iteration 73150, loss = 0.09518823772668839\n",
      "iteration 73200, loss = 0.09518823772668839\n",
      "iteration 73250, loss = 0.09518823772668839\n",
      "iteration 73300, loss = 0.09518823772668839\n",
      "iteration 73350, loss = 0.09518823772668839\n",
      "iteration 73400, loss = 0.09518823772668839\n",
      "iteration 73450, loss = 0.09518823772668839\n",
      "iteration 73500, loss = 0.09518823772668839\n",
      "iteration 73550, loss = 0.09518823772668839\n",
      "iteration 73600, loss = 0.09518823772668839\n",
      "iteration 73650, loss = 0.09518823772668839\n",
      "iteration 73700, loss = 0.09518823772668839\n",
      "iteration 73750, loss = 0.09518823772668839\n",
      "iteration 73800, loss = 0.09518823772668839\n",
      "iteration 73850, loss = 0.09518823772668839\n",
      "iteration 73900, loss = 0.09518823772668839\n",
      "iteration 73950, loss = 0.09518823772668839\n",
      "iteration 74000, loss = 0.09518823772668839\n",
      "iteration 74050, loss = 0.09518823772668839\n",
      "iteration 74100, loss = 0.09518823772668839\n",
      "iteration 74150, loss = 0.09518823772668839\n",
      "iteration 74200, loss = 0.09518823772668839\n",
      "iteration 74250, loss = 0.09518823772668839\n",
      "iteration 74300, loss = 0.09518823772668839\n",
      "iteration 74350, loss = 0.09518823772668839\n",
      "iteration 74400, loss = 0.09518823772668839\n",
      "iteration 74450, loss = 0.09518823772668839\n",
      "iteration 74500, loss = 0.09518823772668839\n",
      "iteration 74550, loss = 0.09518823772668839\n",
      "iteration 74600, loss = 0.09518823772668839\n",
      "iteration 74650, loss = 0.09518823772668839\n",
      "iteration 74700, loss = 0.09518823772668839\n",
      "iteration 74750, loss = 0.09518823772668839\n",
      "iteration 74800, loss = 0.09518823772668839\n",
      "iteration 74850, loss = 0.09518823772668839\n",
      "iteration 74900, loss = 0.09518823772668839\n",
      "iteration 74950, loss = 0.09518823772668839\n",
      "iteration 75000, loss = 0.09518823772668839\n",
      "iteration 75050, loss = 0.09518823772668839\n",
      "iteration 75100, loss = 0.09518823772668839\n",
      "iteration 75150, loss = 0.09518823772668839\n",
      "iteration 75200, loss = 0.09518823772668839\n",
      "iteration 75250, loss = 0.09518823772668839\n",
      "iteration 75300, loss = 0.09518823772668839\n",
      "iteration 75350, loss = 0.09518823772668839\n",
      "iteration 75400, loss = 0.09518823772668839\n",
      "iteration 75450, loss = 0.09518823772668839\n",
      "iteration 75500, loss = 0.09518823772668839\n",
      "iteration 75550, loss = 0.09518823772668839\n",
      "iteration 75600, loss = 0.09518823772668839\n",
      "iteration 75650, loss = 0.09518823772668839\n",
      "iteration 75700, loss = 0.09518823772668839\n",
      "iteration 75750, loss = 0.09518823772668839\n",
      "iteration 75800, loss = 0.09518823772668839\n",
      "iteration 75850, loss = 0.09518823772668839\n",
      "iteration 75900, loss = 0.09518823772668839\n",
      "iteration 75950, loss = 0.09518823772668839\n",
      "iteration 76000, loss = 0.09518823772668839\n",
      "iteration 76050, loss = 0.09518823772668839\n",
      "iteration 76100, loss = 0.09518823772668839\n",
      "iteration 76150, loss = 0.09518823772668839\n",
      "iteration 76200, loss = 0.09518823772668839\n",
      "iteration 76250, loss = 0.09518823772668839\n",
      "iteration 76300, loss = 0.09518823772668839\n",
      "iteration 76350, loss = 0.09518823772668839\n",
      "iteration 76400, loss = 0.09518823772668839\n",
      "iteration 76450, loss = 0.09518823772668839\n",
      "iteration 76500, loss = 0.09518823772668839\n",
      "iteration 76550, loss = 0.09518823772668839\n",
      "iteration 76600, loss = 0.09518823772668839\n",
      "iteration 76650, loss = 0.09518823772668839\n",
      "iteration 76700, loss = 0.09518823772668839\n",
      "iteration 76750, loss = 0.09518823772668839\n",
      "iteration 76800, loss = 0.09518823772668839\n",
      "iteration 76850, loss = 0.09518823772668839\n",
      "iteration 76900, loss = 0.09518823772668839\n",
      "iteration 76950, loss = 0.09518823772668839\n",
      "iteration 77000, loss = 0.09518823772668839\n",
      "iteration 77050, loss = 0.09518823772668839\n",
      "iteration 77100, loss = 0.09518823772668839\n",
      "iteration 77150, loss = 0.09518823772668839\n",
      "iteration 77200, loss = 0.09518823772668839\n",
      "iteration 77250, loss = 0.09518823772668839\n",
      "iteration 77300, loss = 0.09518823772668839\n",
      "iteration 77350, loss = 0.09518823772668839\n",
      "iteration 77400, loss = 0.09518823772668839\n",
      "iteration 77450, loss = 0.09518823772668839\n",
      "iteration 77500, loss = 0.09518823772668839\n",
      "iteration 77550, loss = 0.09518823772668839\n",
      "iteration 77600, loss = 0.09518823772668839\n",
      "iteration 77650, loss = 0.09518823772668839\n",
      "iteration 77700, loss = 0.09518823772668839\n",
      "iteration 77750, loss = 0.09518823772668839\n",
      "iteration 77800, loss = 0.09518823772668839\n",
      "iteration 77850, loss = 0.09518823772668839\n",
      "iteration 77900, loss = 0.09518823772668839\n",
      "iteration 77950, loss = 0.09518823772668839\n",
      "iteration 78000, loss = 0.09518823772668839\n",
      "iteration 78050, loss = 0.09518823772668839\n",
      "iteration 78100, loss = 0.09518823772668839\n",
      "iteration 78150, loss = 0.09518823772668839\n",
      "iteration 78200, loss = 0.09518823772668839\n",
      "iteration 78250, loss = 0.09518823772668839\n",
      "iteration 78300, loss = 0.09518823772668839\n",
      "iteration 78350, loss = 0.09518823772668839\n",
      "iteration 78400, loss = 0.09518823772668839\n",
      "iteration 78450, loss = 0.09518823772668839\n",
      "iteration 78500, loss = 0.09518823772668839\n",
      "iteration 78550, loss = 0.09518823772668839\n",
      "iteration 78600, loss = 0.09518823772668839\n",
      "iteration 78650, loss = 0.09518823772668839\n",
      "iteration 78700, loss = 0.09518823772668839\n",
      "iteration 78750, loss = 0.09518823772668839\n",
      "iteration 78800, loss = 0.09518823772668839\n",
      "iteration 78850, loss = 0.09518823772668839\n",
      "iteration 78900, loss = 0.09518823772668839\n",
      "iteration 78950, loss = 0.09518823772668839\n",
      "iteration 79000, loss = 0.09518823772668839\n",
      "iteration 79050, loss = 0.09518823772668839\n",
      "iteration 79100, loss = 0.09518823772668839\n",
      "iteration 79150, loss = 0.09518823772668839\n",
      "iteration 79200, loss = 0.09518823772668839\n",
      "iteration 79250, loss = 0.09518823772668839\n",
      "iteration 79300, loss = 0.09518823772668839\n",
      "iteration 79350, loss = 0.09518823772668839\n",
      "iteration 79400, loss = 0.09518823772668839\n",
      "iteration 79450, loss = 0.09518823772668839\n",
      "iteration 79500, loss = 0.09518823772668839\n",
      "iteration 79550, loss = 0.09518823772668839\n",
      "iteration 79600, loss = 0.09518823772668839\n",
      "iteration 79650, loss = 0.09518823772668839\n",
      "iteration 79700, loss = 0.09518823772668839\n",
      "iteration 79750, loss = 0.09518823772668839\n",
      "iteration 79800, loss = 0.09518823772668839\n",
      "iteration 79850, loss = 0.09518823772668839\n",
      "iteration 79900, loss = 0.09518823772668839\n",
      "iteration 79950, loss = 0.09518823772668839\n",
      "iteration 80000, loss = 0.09518823772668839\n",
      "iteration 80050, loss = 0.09518823772668839\n",
      "iteration 80100, loss = 0.09518823772668839\n",
      "iteration 80150, loss = 0.09518823772668839\n",
      "iteration 80200, loss = 0.09518823772668839\n",
      "iteration 80250, loss = 0.09518823772668839\n",
      "iteration 80300, loss = 0.09518823772668839\n",
      "iteration 80350, loss = 0.09518823772668839\n",
      "iteration 80400, loss = 0.09518823772668839\n",
      "iteration 80450, loss = 0.09518823772668839\n",
      "iteration 80500, loss = 0.09518823772668839\n",
      "iteration 80550, loss = 0.09518823772668839\n",
      "iteration 80600, loss = 0.09518823772668839\n",
      "iteration 80650, loss = 0.09518823772668839\n",
      "iteration 80700, loss = 0.09518823772668839\n",
      "iteration 80750, loss = 0.09518823772668839\n",
      "iteration 80800, loss = 0.09518823772668839\n",
      "iteration 80850, loss = 0.09518823772668839\n",
      "iteration 80900, loss = 0.09518823772668839\n",
      "iteration 80950, loss = 0.09518823772668839\n",
      "iteration 81000, loss = 0.09518823772668839\n",
      "iteration 81050, loss = 0.09518823772668839\n",
      "iteration 81100, loss = 0.09518823772668839\n",
      "iteration 81150, loss = 0.09518823772668839\n",
      "iteration 81200, loss = 0.09518823772668839\n",
      "iteration 81250, loss = 0.09518823772668839\n",
      "iteration 81300, loss = 0.09518823772668839\n",
      "iteration 81350, loss = 0.09518823772668839\n",
      "iteration 81400, loss = 0.09518823772668839\n",
      "iteration 81450, loss = 0.09518823772668839\n",
      "iteration 81500, loss = 0.09518823772668839\n",
      "iteration 81550, loss = 0.09518823772668839\n",
      "iteration 81600, loss = 0.09518823772668839\n",
      "iteration 81650, loss = 0.09518823772668839\n",
      "iteration 81700, loss = 0.09518823772668839\n",
      "iteration 81750, loss = 0.09518823772668839\n",
      "iteration 81800, loss = 0.09518823772668839\n",
      "iteration 81850, loss = 0.09518823772668839\n",
      "iteration 81900, loss = 0.09518823772668839\n",
      "iteration 81950, loss = 0.09518823772668839\n",
      "iteration 82000, loss = 0.09518823772668839\n",
      "iteration 82050, loss = 0.09518823772668839\n",
      "iteration 82100, loss = 0.09518823772668839\n",
      "iteration 82150, loss = 0.09518823772668839\n",
      "iteration 82200, loss = 0.09518823772668839\n",
      "iteration 82250, loss = 0.09518823772668839\n",
      "iteration 82300, loss = 0.09518823772668839\n",
      "iteration 82350, loss = 0.09518823772668839\n",
      "iteration 82400, loss = 0.09518823772668839\n",
      "iteration 82450, loss = 0.09518823772668839\n",
      "iteration 82500, loss = 0.09518823772668839\n",
      "iteration 82550, loss = 0.09518823772668839\n",
      "iteration 82600, loss = 0.09518823772668839\n",
      "iteration 82650, loss = 0.09518823772668839\n",
      "iteration 82700, loss = 0.09518823772668839\n",
      "iteration 82750, loss = 0.09518823772668839\n",
      "iteration 82800, loss = 0.09518823772668839\n",
      "iteration 82850, loss = 0.09518823772668839\n",
      "iteration 82900, loss = 0.09518823772668839\n",
      "iteration 82950, loss = 0.09518823772668839\n",
      "iteration 83000, loss = 0.09518823772668839\n",
      "iteration 83050, loss = 0.09518823772668839\n",
      "iteration 83100, loss = 0.09518823772668839\n",
      "iteration 83150, loss = 0.09518823772668839\n",
      "iteration 83200, loss = 0.09518823772668839\n",
      "iteration 83250, loss = 0.09518823772668839\n",
      "iteration 83300, loss = 0.09518823772668839\n",
      "iteration 83350, loss = 0.09518823772668839\n",
      "iteration 83400, loss = 0.09518823772668839\n",
      "iteration 83450, loss = 0.09518823772668839\n",
      "iteration 83500, loss = 0.09518823772668839\n",
      "iteration 83550, loss = 0.09518823772668839\n",
      "iteration 83600, loss = 0.09518823772668839\n",
      "iteration 83650, loss = 0.09518823772668839\n",
      "iteration 83700, loss = 0.09518823772668839\n",
      "iteration 83750, loss = 0.09518823772668839\n",
      "iteration 83800, loss = 0.09518823772668839\n",
      "iteration 83850, loss = 0.09518823772668839\n",
      "iteration 83900, loss = 0.09518823772668839\n",
      "iteration 83950, loss = 0.09518823772668839\n",
      "iteration 84000, loss = 0.09518823772668839\n",
      "iteration 84050, loss = 0.09518823772668839\n",
      "iteration 84100, loss = 0.09518823772668839\n",
      "iteration 84150, loss = 0.09518823772668839\n",
      "iteration 84200, loss = 0.09518823772668839\n",
      "iteration 84250, loss = 0.09518823772668839\n",
      "iteration 84300, loss = 0.09518823772668839\n",
      "iteration 84350, loss = 0.09518823772668839\n",
      "iteration 84400, loss = 0.09518823772668839\n",
      "iteration 84450, loss = 0.09518823772668839\n",
      "iteration 84500, loss = 0.09518823772668839\n",
      "iteration 84550, loss = 0.09518823772668839\n",
      "iteration 84600, loss = 0.09518823772668839\n",
      "iteration 84650, loss = 0.09518823772668839\n",
      "iteration 84700, loss = 0.09518823772668839\n",
      "iteration 84750, loss = 0.09518823772668839\n",
      "iteration 84800, loss = 0.09518823772668839\n",
      "iteration 84850, loss = 0.09518823772668839\n",
      "iteration 84900, loss = 0.09518823772668839\n",
      "iteration 84950, loss = 0.09518823772668839\n",
      "iteration 85000, loss = 0.09518823772668839\n",
      "iteration 85050, loss = 0.09518823772668839\n",
      "iteration 85100, loss = 0.09518823772668839\n",
      "iteration 85150, loss = 0.09518823772668839\n",
      "iteration 85200, loss = 0.09518823772668839\n",
      "iteration 85250, loss = 0.09518823772668839\n",
      "iteration 85300, loss = 0.09518823772668839\n",
      "iteration 85350, loss = 0.09518823772668839\n",
      "iteration 85400, loss = 0.09518823772668839\n",
      "iteration 85450, loss = 0.09518823772668839\n",
      "iteration 85500, loss = 0.09518823772668839\n",
      "iteration 85550, loss = 0.09518823772668839\n",
      "iteration 85600, loss = 0.09518823772668839\n",
      "iteration 85650, loss = 0.09518823772668839\n",
      "iteration 85700, loss = 0.09518823772668839\n",
      "iteration 85750, loss = 0.09518823772668839\n",
      "iteration 85800, loss = 0.09518823772668839\n",
      "iteration 85850, loss = 0.09518823772668839\n",
      "iteration 85900, loss = 0.09518823772668839\n",
      "iteration 85950, loss = 0.09518823772668839\n",
      "iteration 86000, loss = 0.09518823772668839\n",
      "iteration 86050, loss = 0.09518823772668839\n",
      "iteration 86100, loss = 0.09518823772668839\n",
      "iteration 86150, loss = 0.09518823772668839\n",
      "iteration 86200, loss = 0.09518823772668839\n",
      "iteration 86250, loss = 0.09518823772668839\n",
      "iteration 86300, loss = 0.09518823772668839\n",
      "iteration 86350, loss = 0.09518823772668839\n",
      "iteration 86400, loss = 0.09518823772668839\n",
      "iteration 86450, loss = 0.09518823772668839\n",
      "iteration 86500, loss = 0.09518823772668839\n",
      "iteration 86550, loss = 0.09518823772668839\n",
      "iteration 86600, loss = 0.09518823772668839\n",
      "iteration 86650, loss = 0.09518823772668839\n",
      "iteration 86700, loss = 0.09518823772668839\n",
      "iteration 86750, loss = 0.09518823772668839\n",
      "iteration 86800, loss = 0.09518823772668839\n",
      "iteration 86850, loss = 0.09518823772668839\n",
      "iteration 86900, loss = 0.09518823772668839\n",
      "iteration 86950, loss = 0.09518823772668839\n",
      "iteration 87000, loss = 0.09518823772668839\n",
      "iteration 87050, loss = 0.09518823772668839\n",
      "iteration 87100, loss = 0.09518823772668839\n",
      "iteration 87150, loss = 0.09518823772668839\n",
      "iteration 87200, loss = 0.09518823772668839\n",
      "iteration 87250, loss = 0.09518823772668839\n",
      "iteration 87300, loss = 0.09518823772668839\n",
      "iteration 87350, loss = 0.09518823772668839\n",
      "iteration 87400, loss = 0.09518823772668839\n",
      "iteration 87450, loss = 0.09518823772668839\n",
      "iteration 87500, loss = 0.09518823772668839\n",
      "iteration 87550, loss = 0.09518823772668839\n",
      "iteration 87600, loss = 0.09518823772668839\n",
      "iteration 87650, loss = 0.09518823772668839\n",
      "iteration 87700, loss = 0.09518823772668839\n",
      "iteration 87750, loss = 0.09518823772668839\n",
      "iteration 87800, loss = 0.09518823772668839\n",
      "iteration 87850, loss = 0.09518823772668839\n",
      "iteration 87900, loss = 0.09518823772668839\n",
      "iteration 87950, loss = 0.09518823772668839\n",
      "iteration 88000, loss = 0.09518823772668839\n",
      "iteration 88050, loss = 0.09518823772668839\n",
      "iteration 88100, loss = 0.09518823772668839\n",
      "iteration 88150, loss = 0.09518823772668839\n",
      "iteration 88200, loss = 0.09518823772668839\n",
      "iteration 88250, loss = 0.09518823772668839\n",
      "iteration 88300, loss = 0.09518823772668839\n",
      "iteration 88350, loss = 0.09518823772668839\n",
      "iteration 88400, loss = 0.09518823772668839\n",
      "iteration 88450, loss = 0.09518823772668839\n",
      "iteration 88500, loss = 0.09518823772668839\n",
      "iteration 88550, loss = 0.09518823772668839\n",
      "iteration 88600, loss = 0.09518823772668839\n",
      "iteration 88650, loss = 0.09518823772668839\n",
      "iteration 88700, loss = 0.09518823772668839\n",
      "iteration 88750, loss = 0.09518823772668839\n",
      "iteration 88800, loss = 0.09518823772668839\n",
      "iteration 88850, loss = 0.09518823772668839\n",
      "iteration 88900, loss = 0.09518823772668839\n",
      "iteration 88950, loss = 0.09518823772668839\n",
      "iteration 89000, loss = 0.09518823772668839\n",
      "iteration 89050, loss = 0.09518823772668839\n",
      "iteration 89100, loss = 0.09518823772668839\n",
      "iteration 89150, loss = 0.09518823772668839\n",
      "iteration 89200, loss = 0.09518823772668839\n",
      "iteration 89250, loss = 0.09518823772668839\n",
      "iteration 89300, loss = 0.09518823772668839\n",
      "iteration 89350, loss = 0.09518823772668839\n",
      "iteration 89400, loss = 0.09518823772668839\n",
      "iteration 89450, loss = 0.09518823772668839\n",
      "iteration 89500, loss = 0.09518823772668839\n",
      "iteration 89550, loss = 0.09518823772668839\n",
      "iteration 89600, loss = 0.09518823772668839\n",
      "iteration 89650, loss = 0.09518823772668839\n",
      "iteration 89700, loss = 0.09518823772668839\n",
      "iteration 89750, loss = 0.09518823772668839\n",
      "iteration 89800, loss = 0.09518823772668839\n",
      "iteration 89850, loss = 0.09518823772668839\n",
      "iteration 89900, loss = 0.09518823772668839\n",
      "iteration 89950, loss = 0.09518823772668839\n",
      "iteration 90000, loss = 0.09518823772668839\n",
      "iteration 90050, loss = 0.09518823772668839\n",
      "iteration 90100, loss = 0.09518823772668839\n",
      "iteration 90150, loss = 0.09518823772668839\n",
      "iteration 90200, loss = 0.09518823772668839\n",
      "iteration 90250, loss = 0.09518823772668839\n",
      "iteration 90300, loss = 0.09518823772668839\n",
      "iteration 90350, loss = 0.09518823772668839\n",
      "iteration 90400, loss = 0.09518823772668839\n",
      "iteration 90450, loss = 0.09518823772668839\n",
      "iteration 90500, loss = 0.09518823772668839\n",
      "iteration 90550, loss = 0.09518823772668839\n",
      "iteration 90600, loss = 0.09518823772668839\n",
      "iteration 90650, loss = 0.09518823772668839\n",
      "iteration 90700, loss = 0.09518823772668839\n",
      "iteration 90750, loss = 0.09518823772668839\n",
      "iteration 90800, loss = 0.09518823772668839\n",
      "iteration 90850, loss = 0.09518823772668839\n",
      "iteration 90900, loss = 0.09518823772668839\n",
      "iteration 90950, loss = 0.09518823772668839\n",
      "iteration 91000, loss = 0.09518823772668839\n",
      "iteration 91050, loss = 0.09518823772668839\n",
      "iteration 91100, loss = 0.09518823772668839\n",
      "iteration 91150, loss = 0.09518823772668839\n",
      "iteration 91200, loss = 0.09518823772668839\n",
      "iteration 91250, loss = 0.09518823772668839\n",
      "iteration 91300, loss = 0.09518823772668839\n",
      "iteration 91350, loss = 0.09518823772668839\n",
      "iteration 91400, loss = 0.09518823772668839\n",
      "iteration 91450, loss = 0.09518823772668839\n",
      "iteration 91500, loss = 0.09518823772668839\n",
      "iteration 91550, loss = 0.09518823772668839\n",
      "iteration 91600, loss = 0.09518823772668839\n",
      "iteration 91650, loss = 0.09518823772668839\n",
      "iteration 91700, loss = 0.09518823772668839\n",
      "iteration 91750, loss = 0.09518823772668839\n",
      "iteration 91800, loss = 0.09518823772668839\n",
      "iteration 91850, loss = 0.09518823772668839\n",
      "iteration 91900, loss = 0.09518823772668839\n",
      "iteration 91950, loss = 0.09518823772668839\n",
      "iteration 92000, loss = 0.09518823772668839\n",
      "iteration 92050, loss = 0.09518823772668839\n",
      "iteration 92100, loss = 0.09518823772668839\n",
      "iteration 92150, loss = 0.09518823772668839\n",
      "iteration 92200, loss = 0.09518823772668839\n",
      "iteration 92250, loss = 0.09518823772668839\n",
      "iteration 92300, loss = 0.09518823772668839\n",
      "iteration 92350, loss = 0.09518823772668839\n",
      "iteration 92400, loss = 0.09518823772668839\n",
      "iteration 92450, loss = 0.09518823772668839\n",
      "iteration 92500, loss = 0.09518823772668839\n",
      "iteration 92550, loss = 0.09518823772668839\n",
      "iteration 92600, loss = 0.09518823772668839\n",
      "iteration 92650, loss = 0.09518823772668839\n",
      "iteration 92700, loss = 0.09518823772668839\n",
      "iteration 92750, loss = 0.09518823772668839\n",
      "iteration 92800, loss = 0.09518823772668839\n",
      "iteration 92850, loss = 0.09518823772668839\n",
      "iteration 92900, loss = 0.09518823772668839\n",
      "iteration 92950, loss = 0.09518823772668839\n",
      "iteration 93000, loss = 0.09518823772668839\n",
      "iteration 93050, loss = 0.09518823772668839\n",
      "iteration 93100, loss = 0.09518823772668839\n",
      "iteration 93150, loss = 0.09518823772668839\n",
      "iteration 93200, loss = 0.09518823772668839\n",
      "iteration 93250, loss = 0.09518823772668839\n",
      "iteration 93300, loss = 0.09518823772668839\n",
      "iteration 93350, loss = 0.09518823772668839\n",
      "iteration 93400, loss = 0.09518823772668839\n",
      "iteration 93450, loss = 0.09518823772668839\n",
      "iteration 93500, loss = 0.09518823772668839\n",
      "iteration 93550, loss = 0.09518823772668839\n",
      "iteration 93600, loss = 0.09518823772668839\n",
      "iteration 93650, loss = 0.09518823772668839\n",
      "iteration 93700, loss = 0.09518823772668839\n",
      "iteration 93750, loss = 0.09518823772668839\n",
      "iteration 93800, loss = 0.09518823772668839\n",
      "iteration 93850, loss = 0.09518823772668839\n",
      "iteration 93900, loss = 0.09518823772668839\n",
      "iteration 93950, loss = 0.09518823772668839\n",
      "iteration 94000, loss = 0.09518823772668839\n",
      "iteration 94050, loss = 0.09518823772668839\n",
      "iteration 94100, loss = 0.09518823772668839\n",
      "iteration 94150, loss = 0.09518823772668839\n",
      "iteration 94200, loss = 0.09518823772668839\n",
      "iteration 94250, loss = 0.09518823772668839\n",
      "iteration 94300, loss = 0.09518823772668839\n",
      "iteration 94350, loss = 0.09518823772668839\n",
      "iteration 94400, loss = 0.09518823772668839\n",
      "iteration 94450, loss = 0.09518823772668839\n",
      "iteration 94500, loss = 0.09518823772668839\n",
      "iteration 94550, loss = 0.09518823772668839\n",
      "iteration 94600, loss = 0.09518823772668839\n",
      "iteration 94650, loss = 0.09518823772668839\n",
      "iteration 94700, loss = 0.09518823772668839\n",
      "iteration 94750, loss = 0.09518823772668839\n",
      "iteration 94800, loss = 0.09518823772668839\n",
      "iteration 94850, loss = 0.09518823772668839\n",
      "iteration 94900, loss = 0.09518823772668839\n",
      "iteration 94950, loss = 0.09518823772668839\n",
      "iteration 95000, loss = 0.09518823772668839\n",
      "iteration 95050, loss = 0.09518823772668839\n",
      "iteration 95100, loss = 0.09518823772668839\n",
      "iteration 95150, loss = 0.09518823772668839\n",
      "iteration 95200, loss = 0.09518823772668839\n",
      "iteration 95250, loss = 0.09518823772668839\n",
      "iteration 95300, loss = 0.09518823772668839\n",
      "iteration 95350, loss = 0.09518823772668839\n",
      "iteration 95400, loss = 0.09518823772668839\n",
      "iteration 95450, loss = 0.09518823772668839\n",
      "iteration 95500, loss = 0.09518823772668839\n",
      "iteration 95550, loss = 0.09518823772668839\n",
      "iteration 95600, loss = 0.09518823772668839\n",
      "iteration 95650, loss = 0.09518823772668839\n",
      "iteration 95700, loss = 0.09518823772668839\n",
      "iteration 95750, loss = 0.09518823772668839\n",
      "iteration 95800, loss = 0.09518823772668839\n",
      "iteration 95850, loss = 0.09518823772668839\n",
      "iteration 95900, loss = 0.09518823772668839\n",
      "iteration 95950, loss = 0.09518823772668839\n",
      "iteration 96000, loss = 0.09518823772668839\n",
      "iteration 96050, loss = 0.09518823772668839\n",
      "iteration 96100, loss = 0.09518823772668839\n",
      "iteration 96150, loss = 0.09518823772668839\n",
      "iteration 96200, loss = 0.09518823772668839\n",
      "iteration 96250, loss = 0.09518823772668839\n",
      "iteration 96300, loss = 0.09518823772668839\n",
      "iteration 96350, loss = 0.09518823772668839\n",
      "iteration 96400, loss = 0.09518823772668839\n",
      "iteration 96450, loss = 0.09518823772668839\n",
      "iteration 96500, loss = 0.09518823772668839\n",
      "iteration 96550, loss = 0.09518823772668839\n",
      "iteration 96600, loss = 0.09518823772668839\n",
      "iteration 96650, loss = 0.09518823772668839\n",
      "iteration 96700, loss = 0.09518823772668839\n",
      "iteration 96750, loss = 0.09518823772668839\n",
      "iteration 96800, loss = 0.09518823772668839\n",
      "iteration 96850, loss = 0.09518823772668839\n",
      "iteration 96900, loss = 0.09518823772668839\n",
      "iteration 96950, loss = 0.09518823772668839\n",
      "iteration 97000, loss = 0.09518823772668839\n",
      "iteration 97050, loss = 0.09518823772668839\n",
      "iteration 97100, loss = 0.09518823772668839\n",
      "iteration 97150, loss = 0.09518823772668839\n",
      "iteration 97200, loss = 0.09518823772668839\n",
      "iteration 97250, loss = 0.09518823772668839\n",
      "iteration 97300, loss = 0.09518823772668839\n",
      "iteration 97350, loss = 0.09518823772668839\n",
      "iteration 97400, loss = 0.09518823772668839\n",
      "iteration 97450, loss = 0.09518823772668839\n",
      "iteration 97500, loss = 0.09518823772668839\n",
      "iteration 97550, loss = 0.09518823772668839\n",
      "iteration 97600, loss = 0.09518823772668839\n",
      "iteration 97650, loss = 0.09518823772668839\n",
      "iteration 97700, loss = 0.09518823772668839\n",
      "iteration 97750, loss = 0.09518823772668839\n",
      "iteration 97800, loss = 0.09518823772668839\n",
      "iteration 97850, loss = 0.09518823772668839\n",
      "iteration 97900, loss = 0.09518823772668839\n",
      "iteration 97950, loss = 0.09518823772668839\n",
      "iteration 98000, loss = 0.09518823772668839\n",
      "iteration 98050, loss = 0.09518823772668839\n",
      "iteration 98100, loss = 0.09518823772668839\n",
      "iteration 98150, loss = 0.09518823772668839\n",
      "iteration 98200, loss = 0.09518823772668839\n",
      "iteration 98250, loss = 0.09518823772668839\n",
      "iteration 98300, loss = 0.09518823772668839\n",
      "iteration 98350, loss = 0.09518823772668839\n",
      "iteration 98400, loss = 0.09518823772668839\n",
      "iteration 98450, loss = 0.09518823772668839\n",
      "iteration 98500, loss = 0.09518823772668839\n",
      "iteration 98550, loss = 0.09518823772668839\n",
      "iteration 98600, loss = 0.09518823772668839\n",
      "iteration 98650, loss = 0.09518823772668839\n",
      "iteration 98700, loss = 0.09518823772668839\n",
      "iteration 98750, loss = 0.09518823772668839\n",
      "iteration 98800, loss = 0.09518823772668839\n",
      "iteration 98850, loss = 0.09518823772668839\n",
      "iteration 98900, loss = 0.09518823772668839\n",
      "iteration 98950, loss = 0.09518823772668839\n",
      "iteration 99000, loss = 0.09518823772668839\n",
      "iteration 99050, loss = 0.09518823772668839\n",
      "iteration 99100, loss = 0.09518823772668839\n",
      "iteration 99150, loss = 0.09518823772668839\n",
      "iteration 99200, loss = 0.09518823772668839\n",
      "iteration 99250, loss = 0.09518823772668839\n",
      "iteration 99300, loss = 0.09518823772668839\n",
      "iteration 99350, loss = 0.09518823772668839\n",
      "iteration 99400, loss = 0.09518823772668839\n",
      "iteration 99450, loss = 0.09518823772668839\n",
      "iteration 99500, loss = 0.09518823772668839\n",
      "iteration 99550, loss = 0.09518823772668839\n",
      "iteration 99600, loss = 0.09518823772668839\n",
      "iteration 99650, loss = 0.09518823772668839\n",
      "iteration 99700, loss = 0.09518823772668839\n",
      "iteration 99750, loss = 0.09518823772668839\n",
      "iteration 99800, loss = 0.09518823772668839\n",
      "iteration 99850, loss = 0.09518823772668839\n",
      "iteration 99900, loss = 0.09518823772668839\n",
      "iteration 99950, loss = 0.09518823772668839\n",
      "iteration 100000, loss = 0.09518823772668839\n",
      "iteration 100050, loss = 0.09518823772668839\n",
      "iteration 100100, loss = 0.09518823772668839\n",
      "iteration 100150, loss = 0.09518823772668839\n",
      "iteration 100200, loss = 0.09518823772668839\n",
      "iteration 100250, loss = 0.09518823772668839\n",
      "iteration 100300, loss = 0.09518823772668839\n",
      "iteration 100350, loss = 0.09518823772668839\n",
      "iteration 100400, loss = 0.09518823772668839\n",
      "iteration 100450, loss = 0.09518823772668839\n",
      "iteration 100500, loss = 0.09518823772668839\n",
      "iteration 100550, loss = 0.09518823772668839\n",
      "iteration 100600, loss = 0.09518823772668839\n",
      "iteration 100650, loss = 0.09518823772668839\n",
      "iteration 100700, loss = 0.09518823772668839\n",
      "iteration 100750, loss = 0.09518823772668839\n",
      "iteration 100800, loss = 0.09518823772668839\n",
      "iteration 100850, loss = 0.09518823772668839\n",
      "iteration 100900, loss = 0.09518823772668839\n",
      "iteration 100950, loss = 0.09518823772668839\n",
      "iteration 101000, loss = 0.09518823772668839\n",
      "iteration 101050, loss = 0.09518823772668839\n",
      "iteration 101100, loss = 0.09518823772668839\n",
      "iteration 101150, loss = 0.09518823772668839\n",
      "iteration 101200, loss = 0.09518823772668839\n",
      "iteration 101250, loss = 0.09518823772668839\n",
      "iteration 101300, loss = 0.09518823772668839\n",
      "iteration 101350, loss = 0.09518823772668839\n",
      "iteration 101400, loss = 0.09518823772668839\n",
      "iteration 101450, loss = 0.09518823772668839\n",
      "iteration 101500, loss = 0.09518823772668839\n",
      "iteration 101550, loss = 0.09518823772668839\n",
      "iteration 101600, loss = 0.09518823772668839\n",
      "iteration 101650, loss = 0.09518823772668839\n",
      "iteration 101700, loss = 0.09518823772668839\n",
      "iteration 101750, loss = 0.09518823772668839\n",
      "iteration 101800, loss = 0.09518823772668839\n",
      "iteration 101850, loss = 0.09518823772668839\n",
      "iteration 101900, loss = 0.09518823772668839\n",
      "iteration 101950, loss = 0.09518823772668839\n",
      "iteration 102000, loss = 0.09518823772668839\n",
      "iteration 102050, loss = 0.09518823772668839\n",
      "iteration 102100, loss = 0.09518823772668839\n",
      "iteration 102150, loss = 0.09518823772668839\n",
      "iteration 102200, loss = 0.09518823772668839\n",
      "iteration 102250, loss = 0.09518823772668839\n",
      "iteration 102300, loss = 0.09518823772668839\n",
      "iteration 102350, loss = 0.09518823772668839\n",
      "iteration 102400, loss = 0.09518823772668839\n",
      "iteration 102450, loss = 0.09518823772668839\n",
      "iteration 102500, loss = 0.09518823772668839\n",
      "iteration 102550, loss = 0.09518823772668839\n",
      "iteration 102600, loss = 0.09518823772668839\n",
      "iteration 102650, loss = 0.09518823772668839\n",
      "iteration 102700, loss = 0.09518823772668839\n",
      "iteration 102750, loss = 0.09518823772668839\n",
      "iteration 102800, loss = 0.09518823772668839\n",
      "iteration 102850, loss = 0.09518823772668839\n",
      "iteration 102900, loss = 0.09518823772668839\n",
      "iteration 102950, loss = 0.09518823772668839\n",
      "iteration 103000, loss = 0.09518823772668839\n",
      "iteration 103050, loss = 0.09518823772668839\n",
      "iteration 103100, loss = 0.09518823772668839\n",
      "iteration 103150, loss = 0.09518823772668839\n",
      "iteration 103200, loss = 0.09518823772668839\n",
      "iteration 103250, loss = 0.09518823772668839\n",
      "iteration 103300, loss = 0.09518823772668839\n",
      "iteration 103350, loss = 0.09518823772668839\n",
      "iteration 103400, loss = 0.09518823772668839\n",
      "iteration 103450, loss = 0.09518823772668839\n",
      "iteration 103500, loss = 0.09518823772668839\n",
      "iteration 103550, loss = 0.09518823772668839\n",
      "iteration 103600, loss = 0.09518823772668839\n",
      "iteration 103650, loss = 0.09518823772668839\n",
      "iteration 103700, loss = 0.09518823772668839\n",
      "iteration 103750, loss = 0.09518823772668839\n",
      "iteration 103800, loss = 0.09518823772668839\n",
      "iteration 103850, loss = 0.09518823772668839\n",
      "iteration 103900, loss = 0.09518823772668839\n",
      "iteration 103950, loss = 0.09518823772668839\n",
      "iteration 104000, loss = 0.09518823772668839\n",
      "iteration 104050, loss = 0.09518823772668839\n",
      "iteration 104100, loss = 0.09518823772668839\n",
      "iteration 104150, loss = 0.09518823772668839\n",
      "iteration 104200, loss = 0.09518823772668839\n",
      "iteration 104250, loss = 0.09518823772668839\n",
      "iteration 104300, loss = 0.09518823772668839\n",
      "iteration 104350, loss = 0.09518823772668839\n",
      "iteration 104400, loss = 0.09518823772668839\n",
      "iteration 104450, loss = 0.09518823772668839\n",
      "iteration 104500, loss = 0.09518823772668839\n",
      "iteration 104550, loss = 0.09518823772668839\n",
      "iteration 104600, loss = 0.09518823772668839\n",
      "iteration 104650, loss = 0.09518823772668839\n",
      "iteration 104700, loss = 0.09518823772668839\n",
      "iteration 104750, loss = 0.09518823772668839\n",
      "iteration 104800, loss = 0.09518823772668839\n",
      "iteration 104850, loss = 0.09518823772668839\n",
      "iteration 104900, loss = 0.09518823772668839\n",
      "iteration 104950, loss = 0.09518823772668839\n",
      "iteration 105000, loss = 0.09518823772668839\n",
      "iteration 105050, loss = 0.09518823772668839\n",
      "iteration 105100, loss = 0.09518823772668839\n",
      "iteration 105150, loss = 0.09518823772668839\n",
      "iteration 105200, loss = 0.09518823772668839\n",
      "iteration 105250, loss = 0.09518823772668839\n",
      "iteration 105300, loss = 0.09518823772668839\n",
      "iteration 105350, loss = 0.09518823772668839\n",
      "iteration 105400, loss = 0.09518823772668839\n",
      "iteration 105450, loss = 0.09518823772668839\n",
      "iteration 105500, loss = 0.09518823772668839\n",
      "iteration 105550, loss = 0.09518823772668839\n",
      "iteration 105600, loss = 0.09518823772668839\n",
      "iteration 105650, loss = 0.09518823772668839\n",
      "iteration 105700, loss = 0.09518823772668839\n",
      "iteration 105750, loss = 0.09518823772668839\n",
      "iteration 105800, loss = 0.09518823772668839\n",
      "iteration 105850, loss = 0.09518823772668839\n",
      "iteration 105900, loss = 0.09518823772668839\n",
      "iteration 105950, loss = 0.09518823772668839\n",
      "iteration 106000, loss = 0.09518823772668839\n",
      "iteration 106050, loss = 0.09518823772668839\n",
      "iteration 106100, loss = 0.09518823772668839\n",
      "iteration 106150, loss = 0.09518823772668839\n",
      "iteration 106200, loss = 0.09518823772668839\n",
      "iteration 106250, loss = 0.09518823772668839\n",
      "iteration 106300, loss = 0.09518823772668839\n",
      "iteration 106350, loss = 0.09518823772668839\n",
      "iteration 106400, loss = 0.09518823772668839\n",
      "iteration 106450, loss = 0.09518823772668839\n",
      "iteration 106500, loss = 0.09518823772668839\n",
      "iteration 106550, loss = 0.09518823772668839\n",
      "iteration 106600, loss = 0.09518823772668839\n",
      "iteration 106650, loss = 0.09518823772668839\n",
      "iteration 106700, loss = 0.09518823772668839\n",
      "iteration 106750, loss = 0.09518823772668839\n",
      "iteration 106800, loss = 0.09518823772668839\n",
      "iteration 106850, loss = 0.09518823772668839\n",
      "iteration 106900, loss = 0.09518823772668839\n",
      "iteration 106950, loss = 0.09518823772668839\n",
      "iteration 107000, loss = 0.09518823772668839\n",
      "iteration 107050, loss = 0.09518823772668839\n",
      "iteration 107100, loss = 0.09518823772668839\n",
      "iteration 107150, loss = 0.09518823772668839\n",
      "iteration 107200, loss = 0.09518823772668839\n",
      "iteration 107250, loss = 0.09518823772668839\n",
      "iteration 107300, loss = 0.09518823772668839\n",
      "iteration 107350, loss = 0.09518823772668839\n",
      "iteration 107400, loss = 0.09518823772668839\n",
      "iteration 107450, loss = 0.09518823772668839\n",
      "iteration 107500, loss = 0.09518823772668839\n",
      "iteration 107550, loss = 0.09518823772668839\n",
      "iteration 107600, loss = 0.09518823772668839\n",
      "iteration 107650, loss = 0.09518823772668839\n",
      "iteration 107700, loss = 0.09518823772668839\n",
      "iteration 107750, loss = 0.09518823772668839\n",
      "iteration 107800, loss = 0.09518823772668839\n",
      "iteration 107850, loss = 0.09518823772668839\n",
      "iteration 107900, loss = 0.09518823772668839\n",
      "iteration 107950, loss = 0.09518823772668839\n",
      "iteration 108000, loss = 0.09518823772668839\n",
      "iteration 108050, loss = 0.09518823772668839\n",
      "iteration 108100, loss = 0.09518823772668839\n",
      "iteration 108150, loss = 0.09518823772668839\n",
      "iteration 108200, loss = 0.09518823772668839\n",
      "iteration 108250, loss = 0.09518823772668839\n",
      "iteration 108300, loss = 0.09518823772668839\n",
      "iteration 108350, loss = 0.09518823772668839\n",
      "iteration 108400, loss = 0.09518823772668839\n",
      "iteration 108450, loss = 0.09518823772668839\n",
      "iteration 108500, loss = 0.09518823772668839\n",
      "iteration 108550, loss = 0.09518823772668839\n",
      "iteration 108600, loss = 0.09518823772668839\n",
      "iteration 108650, loss = 0.09518823772668839\n",
      "iteration 108700, loss = 0.09518823772668839\n",
      "iteration 108750, loss = 0.09518823772668839\n",
      "iteration 108800, loss = 0.09518823772668839\n",
      "iteration 108850, loss = 0.09518823772668839\n",
      "iteration 108900, loss = 0.09518823772668839\n",
      "iteration 108950, loss = 0.09518823772668839\n",
      "iteration 109000, loss = 0.09518823772668839\n",
      "iteration 109050, loss = 0.09518823772668839\n",
      "iteration 109100, loss = 0.09518823772668839\n",
      "iteration 109150, loss = 0.09518823772668839\n",
      "iteration 109200, loss = 0.09518823772668839\n",
      "iteration 109250, loss = 0.09518823772668839\n",
      "iteration 109300, loss = 0.09518823772668839\n",
      "iteration 109350, loss = 0.09518823772668839\n",
      "iteration 109400, loss = 0.09518823772668839\n",
      "iteration 109450, loss = 0.09518823772668839\n",
      "iteration 109500, loss = 0.09518823772668839\n",
      "iteration 109550, loss = 0.09518823772668839\n",
      "iteration 109600, loss = 0.09518823772668839\n",
      "iteration 109650, loss = 0.09518823772668839\n",
      "iteration 109700, loss = 0.09518823772668839\n",
      "iteration 109750, loss = 0.09518823772668839\n",
      "iteration 109800, loss = 0.09518823772668839\n",
      "iteration 109850, loss = 0.09518823772668839\n",
      "iteration 109900, loss = 0.09518823772668839\n",
      "iteration 109950, loss = 0.09518823772668839\n",
      "iteration 110000, loss = 0.09518823772668839\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/xinby/Desktop/ByCsdiy/DeepLearning_Q/lec5-module.ipynb Cell 17\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/xinby/Desktop/ByCsdiy/DeepLearning_Q/lec5-module.ipynb#X22sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m loss \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmean(torch\u001b[39m.\u001b[39msquare(y \u001b[39m-\u001b[39m yhat))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/xinby/Desktop/ByCsdiy/DeepLearning_Q/lec5-module.ipynb#X22sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m opt\u001b[39m.\u001b[39mzero_grad() \u001b[39m# 梯度清零\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/xinby/Desktop/ByCsdiy/DeepLearning_Q/lec5-module.ipynb#X22sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward() \u001b[39m# 反向传播\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/xinby/Desktop/ByCsdiy/DeepLearning_Q/lec5-module.ipynb#X22sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m opt\u001b[39m.\u001b[39mstep() \u001b[39m# \u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/xinby/Desktop/ByCsdiy/DeepLearning_Q/lec5-module.ipynb#X22sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m losses\u001b[39m.\u001b[39mappend(loss\u001b[39m.\u001b[39mitem())\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/deepL_Q/lib/python3.11/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/deepL_Q/lib/python3.11/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39m_execution_engine\u001b[39m.\u001b[39mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, accumulate_grad\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 迭代次数\n",
    "nepoch = 500000\n",
    "# 学习率，即步长\n",
    "learning_rate = 0.00001\n",
    "# 记录损失函数值\n",
    "losses = []\n",
    "\n",
    "opt = torch.optim.SGD(model.parameters(), lr=learning_rate) # pytorch提供了一个优化器，对于model这个对象里的所有para进行优化\n",
    "\n",
    "for i in range(nepoch):\n",
    "    yhat = model(x)\n",
    "    loss = torch.mean(torch.square(y - yhat))\n",
    "\n",
    "    opt.zero_grad() # 梯度清零\n",
    "    loss.backward() # 反向传播\n",
    "    opt.step() # \n",
    "\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    if i % 50 == 0:\n",
    "        print(f\"iteration {i}, loss = {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([ 1.8304,  0.4262, -1.1998,  0.3770, -1.0890], requires_grad=True)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x24c288fcb80>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(list(model.parameters()))\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模块化编程的好处是模型的结构可以随时发生调整，而后续的循环、优化等部分的代码可以保持不变。例如，假设我们此时希望加入一个截距项，可以把模型修改为如下形式："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([ 1.8645,  0.4071, -1.1971,  0.3489, -1.1437], requires_grad=True), Parameter containing:\n",
      "tensor([0.], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self, beta_dim):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.bhat = nn.Parameter(torch.randn(beta_dim))\n",
    "        self.b0 = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        yhat = torch.matmul(x, self.bhat) + self.b0\n",
    "        return yhat\n",
    "\n",
    "np.random.seed(123456)\n",
    "torch.random.manual_seed(123456)\n",
    "\n",
    "model = MyModel(beta_dim=p)\n",
    "print(list(model.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来模型训练的代码可以不做任何改变："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0, loss = 0.10102550685405731\n",
      "iteration 50, loss = 0.09571389853954315\n",
      "iteration 100, loss = 0.09518022835254669\n",
      "iteration 150, loss = 0.0951167494058609\n",
      "iteration 200, loss = 0.09510793536901474\n",
      "iteration 250, loss = 0.09510650485754013\n",
      "iteration 300, loss = 0.09510625153779984\n",
      "iteration 350, loss = 0.09510622173547745\n",
      "iteration 400, loss = 0.09510618448257446\n",
      "iteration 450, loss = 0.09510619193315506\n",
      "[Parameter containing:\n",
      "tensor([ 1.8301,  0.4255, -1.2002,  0.3755, -1.0897], requires_grad=True), Parameter containing:\n",
      "tensor([0.0094], requires_grad=True)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x24c28973760>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 迭代次数\n",
    "nepoch = 500\n",
    "# 学习率，即步长\n",
    "learning_rate = 0.01\n",
    "# 记录损失函数值\n",
    "losses = []\n",
    "\n",
    "opt = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for i in range(nepoch):\n",
    "    yhat = model(x)\n",
    "    loss = torch.mean(torch.square(y - yhat))\n",
    "\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    if i % 50 == 0:\n",
    "        print(f\"iteration {i}, loss = {loss.item()}\")\n",
    "\n",
    "print(list(model.parameters()))\n",
    "plt.plot(losses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
