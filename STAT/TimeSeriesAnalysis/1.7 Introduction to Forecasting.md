# Introduction to Forecasting

> Ref: *Introduction to Time Series Analysis, Y. S. TAN, Chapter 1.7 https://yanshuo.quarto.pub/nus-ts-book/07-intro_forecasting.html*

## The Target of a Forecast

预测是时间序列分析的一个重要应用. 形式上, 一个最简单的预测可以认为是在给定前 $n$ 个时间点的数据 $x_1, x_2, \ldots, x_n$ 的情况下, 预测 $h$ 步之后的数据 $x_{n+h}$, 其中 $h = 1, 2, \ldots$. 通常会记这个预测的点估计值为: $\hat{x}_{n+h|n}$. 称 $h$ 为 forecast horizon. 有时我们也会以置信区间 (confidence region) 的形式给出预测值的区间估计, 常在图中以一个阴影条带标识.

![20250202123801](https://yanshuo.quarto.pub/nus-ts-book/07-intro_forecasting_files/figure-html/fig-forecasting-diabetes-1.png)

## Simple Forecasting Methods

在实际应用中, 有很多简单的预测方法. 这些例子有时对于一些规律性较强较为简单的时间序列是有效的. 

### Mean Forecast

最简单的预测方法是使用历史数据的均值作为预测值:
$$
\hat{x}_{n+h|n} := \bar{x}_n = \frac{1}{n} \sum_{i=1}^n x_i
$$

![](https://yanshuo.quarto.pub/nus-ts-book/07-intro_forecasting_files/figure-html/fig-forecasting-mean-1.png)

### Naive Forecast

Naive Forecast 是指直接将最近的观测值作为预测值:
$$
\hat{x}_{n+h|n} := x_n
$$

![](https://yanshuo.quarto.pub/nus-ts-book/07-intro_forecasting_files/figure-html/fig-forecasting-naive-1.png)

Naive Forecast 有时较适合一些符合鞅 (martingale) 性质的 (可以认为类似于随机游走的) 时间序列的预测 , 例如短期的股票价格序列.

### Seasonal Naive Forecast

Seasonal Naive Forecast 是指将最近一个季节性周期内的观测值重复作为下一个季节性周期的预测值:
$$
\hat{x}_{n+h|n} := x_{n - ((p-h) \text{~mod~} p)}
$$  
其中 
- $p$ 是季节性周期的长度
- $\text{mod}$ 是取余数运算, 例如 $5 \div 2 = 2 \cdots 1$, 则 $5 \text{~mod~} 2 = 1$.

数学上理解这个下标, 如果先不考虑取余数运算, 那么 $x_{n - (p-h)} = x_{n - p + h}$, 即站在 $n$ 时点上, 回到上一个同期季节点 $n-p$上, 再同理往前推 $h$ 个时间步长. 而取余数运算则是为了保证永远回到的是上一个同期季节点, 不会超过一个季节周期.

进一步举例说明, 我们用季度数据为例 (即 $p=4$). 假设我们已经知道前 $n = 8$ 个季度的数据 $x_1, x_2, \ldots, x_8$, 那么我们可以预测第 $9$ 个季度的数据为 $x_9 := x_{8 - ((4-1) \mod 4)} = x_5$, 第 $10$ 个季度的数据为 $x_{10} := x_{8 - ((4-2) \mod 4)} = x_6$, 以此类推. 直观的讲, 就是上个春天的数据预测下个春天的数据, 依次类推.

![](https://yanshuo.quarto.pub/nus-ts-book/07-intro_forecasting_files/figure-html/fig-forecasting-snaive-1.png)

### Linear Trend Forecast

对于给定的时间序列, 我们也可以把 $x_t$ 当作因变量, 时间 $t$ 当作自变量, 和正常的回归问题一样, 用线性回归模型来拟合数据. 例如, 我们可以用最小二乘法来拟合一个线性模型:
$$
x_t = \beta_0 + \beta_1 t + \varepsilon_t
$$
其中 $\varepsilon_t$ 是误差项. 

通过这个线性模型, 当我们求得 $\hat{\beta}_0$ 和 $\hat{\beta}_1$ 之后, 我们可以预测未来某时间点 $t = n+h$ 的数据为:
$$
\hat{x}_{n+h|n} := \hat{\beta}_0 + \hat{\beta}_1 (n+h)
$$

这个模型尤其适合于那些有明显线性趋势的时间序列.

![](https://yanshuo.quarto.pub/nus-ts-book/07-intro_forecasting_files/figure-html/fig-forecasting-linear-1.png)

### Drift Forecast

Drift Forecast 类似于 Linear Trend Forecast, 同样是用线性关系 (以 $x_t$ 为因变量, $t$ 为自变量) 来拟合数据, 但更为简化. 其直接将最后一个观测值和第一个观测值两点形成一条直线, 用这条直线来预测未来的数据. 具体来说, 给定数据 $x_1, x_2, \ldots, x_n$, Drift Forecast 直接用 $(1, x_1)$ 和 $(n, x_n)$ 这两个点来拟合一条直线为:
$$
x_t = x_n +  \frac{x_n - x_1}{n-1} (t-n)
$$

若要预测$h$步之后的数据, 则令 $t = n+h$, 即:
$$
\hat{x}_{n+h|n} := x_n +  \frac{x_n - x_1}{n-1} h   
$$

![](https://yanshuo.quarto.pub/nus-ts-book/07-intro_forecasting_files/figure-html/fig-forecasting-drift-1.png)

## Forecasting with Decomposition

在更为现实的场景中, 时间序列数据可能会同时包含了趋势 (trend) 和季节性 (seasonality) 等多种成分. 这使得上面的简单预测方法可能不再适用. 为了更好的预测, 我们可以先对时间序列进行分解, 将其分解季节性和针对季节性调整后的部分:
$$
x_t = \hat{S}_t + \hat{A}_t 
$$

然后分别进行预测, 最后再将预测结果合并:
$$
\hat{x}_{n+h|n} = \hat{S}_{n+h} + \hat{A}_{n+h}
$$

这种方法称为 Seasonal Decomposition of Time Series (STL) 方法. 其中 $\hat{S}_t$ 是季节性成分, $\hat{A}_t$ 是去除季节性后的时间序列.

具体的数学原理在后续章节中会详细介绍. 在 R 中, 我们可以使用 `stl` 函数来进行时间序列的分解:

```R
data |>  # 将data作为输入传入 pipeline
    model(StlModel = decomposition_model( # 创造一个model进行decomposition
        STL(TotalC), # 使用STL对要分析的时序数据TotalC进行分解, 得到季节性成分和季节性调整后的成分.
        TSLM(season_adjust ~ trend()), # 对STL得到的季节性调整后的成分(season_adjust) 建立一个线性模型, 捕捉长期趋势进行预测
        SNAIVE(season_year)) # 对于季节性成分, 使用上面提到的 Seasonal Naive Forecast 进行预测
     ) |>
    forecast(h = 24) |> # 指定预测的步长为24
    autoplot(diabetes, level = NULL) # 将预测结果与原始数据一起作图, 且不显示置信区间 (level = NULL)    
```

其中:
- `STL(TotalC)` 就对应着 $x_t = \hat{S}_t + \hat{A}_t$ 的分解
- `TSLM(season_adjust ~ trend())` 就对应着 $\hat{A}_t = \hat{\beta}_0 + \hat{\beta}_1 t$ 的线性模型
- `SNAIVE(season_year)` 就对应着 $\hat{S}_t = x_{n - ((p-h) \text{~mod~} p)}$ 的 Seasonal Naive Forecast
- 因此合并后的预测值就是 $\hat{x}_{n+h|n} = \hat{S}_{n+h} + \hat{A}_{n+h}$

## Transformations

在实际应用中, 我们有时会对时间序列进行变换, 以便更好的拟合模型. 例如, 对数变换, 平方根变换, 差分变换等. 这些变换可以使得数据更加稳定, 更符合模型的假设.

具体而言, 对于数据 $x_1, x_2, \ldots, x_n$, 若认为数据需要进行一些变换处理, 其原则是**先变换, 后建模**. 以对数变换为例, 我们可以先对数据进行对数变换, 得到 $y_1 = \log(x_1), y_2 = \log(x_2), \ldots, y_n = \log(x_n)$, 然后再用 $y_1, y_2, \ldots, y_n$ 来建模. 预测时, 我们可以先正常建模预测得到 $\hat{y}_{n+h|n}$, 然后再将其反变换得到 $\hat{x}_{n+h|n} = \exp(\hat{y}_{n+h|n})$.

该过程也可以通过上述的 pipeline 来实现:

```R
diabetes |> 
    model(StlModel = decomposition_model(
        STL(log(TotalC)), # 注意数据变换所在的位置
        TSLM(season_adjust ~ trend()),
        SNAIVE(season_year))
     ) |>
    forecast(h = 24) |>
    autoplot(diabetes, level = NULL)
```

## Further Discussions on Forecasting Methods

### Statistical Methods

事实上, 上述的简单预测方法都基于一些统计学的原理和相应的假设, 例如:
- Mean Forecast ($\hat{x}_{n+h|n} := \bar{x}_n$) :
  - 假设时序数据是由一个起决定性的均值 $\theta$ 和对应每个时间点的一些随机误差 $\varepsilon_t$ (对于这个误差还常假设其独立同分布于正态分布 $\varepsilon_t \sim \mathcal N(0, \sigma^2)$) 组成, 即 $x_t = \theta + \varepsilon_t$.
  - 这时, 由于 $\mathbb{E}[x_t] = \theta$, 因此 $\theta$ 的一个无偏估计就是 $\bar{x}_n$.
  - 而对于我们希望的预测值 $\hat{x}_{n+h|n}$, 同样有 $\mathbb{E}[\hat{x}_{n+h|n}] = \mathbb{E}[\theta + \varepsilon_{n+h}] = \theta$. 因此 $\hat{x}_{n+h|n} = \theta$ 也是一个无偏估计.
- Naive Forecast ($\hat{x}_{n+h|n} := x_n$) :
  - 这里的一个重要假设是时间序列是一个鞅 (martingale), 即 $\mathbb{E}[x_{n+h}|x_1, x_2, \ldots, x_n] = x_n$. 这意味着在给定前 $n$ 个时间点的数据的情况下, 对于下一个时间点的预测值, 最好的预测就是直接取最近的观测值. **未来的最佳预测就是现在, 没有额外的信息可以帮助我们更好的预测未来.** *(颇有活在当下的意味)*.
  - 因此对于一些股票数据, 我们常常认为其为 $x_t = x_{t-1} + \varepsilon_t, \varepsilon_t \stackrel{iid}{\sim} \mathcal{N}(0, \sigma^2)$, 即一个随机游走, 这时 Naive Forecast 就是最佳的预测方法. 因为$x_{t+1}$ 只依赖于 $x_t$ 和一个不可估计的随机误差项 $\varepsilon_t$. 
  - 这种预测方法适用于例如短期股票、汇率等随机游走的时间序列. 而对于明显的具有趋势性(如GDP增长)或季节性(如零售销售)的时间序列, Naive Forecast 就不再适用.

从上面的例子看出, 对于不同领域的时间序列, 我们都对于其数据的模式(或者说其分布)进行了一些假设, 然后基于这些假设来选择合适的预测方法. 这也是时间序列分析的一个重要内容. 后面还将介绍 ARIMA 等更为复杂的统计学预测方法, 其基本思想也是完全相同的. 例如 对于一个 $\text{ARIMA} (1,0,0)$, 其模型假设为:
$$
x_t = \theta x_{t-1} + \varepsilon_t
$$
其实这和上面的随机游走模型也有很多相似之处. 整体思想依然为上一个时间点的数据的 $\theta$ 倍加上一个随机误差项. 只不过其背后还有更多的假设和对应理论支持.

### Regression Methods for Forecasting

前面的讨论还只是针对于如何只利用自己过往的历史数据来进行预测. 但实际上, 我们还可以利用其他的一些外部变量来进行预测. 这是计量经济学中的一个重要内容. 例如, 我们可以用一些宏观经济指标, 人口数据等来预测某个公司的销售额. 这时我们就可以用回归模型来进行预测. 

### Machine Learning & Deep Learning Methods for Forecasting

事实上, 若是针对于时间序列的预测, 机器学习和深度学习方法是如今最为流行的不二之选, 例如机器学习中非常使用的 LightGBM 等梯度提升树模型, 以及深度学习中的 RNN, LSTM, GRU, Transformer 等模型. 这些模型在时间序列预测中面更为现实和复杂的场景有着非常好的表现和预测精度. 在更面相业界实际表现而不关注理论分析的情况下, 这些模型往往是更好的选择. 而理论学者们对于时序的研究也更多的关注在预测之外的如可解释性、因果性等问题上.

## Prediction Intervals

### Calculating Prediction Intervals

除了给出一个点估计值 $\hat{x}_{n+h|n}$ 之外, 我们还可以给出一个区间估计, 即置信区间. 置信区间的意义是, 在给定置信水平下, 我们可以认为真实值落在这个区间内的概率是多少. 例如, 95% 的置信区间就是说, 在给定置信水平下, 有95% 的概率真实值被包含在这个区间内. 

在前面的点估计方法中, 除了点估计值 $\hat{x}_{n+h|n}$ 之外, 我们还可以通过一些方法计算出其对应的标准误差 $\text{SE}(\hat{x}_{n+h|n}) \triangleq \hat\sigma_{h}$, 这个标准误差可以用来计算置信区间. 例如, 对于一个正态分布的置信区间, 我们可以计算出:
$$
(\hat{x}_{n+h|n} - z_{\alpha/2} \hat\sigma_{h}, \hat{x}_{n+h|n} + z_{\alpha/2} \hat\sigma_{h}) \approx (\hat{x}_{n+h|n} - 1.96 \hat\sigma_{h}, \hat{x}_{n+h|n} + 1.96 \hat\sigma_{h})
$$
其中 $z_{\alpha/2}$ 是标准正态分布的 $\alpha/2$ 分位数.

### Can We Trust the Prediction Intervals?

当然, 所有的预测方法都是基于一些假设的, 这些假设可能在实际应用中并不成立. 因此这些预测值和置信区间都需要谨慎对待. 可能出现的问题有:
  - 参数估计的结果本身就是对真实参数值的近似, 包含误差
  - 模型的假设可能并不完全成立, 例如我们假设了一个线性模型, 但实际上数据可能并不是线性的. 基于一个错误的模型进行预测, 得到的预测值和置信区间可能是不准确的
  - 对于复杂系统的噪音很大, 预测的不确定性可能会很大; 并且对于一些复杂系统, 一些不可预测的反馈机制和外部干扰可能会导致预测的不确定性增大

因此, 对于预测值和置信区间我们往往当做是一个最理想状况下的近似, 而不是绝对的真实值. 常常我们会对预测值和置信区间进行一些稳健性的分析等修正为给定置信水平下更宽的置信区间, 以应对这些不确定性.

### Bootstrap Prediction Intervals

前面提到, 置信区间的计算依赖于对标准误差 $\hat\sigma_{h}$ 的估计. 但是并不是所有的情况下我们都能准确的用统计学的推导来计算出这个标准误差. 而 Bootstrap 方法就是一种不依赖于统计模型的数值模拟方法, 可以直接用我们手上的数据来估计置信区间. 其应用非常广泛而不只于时间序列分析.

这里首先笼统地介绍一下 Bootstrap 方法的基本思想. (针对时序数据, 考虑到其对于时间的依赖性(其有序的属性), 还会在最基础的 Bootstrap 方法上进行一些改进. 具体的内容会在后续章节中详细介绍) 假设我们有一组普通的样本数据 $x_1, x_2, \ldots, x_n$, 我们希望估计例如中位数$\text{Med}(x)$ 的置信区间. 传统的 Bootstrap 方法会进行如下步骤:
1. **从原始数据中 *有放回* 的抽取 *与原有数据样本量相同* 的$n$个数据, 组成组个新的 Bootstrap 样本**, 记为 $\tilde X^{(1)} = \{\tilde x_1^{(1)}, \tilde x_2^{(1)}, \ldots, \tilde x_n^{(1)}\}$.
2. 重复上述步骤 $B$ 次, 得到 $B$ 组新的 Bootstrap 样本 $\tilde X^{(1)}, \tilde X^{(2)}, \ldots, \tilde X^{(B)}$.
3. 对于每一组 Bootstrap 样本 $\tilde X^{(b)}$, 我们可以计算出我们感兴趣的统计量, 例如中位数 $\text{Med}(\tilde X^{(b)})$. 这样我们就得到了 $B$ 个 Bootstrap 样本的中位数的估计值 $m_1=\text{Med}(\tilde X^{(1)}), m_2=\text{Med}(\tilde X^{(2)}), \ldots, m_B=\text{Med}(\tilde X^{(B)})$.
4. 最后, 我们可以通过这 $B$ 个 Bootstrap 样本的中位数的估计值 $m_1, m_2, \ldots, m_B$ 来计算出中位数的置信区间. 由于这$B$个估计值是从原始数据中有放回的抽取得到的, 因此我们可以认为这些估计值是对真实中位数的一个近似但又各不相同具有一定的随机性. 一种常见的计算方法是取这些估计值的分位数, 例如取 $\{m_1, m_2, \ldots, m_B\}$ 中的 2.5% 和 97.5% 分位数的那个$m_{(0.025)}$ 和 $m_{(0.975)}$ 作为置信区间的下界和上界, 即 $CI  = (m_{(0.025)}, m_{(0.975)})$.

## Evaluation of Forecasting Methods

我们已经介绍了很多预测方法, 因此我们同样需要一些方法来评估这些预测方法的好坏, 以便选择最佳的预测方法. 最直观的讲, 如果我们能够知道我们预测的 $\hat{x}_{n+h|n}$ 对应的时点的真实值 $x_{n+h}$, 那么我们可以直接计算预测值和真实值之间的误差, 即:
$$
\hat e_{n+h | n} = x_{n+h} - \hat{x}_{n+h|n}, \quad h = 1, 2, \ldots
$$

当然, 这往往是不可能的, 因为我们的目的就是预测未来的数据. 因此我们需要一些方法来“提前”评估我们的预测方法. 因此我们需要用历史数据来评估我们的预测方法. 

这可能会和机器学习中的 Train-Test Split 有些类似, 即我们对已有的数据进行分割, 一部分用来训练模型, 一部分用来测试模型. 但是仍然对于时间序列数据有一些特殊的地方, 例如我们不能随机的将数据分割, 因为时间序列数据有着时间的依赖性.

### LOOCV in Supervised Learning

首先简短回顾一下对于ML中一般的没有时间依赖性的数据的对模型的预测效果进行验证的方法, 方便起见这里以最简单的 LOOCV (Leave-One-Out Cross Validation) 为例. 

假设我们的数据集共包含 $n$ 个样本: $D=\{(x_1, y_1), (x_2, y_2), \ldots, (x_n, y_n)\}$, 其中 $x_i$ 是第 $i$ 个样本的特征, $y_i$ 是第 $i$ 个样本的标签. ML 中 Supervised Learning 的一个核心目的就是找到一个(或许很复杂的)函数 $f(x)$, 使得其能够很好的近似真实的标签 $y$, 即 $\hat f(x) \approx y$. 这样当我们有一个新的样本 $x^*$ 而没有对应的标签 $y^*$ 时, 我们可以用 $\hat f(x^*)$ 来预测 $y^*$.

然而, 我们如何评估我们的模型的好坏呢? 一个最简单的方法就是在我们用来拟合训练模型的数据集 $D$ 上进行测试. 但是这样的评估方法有一个很大的问题, 因为直观的讲模型某种意义上已经“看到”了这些数据, 因此在这些数据上的表现并不能很好的反映模型对于”没有见过“的新数据的预测能力. 因此我们需要一些方法来评估模型对于新数据的预测能力. 因此我们可以用 LOOCV 方法来评估模型的预测能力. 其具体步骤如下:
1. 对于数据集 $D=\{(x_1, y_1), (x_2, y_2), \ldots, (x_n, y_n)\}$, 我们每次随机抽取一个样本 $(x_j, y_j)$ 放在一边作为测试集 $D_{\text{test}} = \{(x_j, y_j)\}$, 其余的数据作为训练集 $D_{\text{train}} ^{(-j)} = D \setminus \{(x_j, y_j)\}$ (即去掉第 $j$ 个样本对儿的其余的数据). 我们用训练集 $D_{\text{train}} ^{(-j)}$ 来训练模型, 得到模型 $\hat f_{(-j)}(x)$.
2. 用测试集 $D _{\text{test}} = \{(x_j, y_j)\}$ 来测试模型 $\hat f_{(-j)}(x)$, 计算预测误差 $\hat e_j = \hat f_{(-j)}(x_j) - y_j$.
3. 重复上述步骤 $n$ 次, 得到 $n$ 个预测误差 $\hat e_1, \hat e_2, \ldots, \hat e_n$. 统计学上我们习惯用这些预测误差的平方和来评估模型的预测能力, 即样本 Mean Squared Error (MSE):
    $$
    \hat{\text{Err}}_{\text{LOOCV}} := \text{MSE} = \frac{1}{n} \sum_{j=1}^n \hat e_j^2
    $$
    可以证明, 如果有一个在全体数据(而没有leave-one-out)上的模型 $\hat f(x)$, 那么我们得到的这个 $\hat{\text{Err}}_{\text{LOOCV}}$ 是对于这个模型的预测均方误 $\mathbb{E}[(\hat f(x) - y)^2]$ 的一个良好估计. 

不过, 对于时间序列数据, 我们不能直接使用这个方法, 因为时间序列数据有着时间的依赖性 (*History matters!*). 我们不可以每次随机的将一个样本放在一边作为测试集, 这样得到的测试集和训练集之间的时间关系是不对的. 因此我们需要一些更为复杂的方法来评估时间序列数据的预测能力.

### Train-Test Split in Time Series

对于时间序列数据, 一个常用的方法是直接在时间轴上进行截断以得到训练集和测试集. 例如我们可以选取前 $m$ 个时间点的数据 $D_{\text{train}} = \{(x_1, y_1), (x_2, y_2), \ldots, (x_m, y_m)\}$  作为训练集得到模型 $\hat f(x)$, 然后用接下来的 $n-m$ 个时间点的数据 $D_{\text{test}} = \{(x_{m+1}, y_{m+1}), (x_{m+2}, y_{m+2}), \ldots, (x_n, y_n)\}$ 作为测试集来评估模型的预测能力得到预测误差 $\hat \epsilon_{m+1}, \hat \epsilon_{m+2}, \ldots, \hat \epsilon_n$. 

对于误差的度量除了 MSE 之外, 我们还可以用其他的一些度量来评估模型的预测能力 (这里为了方便, 重新记我们得到的 $p = n-m$ 个预测误差为 $\hat e_1, \hat e_2, \ldots, \hat e_{p}$)
- MSE (Mean Squared Error)
    $$\text{MSE} = \frac{1}{p} \sum_{i=1}^p \hat e_i^2$$
    - MSE 是误差的平方的平均值. 其单位是原始数据的单位的平方, 因此有时不太直观 
    - MSE 对于一些 outlier 很敏感. 如果有一些离群值产生了很大的误差, 那么 MSE 会很大, 从而被一两个有问题的数据值影响了整体的评估. (不过某些时候如果我们对于这些极端值很感兴趣, 那么 MSE 也是一个很好的度量)
- MAE (Mean Absolute Error)
    $$\text{MAE} = \frac{1}{p} \sum_{i=1}^p |\hat e_i|$$
    - MAE 是误差的绝对值的平均值. 其单位和原始数据的单位一致, 相较于 MSE 更为直观
    - MAE 对于极端值不敏感, 因此更为稳健
- RMSE (Root Mean Squared Error)
    $$\text{RMSE} = \sqrt{\frac{1}{p} \sum_{i=1}^p \hat e_i^2}$$
    - RMSE 是 MSE 的平方根, 其单位和原始数据的单位一致, 故也比较直观

有时我们还需要进一步对这些度量进行标准化 (scaling) 来去除单位(量纲)的影响, 从而得到一个类似于百分比变化的度量. 因此, 我们还有如下标准化误差 (Scaled Error) 的度量:
- MSSE (Mean Squared Scaled Error)
    $$\text{MSSE} = \frac{\text{MSE}}{ \frac{1}{m-1} \sum_{t=2}^m (x_t - x_{t-1})^2}$$
    - MSSE 的分子是在测试集上的 MSE, 分母是训练集上的类似于一个方差的量
- MASE (Mean Absolute Scaled Error)
    $$\text{MASE} = \frac{\text{MAE}}{ \frac{1}{m-1} \sum_{t=2}^m |x_t - x_{t-1}|}$$
    - MASE 的分子是在测试集上的 MAE, 分母是训练集上的类似于一个绝对值的方差的量

在我们考虑多个变量的时候, 由于不同变量的单位可能不同, 因此进行这样的标准化可以更好的比较不同变量的预测能力.

### Cross-Validation in Time Series

刚刚的LOOCV曾指出, 对于时间序列数据, 传统直接进行随机分组的交叉验证方法是不适用的. 其提及的**时间序列数据的时间依赖性**具体有如下几个表现方面:
- **时间序列数据是有序的**: 我们不能打乱时间序列数据的顺序进行抽样分组, 因为时间序列数据的顺序是有意义的.
- **随机抽样可能带来数据泄露(Data Leakage)**: 在普通的LOOCV中, 我们可能抽取了例如 ${x_1, x_3,x_4, x_5}$ 作为训练集, ${x_2}$ 作为测试集. 尽管这在普通的数据集中是没有问题的, 但是在时间序列数据中, 相当于我们已经知道了未来的数据 ($x_3, x_4, x_5$) 的信息而估计历史$x_2$的信息, 这是不合理的, 该问题被称为数据泄露(Data Leakage).
- **预测的时间跨度不同**: 在时间序列数据中, 往往不同的时间跨度的预测对应着不同的任务, 其有着非常不同的分布和特性. 例如, 预测下一秒钟的股票价格和预测明年的股票价格是完全不同的任务. 

***Rolling Forecasting Origin Validation*** 

因此一个针对于时间序列数据的交叉验证方法是 **Time Series Cross-Validation (TSCV)**, 或**Rolling Forecasting Origin Validation**. 其基本思想是:
1.  首先确定一个窗口大小 $w$和一个步长 $s$ (特别的, 常取$s=1$. 此时被叫做**一步向前预测** (One-Step Ahead Forecasting), 后面也用一步向前预测为例).
2.  从起点开始, 每次取一个长度为 $w$ 的窗口中的数据集作为训练集, 然后取下一个时间点的数据作为测试集, 训练模型, 预测下一个时间点的数据, 计算预测误差. 
3. 将窗口向前移动一个步长, 丢弃最早的一个数据, 加入一个新的数据, 重复上述步骤. 直到窗口到达数据集的末尾.

例如, 假设我们有$10$个时间点的数据集, 并设窗口大小为$6$, 步长为$1$, 则我们的TSCV的过程如下:
- 第一步: 用 $x_1, x_2, x_3, x_4, x_5, x_6$ 训练模型, 预测 $x_7$, 计算预测误差
- 第二步: 用 $x_2, x_3, x_4, x_5, x_6, x_7$ 训练模型, 预测 $x_8$, 计算预测误差
- 第三步: 用 $x_3, x_4, x_5, x_6, x_7, x_8$ 训练模型, 预测 $x_9$, 计算预测误差
- 第四步: 用 $x_4, x_5, x_6, x_7, x_8, x_9$ 训练模型, 预测 $x_{10}$, 计算预测误差

***Expanding Window Cross-Validation***

另一个常用的方法是 **Expanding Window Cross-Validation**. 其基本思想大致相同, 只不过每次在窗口移动时, 只加入新的数据, 而不再丢弃旧的数据. 这样可以更好的利用历史数据来训练模型. 下图是一个 Expanding Window Cross-Validation 的示意图, 其中蓝色的部分是训练集, 红色的部分是测试集. 灰色部分不参与任何计算. 预测的步长为4.

![Expanding Window Cross-Validation](https://yanshuo.quarto.pub/nus-ts-book/_images/cv4-1.png)