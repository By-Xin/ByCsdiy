{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cc6562c",
   "metadata": {},
   "source": [
    "# Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486aee15",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.\\\n",
    "    master(\"local[*]\").\\\n",
    "    config(\"spark.executor.memory\", \"4g\").\\\n",
    "    config(\"spark.driver.memory\", \"4g\").\\\n",
    "    config(\"spark.ui.showConsoleProgress\", \"false\").\\\n",
    "    appName(\"DataFrame\").\\\n",
    "    getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "# sc.setLogLevel(\"ERROR\")\n",
    "print(spark)\n",
    "print(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c810d39",
   "metadata": {},
   "source": [
    "### 数据读取"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4f00f9",
   "metadata": {},
   "source": [
    "JSON 文件可以直接使用 PySpark 读取："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a48ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = spark.read.json(\"data/lec14-danmu-144541892.json\", multiLine=True)\n",
    "df1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74bb1c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = spark.read.json(\"data/lec14-danmu-144541943.json\", multiLine=True)\n",
    "df2.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa4953c",
   "metadata": {},
   "source": [
    "对于 JSON 文件，PySpark 中一个非常实用的功能是将一列文件作为输入，其返回的结果会自动将所有文件合并："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84103698",
   "metadata": {},
   "outputs": [],
   "source": [
    "jsons = [\"data/lec14-danmu-144541892.json\", \"data/lec14-danmu-144541943.json\"]\n",
    "df3 = spark.read.json(jsons, multiLine=True)\n",
    "df3.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5ba0c2",
   "metadata": {},
   "source": [
    "我们将所有剧集的弹幕文件统一进行读取："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3916995a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cids = [144541892, 144541943, 160377038, 148952771, 150894103, 153392221, 156629080, 159982308, 162395026]\n",
    "cids = [144541892, 144541943, 160377038, 148952771, 150894103]\n",
    "jsons = [f\"data/lec14-danmu-{cid}.json\" for cid in cids]\n",
    "jsons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6d7de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json(jsons, multiLine=True)\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0620df7b",
   "metadata": {},
   "source": [
    "PySpark 还支持许多其他类型的数据，如常用的 CSV 等。请参考[官方文档](https://spark.apache.org/docs/latest/api/python/getting_started/quickstart_df.html#Getting-Data-in/out)。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe487d9",
   "metadata": {},
   "source": [
    "### 查看数据和结构"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1336c04f",
   "metadata": {},
   "source": [
    "使用 `show()` 函数可以打印数据的前若干行，注意数据的合并顺序与输入的文件列表不一定一致："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4e6a73",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b6f51d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.show(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198b1ab9",
   "metadata": {},
   "source": [
    "读取得到的对象其类型为 `DataFrame`："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc5d640",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7402e14f",
   "metadata": {},
   "source": [
    "用 `printSchema()` 可以打印出各变量的类型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfa67ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7924b2",
   "metadata": {},
   "source": [
    "`DataFrame` 本质上是对 RDD 的一种更高层的封装。我们可以直接取出 `DataFrame` 背后的 RDD："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8714addb",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = df.rdd\n",
    "print(rdd)\n",
    "print()\n",
    "print(type(rdd))\n",
    "print()\n",
    "print(rdd.getNumPartitions())\n",
    "print()\n",
    "print(rdd.toDebugString().decode(encoding=\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5783bc18",
   "metadata": {},
   "source": [
    "该 RDD 的元素类型为 `Row`："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09dd2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "r1 = rdd.first()\n",
    "print(r1)\n",
    "print()\n",
    "print(type(r1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f623b72",
   "metadata": {},
   "source": [
    "`Row` 对象类似于字典，可以用 key 取出其中的变量值。另一种方法是直接取属性："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed15e6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(r1[\"cid\"])\n",
    "print()\n",
    "print(r1.cid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f9e871",
   "metadata": {},
   "source": [
    "也可以直接转换为字典："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce01bfd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "r1.asDict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69bf9e0d",
   "metadata": {},
   "source": [
    "PySpark 的 `DataFrame` 也可以转换为 Pandas 的 `DataFrame`，但注意不要轻易对完整数据转换！必要时可以先使用 `limit()` 限制行数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d515bb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_pandas = df.limit(10).toPandas()\n",
    "df_pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7238d73",
   "metadata": {},
   "source": [
    "### 数据存储"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09655f70",
   "metadata": {},
   "source": [
    "`DataFrame` 可以用特定的格式保存到硬盘上，如 CSV："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8be93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在 Windows 系统上运行可能会报错\n",
    "df100 = df.limit(100)\n",
    "df100.write.csv(\"data/lec14-output.csv\", header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080317f8",
   "metadata": {},
   "source": [
    "### 选择操作"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0bfb99",
   "metadata": {},
   "source": [
    "选择列和选择行："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4311a60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\"content\").show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6872972",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(\"content\").show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bd55d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter(df.mode != 1).show(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3816578",
   "metadata": {},
   "source": [
    "`filter()` 可以替换为 `where()`，判断条件也可以用字符串表示（直接使用变量名）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165a9cb2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.where(df.mode != 1).show(5)\n",
    "df.where(\"mode != 1\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7f93ff",
   "metadata": {},
   "source": [
    "不同的操作可以串起来："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0527be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\"content\", \"mode\").where(\"mode != 1\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6910fb3b",
   "metadata": {},
   "source": [
    "`distinct()` 可以用来去重："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5e8fd2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.select(\"date\").distinct().show(n=5)\n",
    "\n",
    "df.select(\"date\").distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a7a1bb",
   "metadata": {},
   "source": [
    "### 数据变换"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90f597d",
   "metadata": {},
   "source": [
    "在 `DataFrame` 中可以根据已有的列变换得到新的列："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5716887d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.select(\"id\").withColumn(\"id_mod\", df.id % 10).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c8489c",
   "metadata": {},
   "source": [
    "PySpark 也提供了很多对列进行变换的函数，定义在 `pyspark.sql.functions` 中："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e892ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_format, from_unixtime\n",
    "\n",
    "df.select(\"post_time\").withColumn(\"post_time_formatted\", from_unixtime(df.post_time)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4014094",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\"date\").distinct().withColumn(\"date_formatted\", date_format(\"date\", \"yyyy.MM.dd\")).\\\n",
    "    withColumn(\"day_of_week\", date_format(\"date\", \"E\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ae8d2d",
   "metadata": {},
   "source": [
    "### 连接操作"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d31621",
   "metadata": {},
   "source": [
    "有时需要将两张或以上的表的信息进行连接，此时可以使用 `join()` 函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f868bfe",
   "metadata": {},
   "source": [
    "首先读取一份视频信息的 `DataFrame`："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c963ddd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_info = spark.read.json(\"data/lec14-video-data.json\", multiLine=True)\n",
    "video_title = video_info.select(\"cid\", \"title\")\n",
    "video_title.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d105f5c",
   "metadata": {},
   "source": [
    "我们希望在查看弹幕的时候，能够知道对应的剧集标题。此时以两张表的 `cid` 作为连接条件："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003dd473",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.join(video_title, df.cid == video_title.cid, \"inner\").drop(df.id).drop(video_title.cid).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5063adf7",
   "metadata": {},
   "source": [
    "### SQL 操作"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2eafc34",
   "metadata": {},
   "source": [
    "更灵活的数据操作可以使用 SQL 语句实现，例如汇总操作。简单统计每集有多少弹幕："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defc247a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将 df 注册为一张表\n",
    "df.createOrReplaceTempView(\"danmu\")\n",
    "\n",
    "danmu_stats = spark.sql(\"select cid, count(*) as num_of_danmu from danmu group by cid\")\n",
    "danmu_stats.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6280a42",
   "metadata": {},
   "source": [
    "连接操作："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a71b254",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "danmu_stats.createOrReplaceTempView(\"danmu_stats\")\n",
    "video_title.createOrReplaceTempView(\"video_title\")\n",
    "\n",
    "spark.sql(\"\"\"select * from danmu_stats inner join video_title\n",
    "             where danmu_stats.cid=video_title.cid\n",
    "             order by title\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fedf23a7",
   "metadata": {},
   "source": [
    "更多 SQL 语句的操作可以参考[这个教程](https://www.w3school.com.cn/sql/index.asp)。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
