{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 《深度学习》2022-2023学年第1学期期末考试"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 姓名"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "王诚嘉"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学号"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2019110093"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "提示与建议：\n",
    "\n",
    "1. 优先完成更有把握的题目；\n",
    "2. 对于简答题，回答核心要点即可，不要提供冗长的陈述；\n",
    "3. 对于编程题，优先完成整体的框架，有时间富余时再进行细节上的调试；\n",
    "4. 对于综合题，每一小问的题目本身可能就是对后续问题的提示；\n",
    "5. 综合题小问的难度不一定是逐渐上升的，先把题目读完，优先完成有把握的问题；\n",
    "6. 可参考课件及以往的作业，但注意与考题的差异，不要生搬硬套。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第1题：简答题（20分）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a) 某信用卡公司希望建立一个深度学习模型，通过分析申请者的背景信息来决定是否发放信用卡。如果使用一个前馈神经网络来建立模型，则该神经网络的输出层应该使用什么激活函数（包括不使用激活函数）？为什么？当该公司决定为某些申请者发放信用卡后，希望再建立一个神经网络，用来决定用户的最大借款额度，那么该神经网络的输出层应该使用什么激活函数（包括不使用激活函数）？为什么？（10分）**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "应该使用sigmoid激活函数，因为决定是否发放信用卡是二分类模型（发或者不发），因此可以采用sigmoid激活函数（值位于0-1之间）。\n",
    "可以采用Relu激活函数或者softplus激活函数，因为最大借款额度一定是一个大于0的值，且这两类激活函数具有计算简单且梯度不饱和的性质。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b) 在(a)问中，建立好两个模型之后，各自可以采用什么损失函数来进行模型训练？简要叙述其理由。（5分）**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一小问可以用逻辑回归的损失函数来进行模型训练，即取二分类模型似然函数的倒数\n",
    "第二小问可以采用最小二乘思想的损失函数，即线性模型的损失函数（利用该用户最大还款额度与我们所预测的值的差来定义）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c) 在一个典型的前馈神经网络中，每一层的隐藏神经元都要先经过一个激活函数后再传递给下一层。是否可以让某些层不经过激活而直接传递给下一层？为什么？（5分）**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "不可以，因为不用激活函数，神经元之间的传递永远是线性变换，因此该隐藏层是多余的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第2题：编程题（45分）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a) 利用 PyTorch 编写一个函数，计算 $d$ 维标准正态分布的对数密度函数 $\\log[p_Z(z)]$，其中**\n",
    "\n",
    "$$p_{Z}(z)=(2\\pi)^{-d/2}\\exp\\left(-\\frac{1}{2}\\Vert z\\Vert^{2}\\right).$$\n",
    "\n",
    "**该函数的输入 `z` 是一个 $n\\times d$ 的矩阵，输出 `l` 是一个 $n\\times 1$ 的向量，其中 `l` 的第 $i$ 个元素是 $\\log[p_Z(\\cdot)]$ 在 `z` 的第 $i$ 行上的取值。（5分）**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.4305, -0.3499,  0.4749,  0.9041, -0.7021,  1.5963],\n",
      "        [ 0.4228, -0.6940,  0.9672,  1.5569, -2.3860,  0.6994],\n",
      "        [-1.0325, -2.6043,  0.8177,  0.1459, -0.9558,  1.4745],\n",
      "        [ 0.5109, -0.2325,  0.3958,  0.8536,  0.8275, -1.0542],\n",
      "        [-0.7374, -0.4202,  0.3071, -1.2767,  0.2009,  0.0190]])\n",
      "tensor([ -7.7096, -10.6147, -11.3267,  -7.0118,  -6.7563])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "def log_normal_pdf(z):\n",
    "    Norm = torch.norm(z,dim=1)\n",
    "    d = z.size()[1]\n",
    "    pz = (2*math.pi)**(-d/2)*torch.exp(-1/2*Norm**2)\n",
    "    l = torch.log(pz)\n",
    "    return l\n",
    "    # 在此处完成函数实现\n",
    "\n",
    "# 测试结果\n",
    "torch.manual_seed(2023)\n",
    "z = torch.randn(5, 6)\n",
    "\n",
    "print(z)\n",
    "print(log_normal_pdf(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b) 利用 PyTorch 编写一个函数 `log_add_exp(x, y)`，其接收两个大小相同的 Tensor 作为输入（x 和 y），并逐元素计算 $f(x,y)=\\log(e^x+e^y)$，最后返回一个同样大小的 Tensor。请注意计算的数值稳定性，并在如下数据上测试计算结果。PyTorch 中提供了 `torch.logaddexp()` 函数用来实现该运算，你可以使用它验证你的结果，但不能直接调用它。（10分）**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "原式等价于 $\\log(e^{x-y}+1)+y $ \n",
    "\n",
    "$\\log(e^{y-x}+1)+x$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-9.9931e+02, -9.3069e+00,  6.9315e-01,  1.0693e+01,  1.0069e+02,\n",
      "         1.0000e+02,  1.0000e+01,  6.9315e-01,  1.0000e+01,  1.0000e+03])\n",
      "tensor([-9.9931e+02, -9.3069e+00,  6.9315e-01,  1.0693e+01,  1.0069e+02,\n",
      "         1.0000e+02,  1.0000e+01,  6.9315e-01,  1.0000e+01,  1.0000e+03])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def log_add_exp(x, y):\n",
    "    abs_m = torch.abs(x-y)\n",
    "    return torch.log(torch.exp(-abs_m)+1)+ torch.max(x,y)\n",
    "    # 在此处完成函数实现\n",
    "\n",
    "# 测试结果\n",
    "x = torch.tensor([-1000.0, -10.0, 0.0, 10.0, 100.0, -100.0, -10.0, 0.0, 10.0, 1000.0])\n",
    "y = torch.tensor([-1000.0, -10.0, 0.0, 10.0, 100.0, 100.0, 10.0, 0.0, -10.0, -1000.0])\n",
    "\n",
    "print(log_add_exp(x, y))\n",
    "print(torch.logaddexp(x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c) 给定矩阵 $X=(x_{ij})$ 和函数 $f(X)=\\log|\\det(Y'Y)|$，其中 $Y=(y_{ij})$, $y_{ij}=\\log(10+\\exp(x_{ij}))$，$\\det(A)$ 表示方阵 $A$ 的行列式，$|\\det(A)|$ 是 $\\det(A)$ 的绝对值。编写 PyTorch 程序，计算 $f(X)$ 关于 $X$ 的导数在 $X=X_0$ 处的取值，其中 $X_0$ 由如下代码给出。（10分）**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[90314.2031,  8579.7178,  4429.4839, 20963.8457, 25978.8438],\n",
      "        [ 8579.7178, 21031.3711, 19312.0215,  9452.1787, 16178.0156],\n",
      "        [ 4429.4839, 19312.0215, 55385.3867,  6005.9038,  1566.2573],\n",
      "        [20963.8457,  9452.1787,  6005.9038, 55093.4648, 20127.3984],\n",
      "        [25978.8438, 16178.0156,  1566.2573, 20127.3984, 34475.4922]],\n",
      "       grad_fn=<MmBackward0>)\n",
      "tensor(3.9088e+22, grad_fn=<DetLuBasedHelperBackward0>)\n",
      "tensor([[ 6.1308e-04, -1.7771e-19,  2.2686e-03,  3.7617e-03, -4.2362e-35],\n",
      "        [ 7.0432e-04, -7.6684e-03,  1.7234e-34, -1.0274e-04,  1.2052e-02],\n",
      "        [ 0.0000e+00,  6.0710e-03,  0.0000e+00,  0.0000e+00,  4.8538e-03],\n",
      "        [ 1.9263e-09,  1.7671e-02,  0.0000e+00,  3.3121e-22, -7.0877e-03],\n",
      "        [ 0.0000e+00,  7.7662e-03, -7.0065e-45,  1.5058e-32, -2.1457e-07],\n",
      "        [ 0.0000e+00,  0.0000e+00, -4.4620e-04,  5.8816e-03, -7.0065e-45],\n",
      "        [-5.9144e-04,  5.1088e-03,  0.0000e+00,  6.3688e-03, -3.8515e-03],\n",
      "        [ 7.4336e-03,  5.7173e-43,  0.0000e+00, -1.8102e-14,  0.0000e+00],\n",
      "        [ 3.7487e-04,  6.2881e-03,  2.7540e-03, -2.2399e-21, -7.3817e-33],\n",
      "        [-3.2028e-15, -4.2453e-03,  8.2968e-03,  0.0000e+00,  0.0000e+00]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(2023)\n",
    "X0 = 100.0 * torch.randn(10, 5)\n",
    "X0.requires_grad = True\n",
    "\n",
    "# 在此处完成代码实现\n",
    "Y = log_add_exp(torch.log(torch.tensor(10)),X0)\n",
    "YtY = torch.matmul(Y.t(),Y)\n",
    "print(YtY)\n",
    "detYtY = torch.det(YtY)\n",
    "print(detYtY)\n",
    "f = torch.log(torch.abs(detYtY))\n",
    "f.backward()\n",
    "print(X0.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d) 在有监督学习中，如果因变量的取值是非负整数，那么可以使用泊松分布来对因变量进行建模。已知泊松分布 $Y\\sim Poisson(\\lambda)$ 的概率质量函数为 $P(Y=y)=\\lambda^y e^{-\\lambda}/y!$，$y=0,1,\\ldots$。对于每一个因变量观测 $Y_i$，假定它服从均值为 $\\lambda_i>0$ 的泊松分布，其中 $\\lambda_i$ 依赖于自变量 $x_i$：**\n",
    "\n",
    "$$Y_i\\sim Poisson(\\lambda_i),\\quad\\lambda_i=f_\\theta(x_i),$$\n",
    "\n",
    "**这里 $f_\\theta(x)$ 是一个前馈神经网络，$\\theta$ 是网络的所有参数。**\n",
    "\n",
    "**给定自变量矩阵 $X_{n\\times p}$ 和因变量向量 $Y_{n\\times 1}$，我们希望对神经网络的参数进行估计，使其能够尽可能贴合数据。请编写 PyTorch 程序，完成以下任务：**\n",
    "\n",
    "1. 创建一个具有两个隐藏层的前馈神经网络用来表达 $f_\\theta(x)$，其中隐藏神经元数量分别为10和3，选择合适的激活函数，尤其是输出层的激活函数（注意 $\\lambda_i$ 的取值范围）；\n",
    "\n",
    "2. 利用极大似然准则推导出该模型的损失函数，进行必要的文字说明；\n",
    "\n",
    "3. 用程序实现损失函数，它以 `x` 和 `y` 作为参数，然后返回当前模型在输入数据上的损失函数取值。调用你编写的函数，计算它在如下模拟数据上的损失函数取值。（20分）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# 模拟数据\n",
    "torch.manual_seed(2023)\n",
    "X = torch.randn(1000, 30)\n",
    "Y = torch.poisson(10.0 * torch.rand(1000, 1))\n",
    "Y.requires_grad = True\n",
    "import torch.nn as nn\n",
    "\n",
    "class FFN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(FFN, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(in_features=input_dim, out_features=10)\n",
    "        self.fc2 = torch.nn.Linear(in_features=10,out_features=3)\n",
    "        self.fc3 = torch.nn.Linear(in_features=3,out_features=1)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = torch.tanh(x)\n",
    "        x = self.fc2(x)\n",
    "        x = torch.tanh(x)\n",
    "        x = self.fc3(x)\n",
    "        x = torch.relu(x)\n",
    "        \n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "torch.manual_seed(123)\n",
    "\n",
    "input_d = X.shape[1]\n",
    "\n",
    "model = FFN(input_dim=input_d)\n",
    "LambdaHat = model(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "取倒数，连乘，取对数，去掉不含参数的log连乘项，得到：\n",
    "$ l^* = \\sum \\lambda_i - \\sum y_i \\log \\lambda_i$;\n",
    "但注意由于这里是倒数，相当于反比关系，因此要再取一个相反数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "#可以利用泊松分布的密度函数的倒数来推出损失函数，即似然函数的倒数\n",
    "def possionloss(y,Lambda):\n",
    "    loglike = torch.mean(- Lambda + y*torch.log(Lambda+1e-10) ) # 极大似然\n",
    "    loss =  - loglike # 等价于极小化损失\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0, loss = 117.67192840576172\n",
      "iteration 5000, loss = 27.223745346069336\n",
      "iteration 10000, loss = 16.995023727416992\n",
      "iteration 15000, loss = 12.284420013427734\n",
      "iteration 20000, loss = 10.74178695678711\n",
      "iteration 25000, loss = 7.958779335021973\n",
      "iteration 30000, loss = 6.519991397857666\n",
      "iteration 35000, loss = 5.031817436218262\n",
      "iteration 40000, loss = 3.598118782043457\n",
      "iteration 45000, loss = 3.394955635070801\n",
      "iteration 50000, loss = 2.4461820125579834\n",
      "iteration 55000, loss = 2.2827858924865723\n",
      "iteration 60000, loss = 2.157288074493408\n",
      "iteration 65000, loss = 1.900415301322937\n",
      "iteration 70000, loss = 1.4616949558258057\n",
      "iteration 75000, loss = 1.3702839612960815\n",
      "iteration 80000, loss = 1.2871809005737305\n",
      "iteration 85000, loss = 1.208464503288269\n",
      "iteration 90000, loss = 1.1339985132217407\n",
      "iteration 95000, loss = 1.0623265504837036\n",
      "[Parameter containing:\n",
      "tensor([[-0.0573,  0.0162, -0.0755,  0.0441, -0.1429,  0.1578, -0.1169, -0.1450,\n",
      "         -0.0952,  0.0774, -0.1128,  0.0586, -0.1414, -0.0936, -0.0364, -0.0401,\n",
      "         -0.1821,  0.1018, -0.0271,  0.0860,  0.1310,  0.0138,  0.0228,  0.1705,\n",
      "         -0.0873,  0.0366, -0.0764,  0.1349,  0.1692, -0.1474],\n",
      "        [ 0.1312,  0.0922,  0.0564,  0.0618,  0.1170, -0.0304, -0.1654, -0.0561,\n",
      "         -0.1490,  0.0123,  0.0164, -0.0751, -0.0059,  0.1229, -0.0368,  0.0392,\n",
      "          0.0332,  0.0050,  0.1008,  0.1241,  0.0667,  0.0495,  0.0658,  0.0635,\n",
      "         -0.0397, -0.1262, -0.0938,  0.1339,  0.0061, -0.1620],\n",
      "        [ 0.0239,  0.1597, -0.1294, -0.0524, -0.0154,  0.0051,  0.0492,  0.0196,\n",
      "         -0.0359,  0.0973,  0.1882,  0.0410, -0.1393, -0.1529,  0.0282, -0.0401,\n",
      "          0.0990, -0.0903,  0.0422,  0.0704,  0.1059,  0.0581,  0.0768, -0.0175,\n",
      "         -0.0064, -0.0650, -0.1454,  0.1282,  0.0286,  0.0963],\n",
      "        [-0.1502,  0.1287,  0.0373,  0.1407, -0.0765,  0.0137,  0.1591,  0.1253,\n",
      "          0.1195, -0.0459,  0.1638, -0.1277,  0.0016, -0.1126, -0.0698, -0.0346,\n",
      "          0.0583,  0.1469,  0.0623, -0.0140,  0.1168, -0.0721,  0.1148, -0.0183,\n",
      "         -0.0334,  0.0431, -0.1368,  0.0114,  0.0311,  0.1326],\n",
      "        [-0.0661,  0.0589,  0.1532, -0.0352,  0.1549, -0.0876,  0.0625,  0.0730,\n",
      "          0.0234,  0.1262, -0.0339,  0.0538, -0.0011, -0.1279, -0.0676, -0.0977,\n",
      "          0.1152,  0.0233,  0.0019, -0.0558, -0.0096, -0.0408,  0.0688,  0.1647,\n",
      "          0.0246, -0.0443, -0.1085, -0.0043, -0.1524, -0.1601],\n",
      "        [-0.0977, -0.0519,  0.0283,  0.0835,  0.0657,  0.1474, -0.0884,  0.0070,\n",
      "         -0.1304,  0.0805, -0.0033,  0.1583, -0.1084,  0.1389,  0.1806,  0.1398,\n",
      "         -0.0209,  0.1467,  0.1150,  0.1502, -0.0152, -0.1330, -0.0297,  0.0262,\n",
      "         -0.1176, -0.1241, -0.1548, -0.0396,  0.1760,  0.0455],\n",
      "        [-0.0285,  0.0039,  0.0940, -0.1168, -0.1407, -0.0725, -0.1160,  0.0895,\n",
      "          0.1143, -0.1537, -0.1029, -0.1683,  0.0577,  0.1230, -0.1648, -0.0687,\n",
      "          0.0960, -0.1040,  0.1323,  0.0216, -0.0490,  0.0240,  0.0751, -0.0511,\n",
      "         -0.1126,  0.0318,  0.0008, -0.0137, -0.0562,  0.0828],\n",
      "        [-0.0576, -0.0149,  0.0465, -0.0674,  0.0363, -0.0361,  0.0599, -0.0896,\n",
      "         -0.0097,  0.0204, -0.0936, -0.1095,  0.0575, -0.0565,  0.0740, -0.1499,\n",
      "         -0.0309, -0.1180,  0.0540, -0.0249, -0.0441, -0.0156, -0.1104,  0.0481,\n",
      "          0.0151, -0.0755,  0.0633, -0.0194, -0.1811,  0.0674],\n",
      "        [ 0.1175, -0.0275, -0.0978, -0.0369,  0.1446,  0.1650, -0.1212, -0.0848,\n",
      "          0.1460,  0.0937,  0.0862, -0.0999,  0.1624, -0.1353,  0.0203, -0.0123,\n",
      "         -0.0143, -0.1414,  0.1763,  0.1766, -0.1224, -0.0083, -0.1034,  0.1775,\n",
      "          0.0957,  0.0722,  0.0567,  0.0030,  0.1148,  0.0289],\n",
      "        [-0.0528,  0.0848,  0.0821, -0.0719, -0.0323, -0.1371, -0.0233, -0.0742,\n",
      "         -0.0651, -0.1427,  0.1075, -0.1626,  0.1709, -0.0695,  0.0070,  0.1165,\n",
      "          0.0071, -0.0524, -0.1126, -0.0548,  0.1471, -0.0794,  0.1176, -0.0252,\n",
      "         -0.0113,  0.0340, -0.0254,  0.0109,  0.1292,  0.0822]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([ 0.2788, -0.2799,  0.2202, -0.2350, -0.3688,  0.3916, -0.3153, -0.4701,\n",
      "         0.2359,  0.3996], requires_grad=True), Parameter containing:\n",
      "tensor([[-0.0342,  0.1581, -0.2542,  0.3346, -0.0257, -0.0264, -0.0896,  0.0424,\n",
      "          0.1145, -0.2384],\n",
      "        [-0.2322, -0.0890,  0.1512,  0.2306,  0.2909, -0.3515,  0.1972,  0.2531,\n",
      "         -0.2303, -0.1389],\n",
      "        [ 0.0957, -0.2022, -0.0536,  0.2030, -0.1270,  0.0815, -0.2492, -0.3693,\n",
      "          0.3051,  0.2696]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.3547, -0.1008,  0.1559], requires_grad=True), Parameter containing:\n",
      "tensor([[-0.8431, -0.7107,  0.4759]], requires_grad=True), Parameter containing:\n",
      "tensor([0.0796], requires_grad=True)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f95b1c8c790>]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pylab as plt\n",
    "\n",
    "# 迭代次数\n",
    "nepoch = 100000\n",
    "# 学习率，即步长\n",
    "learning_rate = 0.005\n",
    "# 记录损失函数值\n",
    "losses = []\n",
    "\n",
    "opt = torch.optim.Adagrad(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for i in range(nepoch):\n",
    "    LamHat = model(X)\n",
    "    loss = possionloss(Y,LamHat)\n",
    "\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    if i % 5000 == 0:\n",
    "        print(f\"iteration {i}, loss = {loss.item()}\")\n",
    "\n",
    "print(list(model.parameters()))\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第3题：综合题（35分）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "标准化流模型（normalizing flow）是一类特殊的神经网络模型（见阅读材料），其最大的特点是具有可逆性。对于一个 $d$ 维的输入 $x\\in\\mathbb{R}^d$，流模型可以看作是一个 $\\mathbb{R}^d\\rightarrow\\mathbb{R}^d$ 的映射 $f_\\theta$，且 $f_\\theta^{-1}$ 存在，其中 $\\theta$ 是该映射的参数。我们一般希望 $z=f_\\theta(x)$ 和 $x=f_\\theta^{-1}(z)$ 都可高效地进行计算。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "流模型中一类常见的实现被称为 Real NVP，它按如下的方式定义一个基础的可逆映射 $f_\\theta$：对于 $d$ 维输入 $x=(x_1,\\ldots,x_d)'$，首先固定一个整数 $0<k<d$，然后令 $z=(z_1,\\ldots,z_d)'=f_\\theta(x)$，其中\n",
    "\n",
    "$$\\begin{align*}\n",
    "z_{1:k} & =x_{1:k},\\\\\n",
    "z_{(k+1):d} & =x_{(k+1):d}\\odot\\exp(\\sigma(x_{1:k}))+\\mu(x_{1:k}).\n",
    "\\end{align*}$$\n",
    "\n",
    "换言之，$x$ 的前 $k$ 个元素保持不变，而剩下的元素 $x_{(k+1):d}$ 将乘以一个等长的向量 $v$，再加上一个等长的向量 $u$ 得到 $z_{(k+1):d}$，其中 $u$ 和 $v$ 又是前 $k$ 个元素 $x_{1:k}$ 的函数，$v=\\exp(\\sigma(x_{1:k}))$，$u=\\mu(x_{1:k})$，此处 $\\mu(\\cdot)$ 和 $\\sigma(\\cdot)$ 是两个 $\\mathbb{R}^k\\rightarrow\\mathbb{R}^{d-k}$ 的前馈神经网络。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**在实际模型实现中，输入数据是一个 $n\\times d$ 的矩阵 $X$，而 $f_\\theta$ 将对 $X$ 的每一行进行上述变换。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Real NVP 还有一个重要的性质，就是其雅各比矩阵的行列式具有简单的形式：\n",
    "\n",
    "$$\\log\\left[\\left|\\det\\left(\\frac{\\partial f_\\theta}{\\partial x}\\right)\\right|\\right]=\\sum_{i=1}^{d-k}\\sigma_i(x_{1:k}),$$\n",
    "\n",
    "其中 $\\sigma_i(x_{1:k})$ 是 $\\sigma(x_{1:k})$ 输出的第 $i$ 个元素。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a) 在以下小问中我们始终让 $k=\\lfloor\\frac{d}{2}\\rfloor$，其中 $\\lfloor x\\rfloor$ 表示不超过 $x$ 的最大整数。请编写一个简单的函数用来计算 $k$。（3分）**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_k(d):\n",
    "    k=math.floor(d/2)\n",
    "    return k\n",
    "    # 在此处完成函数实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b) $\\mu(\\cdot)$ 和 $\\sigma(\\cdot)$ 具有相同的结构，因此我们可以定义一个统一的类，然后从这个类中生成两个前馈神经网络对象。要求该前馈神经网络的类具有两个隐藏层（不包括输入层和输出层），其神经元数量分别为32和16，使用 ReLU 作为激活函数。完成以下模块的构建。（7分）**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FeedForward(\n",
      "  (fc1): Linear(in_features=2, out_features=32, bias=True)\n",
      "  (fc2): Linear(in_features=32, out_features=16, bias=True)\n",
      "  (fc3): Linear(in_features=16, out_features=3, bias=True)\n",
      ")\n",
      "FeedForward(\n",
      "  (fc1): Linear(in_features=2, out_features=32, bias=True)\n",
      "  (fc2): Linear(in_features=32, out_features=16, bias=True)\n",
      "  (fc3): Linear(in_features=16, out_features=3, bias=True)\n",
      ")\n",
      "tensor([[0.0772, 0.0000, 0.1758],\n",
      "        [0.1746, 0.0000, 0.1949],\n",
      "        [0.1221, 0.0000, 0.1793]], grad_fn=<ReluBackward0>)\n",
      "tensor([[0.1531, 0.0000, 0.3252],\n",
      "        [0.1426, 0.0000, 0.2933],\n",
      "        [0.1256, 0.0000, 0.3138]], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d, k):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(in_features = k , out_features = 32)\n",
    "        self.fc2 = torch.nn.Linear(in_features = 32, out_features = 16)\n",
    "        self.fc3 = torch.nn.Linear(in_features = 16, out_features = d-k)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = torch.relu(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# 测试结果\n",
    "torch.manual_seed(2023)\n",
    "mu_fn = FeedForward(d=5, k=2)\n",
    "sigma_fn = FeedForward(d=5, k=2)\n",
    "x1k = torch.randn(3, 2)\n",
    "\n",
    "print(mu_fn)\n",
    "print(sigma_fn)\n",
    "print(mu_fn(x1k))\n",
    "print(sigma_fn(x1k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c) 利用以上模块，编写一个 Real NVP 的模块类，用来实现标准化流模型。该模块类的 `forward()` 函数接收一个 $n\\times d$ 的参数 `x`，然后返回变换的结果 `z`（$n\\times d$ 的矩阵）以及雅各比矩阵的对数行列式取值 `logdet`（$n\\times 1$ 的向量）。（15分）**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.4305, -0.3499,  0.4749,  0.9041, -0.7021,  1.5963,  0.4228],\n",
      "        [-0.6940,  0.9672,  1.5569, -2.3860,  0.6994, -1.0325, -2.6043],\n",
      "        [ 0.9337, -0.1050,  0.7427, -1.3397, -0.3649, -0.2325,  0.3958],\n",
      "        [ 0.8536, -0.4204, -1.4516,  1.0055, -0.1263, -0.3242, -1.2767],\n",
      "        [ 0.2009,  0.0190,  0.3041, -0.9213,  0.9191, -2.4946, -0.2740]])\n",
      "tensor([[ 0.4305, -0.3499,  0.4749,  0.3313,  0.0542,  0.3460,  0.0000],\n",
      "        [-0.6940,  0.9672,  1.5569,  0.1725,  0.0000, -0.0093,  0.0000],\n",
      "        [ 0.9337, -0.1050,  0.7427,  0.4098,  0.0075,  0.1292,  0.0000],\n",
      "        [ 0.8536, -0.4204, -1.4516,  0.1708,  0.2882, -0.0328,  0.0000],\n",
      "        [ 0.2009,  0.0190,  0.3041,  0.2833,  0.0825, -0.1165,  0.0000]],\n",
      "       grad_fn=<CatBackward0>)\n",
      "tensor([0.1158, 0.1513, 0.1030, 0.2318, 0.1394], grad_fn=<SumBackward1>)\n"
     ]
    }
   ],
   "source": [
    "class RealNVP(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(RealNVP, self).__init__()\n",
    "        self.mu = FeedForward(d=input_dim, k=get_k(input_dim))\n",
    "        self.sig = FeedForward(d=input_dim, k = get_k(input_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        d = x.shape[1]\n",
    "        k = get_k(d)\n",
    "        \n",
    "        x1 = x[:,:k]\n",
    "        x2 = x[:,k:]\n",
    "        \n",
    "        # get z1\n",
    "        z1 = x1\n",
    "        \n",
    "        # get z2\n",
    "        sig_out = self.sig(x1)\n",
    "        X2Sig = sig_out * x2\n",
    "        \n",
    "        mu_out = self.mu(x1)\n",
    "        X2SigMu = X2Sig + mu_out\n",
    "        \n",
    "        z2 = X2SigMu\n",
    "        \n",
    "        # concat [Z1:Z2]\n",
    "        z = torch.cat((z1,z2),1)\n",
    "        \n",
    "        # logdet\n",
    "        logdet = torch.sum(sig_out,dim=1)\n",
    "        \n",
    "        return z,logdet\n",
    "\n",
    "# 测试结果\n",
    "torch.manual_seed(2023)\n",
    "n = 5\n",
    "d = 7\n",
    "x = torch.randn(n, d)\n",
    "model = RealNVP(input_dim=d)\n",
    "z, logdet = model(x)\n",
    "\n",
    "print(x)\n",
    "print(z)\n",
    "print(logdet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d) 根据阅读材料中的介绍，我们用一个流模型 $f_\\theta$ 来表达一个 $d$ 维的密度函数，其中 $p_Z$ 是 $d$ 维标准正态分布的密度函数：**\n",
    "\n",
    "$$\n",
    "\\log[p_{X}(x)]=\\log\\left[\\left|\\det\\left(\\frac{\\partial f_\\theta}{\\partial x}\\right)\\right|\\right]+\\log[p_{Z}(f_\\theta(x))].\n",
    "$$\n",
    "\n",
    "**请利用上面建立的 Real NVP 模型，计算该分布模型在 (c) 中数据 `x` 上的对数似然函数值。（5分）**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在此处完成代码实现\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ -8.6021, -15.2171,  -8.1161,  -9.0880, -10.3555],\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from torch.distributions import MultivariateNormal\n",
    "\n",
    "def log_standard_normal_pdf(x):\n",
    "    \"\"\"\n",
    "    计算 d 维标准正态分布的概率密度函数\n",
    "    :param x: d 维向量或包含多个 d 维向量的张量\n",
    "    :return: 对应的概率密度\n",
    "    \"\"\"\n",
    "    d = x.size(-1)\n",
    "    # 创建一个多维标准正态分布（均值为0，协方差矩阵为单位矩阵）\n",
    "    distribution = MultivariateNormal(torch.zeros(d), torch.eye(d))\n",
    "    # 计算概率密度\n",
    "    return distribution.log_prob(x)\n",
    "\n",
    "x.shape\n",
    "logPz = log_standard_normal_pdf(x)\n",
    "logPx = logdet + logPz\n",
    "print(logPx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(e) 在上面定义的 Real NVP 模型中，$f_\\theta$ 的变换输出 $z$ 的前 $k$ 个分量与输入 $x$ 的前 $k$ 个分量是完全一致的，这会使得模型的表达能力受到限制。结合阅读材料，请用文字简述有哪些方法可以增强模型的表达能力，使其能刻画更复杂的非线性关系。（5分）**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "比如可以利用coupling flows，先选取一块保持不变，再利用神经网络训练另外一块在合并"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
