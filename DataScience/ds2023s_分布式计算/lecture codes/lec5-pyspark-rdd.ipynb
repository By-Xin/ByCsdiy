{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "33ca3697",
   "metadata": {},
   "source": [
    "# PySpark RDD"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dd303e5e",
   "metadata": {},
   "source": [
    "### 1. 准备工作"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4330a894",
   "metadata": {},
   "source": [
    "配置和启动 PySpark："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54d9bba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x0000023BCEA43F10>\n",
      "<SparkContext master=local[*] appName=PySpark RDD>\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "# 本地模式\n",
    "spark = SparkSession.builder.\\\n",
    "    master(\"local[*]\").\\\n",
    "    appName(\"PySpark RDD\").\\\n",
    "    getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "print(spark)\n",
    "print(sc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "963646ff",
   "metadata": {},
   "source": [
    "### 2. RDD"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9946dd8e",
   "metadata": {},
   "source": [
    "创建一个包含了数据的列表："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f99e954c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "vec = [math.sin(i + math.exp(i)) for i in range(100)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f82b5dbc",
   "metadata": {},
   "source": [
    "将其转换为分布式数据结构： \n",
    "\n",
    "`sc = spark.sparkContext`\n",
    "\n",
    "`sc.parallelize()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9ed1a90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[10] at readRDDFromFile at PythonRDD.scala:274"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat = sc.parallelize(vec)\n",
    "dat #dat 为 rdd 对象"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8ccf73ad",
   "metadata": {},
   "source": [
    "`dat` 的类型是 RDD（Resilient Distributed Dataset），是一种类似于迭代器的结构，可以看作是某种数据类型的容器。例如，`dat` 代表了一些数字的集合。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3407940c",
   "metadata": {},
   "source": [
    "类似于 Python 中原生的函数式编程工具，可以在 RDD 中使用 Map/Filter/Reduce 等操作。例如计算求和："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9795213b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.2461144364515757"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat.reduce(lambda x, y: x + y) #加和容器内的elem"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "170bff19",
   "metadata": {},
   "source": [
    "灵活使用 Map 函数计算均值：\n",
    "\n",
    "- rdd 可以进行`map` `reduce` `filter` 的串联，顺序即是从左到右\n",
    "- rdd 也可以无限套娃 -> 根本的原则在于只要返回的内容是一个新的rdd，就可以继续嵌套"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "192de0c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 -2.2461144364515757 45.977702048440904\n",
      "2.733223693874732\n"
     ]
    }
   ],
   "source": [
    "dat.map(lambda x: (1, x)).reduce(lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
    "n,sum,sqsum = dat.map(lambda x: (1, x, x*x)).reduce(lambda x, y: (x[0]+y[0],x[1]+y[1],x[2]+y[2]))\n",
    "print(n,sum,sqsum)\n",
    "var = (sqsum - n*sum)/(n-1)\n",
    "print(var)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0059cab6",
   "metadata": {},
   "source": [
    "Filter 操作："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d253c42a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28.0157344172721"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat.filter(lambda x: x > 0).reduce(lambda x, y: x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "16b0d217",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-30.261848853723677"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat.filter(lambda x: x <= 0).reduce(lambda x, y: x + y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "052eb3a4",
   "metadata": {},
   "source": [
    "使用 `collect()` 函数可以将 RDD 中的数据*全部取出*返回给 Python，***但对于大型数据请谨慎操作！***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fa7ba9cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8414709848078965,\n",
       " -0.5452515566923345,\n",
       " 0.035714265168052234,\n",
       " -0.8886479175586053,\n",
       " 0.8876009615390265,\n",
       " 0.5011099612213634,\n",
       " 0.8530218378956966,\n",
       " -0.804086324216863,\n",
       " -0.9644551022640215,\n",
       " 0.4721216528877472,\n",
       " 0.9723105121477627,\n",
       " 0.10237280614077035,\n",
       " 0.7682074937713861,\n",
       " 0.819078842327986,\n",
       " -0.7885252480190347,\n",
       " -0.8483650910372161,\n",
       " -0.2445565303431489,\n",
       " -0.8558519039634782,\n",
       " -0.10196456500793882,\n",
       " 0.14618338451195673,\n",
       " 0.9999672698820121,\n",
       " -0.07924816445805015,\n",
       " -0.10590380349630102,\n",
       " -0.5358980946771431,\n",
       " -0.3136640428099456,\n",
       " 0.7777700593006833,\n",
       " 0.9146442256184393,\n",
       " 0.48464438753683886,\n",
       " 0.11080525077159722,\n",
       " -0.9656357654018414,\n",
       " 0.9737279913519716,\n",
       " 0.5319589257495517,\n",
       " 0.9956230914517219,\n",
       " 0.48682123564309765,\n",
       " -0.48489790225635604,\n",
       " 0.5791514931370622,\n",
       " 0.15744970934574964,\n",
       " -0.9772162984957723,\n",
       " -0.27703274027791913,\n",
       " -0.774633895198441,\n",
       " -0.655895619972691,\n",
       " -0.1021743587186752,\n",
       " 0.25698429832699954,\n",
       " 0.8617085926612364,\n",
       " -0.9899871580091143,\n",
       " -0.9970349352676108,\n",
       " -0.9972719456409868,\n",
       " 0.28667367857218606,\n",
       " 0.9914056689461122,\n",
       " 0.8758191086384202,\n",
       " 0.47110776655601394,\n",
       " 0.9915906593803753,\n",
       " 0.283585074390262,\n",
       " -0.9250746204217681,\n",
       " -0.2760627085070803,\n",
       " -0.9605297369398103,\n",
       " 0.27663703848924504,\n",
       " 0.8376007223520099,\n",
       " 0.31529766772832263,\n",
       " 0.0467250474926968,\n",
       " 0.7301221780683019,\n",
       " -0.019807702907643797,\n",
       " 0.28166848857595156,\n",
       " -0.04132408178025465,\n",
       " 0.9994178624383043,\n",
       " -0.3297882131661022,\n",
       " 0.05249861368120975,\n",
       " -0.4173590172239696,\n",
       " 0.16581933154694034,\n",
       " -0.8634792702714021,\n",
       " -0.4333571589785714,\n",
       " 0.8760424061455578,\n",
       " -0.9689218043045077,\n",
       " -0.41308613000578315,\n",
       " -0.08583325804146333,\n",
       " -0.8348331411895921,\n",
       " 0.9904096809452724,\n",
       " 0.6738518871018663,\n",
       " -0.6467442207097087,\n",
       " -0.5523253959215197,\n",
       " 0.0946425530668779,\n",
       " 0.9326207308098783,\n",
       " -0.9712614277963193,\n",
       " 0.0890088907150176,\n",
       " -0.15045386899932991,\n",
       " -0.33052751754365,\n",
       " -0.9823608103996693,\n",
       " -0.9632260215147505,\n",
       " -0.055780196761602924,\n",
       " 0.09016685881531959,\n",
       " -0.9949953391221849,\n",
       " -0.9561153559383516,\n",
       " -0.922656523771298,\n",
       " -0.14976755114026755,\n",
       " 0.27182393869080274,\n",
       " 0.9978615748967212,\n",
       " -0.9973589836219446,\n",
       " 0.07587629588596118,\n",
       " 0.7870114601458702,\n",
       " -0.7206198329616429]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat.collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "79da68d7",
   "metadata": {},
   "source": [
    "RDD 还提供了许多便捷的函数，如 `count()` 用来计算数据/容器的大小，`take()` 返回前 `n` 个元素等等。完整的函数列表可以参考[官方文档](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.html)。\n",
    "\n",
    "- *本质来讲.count()代表的是rdd的容器长度，即能够next迭代的次数*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "26a55387",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat.count() "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "026de2cf",
   "metadata": {},
   "source": [
    "*`take()` `first()` *可以读取一部分内容*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b73c5f32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8414709848078965"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat.take(5)\n",
    "dat.first()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ced4bdee",
   "metadata": {},
   "source": [
    "### 3. RDD 文件操作"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b67ee1cd",
   "metadata": {},
   "source": [
    "利用 Numpy 创建一个矩阵，并写入文件：\n",
    "- *先定义运算，最后最后再进行读取*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cc36a5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "np.set_printoptions(linewidth=100)\n",
    "\n",
    "np.random.seed(123)\n",
    "n = 100\n",
    "p = 5\n",
    "mat = np.random.normal(size=(n, p))\n",
    "\n",
    "if not os.path.exists(\"data\"):\n",
    "    os.makedirs(\"data\", exist_ok=True)\n",
    "np.savetxt(\"data/mat_np.txt\", mat, fmt=\"%f\", delimiter=\"\\t\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8b44965a",
   "metadata": {},
   "source": [
    "PySpark 读取文件并进行一些简单操作："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4dfebd79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "\n",
      "-1.085631\t0.997345\t0.282978\t-1.506295\t-0.578600\n",
      "1.651437\t-2.426679\t-0.428913\t1.265936\t-0.866740\n",
      "-0.678886\t-0.094709\t1.491390\t-0.638902\t-0.443982\n",
      "-0.434351\t2.205930\t2.186786\t1.004054\t0.386186\n",
      "0.737369\t1.490732\t-0.935834\t1.175829\t-1.253881\n"
     ]
    }
   ],
   "source": [
    "file = sc.textFile(\"data/mat_np.txt\")\n",
    "\n",
    "# 打印矩阵行数\n",
    "print(file.count())\n",
    "\n",
    "# 空行\n",
    "print()\n",
    "\n",
    "# 打印前5行\n",
    "text = file.take(5)\n",
    "print(*text, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c8a8bd08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.085631\t0.997345\t0.282978\t-1.506295\t-0.578600\n",
      "1.651437\t-2.426679\t-0.428913\t1.265936\t-0.866740\n",
      "-0.678886\t-0.094709\t1.491390\t-0.638902\t-0.443982\n",
      "-0.434351\t2.205930\t2.186786\t1.004054\t0.386186\n",
      "0.737369\t1.490732\t-0.935834\t1.175829\t-1.253881\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'-1.085631\\t0.997345\\t0.282978\\t-1.506295\\t-0.578600'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(*(file.take(5)), sep=\"\\n\")\n",
    "file.first()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c0b4b35d",
   "metadata": {},
   "source": [
    "`file` 的类型也是 RDD。`file` 代表了一些字符串的集合，每个元素是矩阵文件中的一行："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b646dc1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.RDD'>\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(type(file))\n",
    "print(type(file.first()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "97f5ca80",
   "metadata": {},
   "source": [
    "我们可以对 RDD 进行变换，使一种元素类型的 RDD 变成另一种元素类型的 RDD。例如，将 `file` 中的每一个字符串变成一个 Numpy 向量，那么变换的结果就是以 Numpy.array 为类型的 RDD。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "596f1562",
   "metadata": {},
   "source": [
    "为此，我们需要先编写一个转换函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a7366799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.085631\t0.997345\t0.282978\t-1.506295\t-0.578600\n",
      "[-1.085631  0.997345  0.282978 -1.506295 -0.5786  ]\n"
     ]
    }
   ],
   "source": [
    "# str => np.array\n",
    "def str_to_vec(line):\n",
    "    # 分割字符串\n",
    "    str_vec = line.split(\"\\t\")\n",
    "    # 将每一个元素从字符串变成数值型\n",
    "    num_vec = map(lambda s: float(s), str_vec)\n",
    "    # 创建 Numpy 向量\n",
    "    return np.fromiter(num_vec, dtype=float)\n",
    "\n",
    "print(file.first())\n",
    "print(str_to_vec(file.first()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7f4c2362",
   "metadata": {},
   "source": [
    "也可以让 Numpy 直接对字符串进行转换："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0ae3d743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.085631\t0.997345\t0.282978\t-1.506295\t-0.578600\n",
      "[-1.085631  0.997345  0.282978 -1.506295 -0.5786  ]\n"
     ]
    }
   ],
   "source": [
    "# str => np.array\n",
    "def str_to_vec(line):\n",
    "    # 分割字符串\n",
    "    str_vec = line.split(\"\\t\")\n",
    "    # 让 Numpy 进行类型转换\n",
    "    return np.array(str_vec, dtype=float)\n",
    "\n",
    "print(file.first())\n",
    "print(str_to_vec(file.first()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "861e0bbe",
   "metadata": {},
   "source": [
    "生成新的 RDD："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4b201abc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.PipelinedRDD'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "dat = file.map(str_to_vec)\n",
    "print(type(dat))\n",
    "print(type(dat.first()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e7a88a30",
   "metadata": {},
   "source": [
    "RDD 的一般操作都支持："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bc058e3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.085631  0.997345  0.282978 -1.506295 -0.5786  ]\n",
      "\n",
      "[array([-1.085631,  0.997345,  0.282978, -1.506295, -0.5786  ]), array([ 1.651437, -2.426679, -0.428913,  1.265936, -0.86674 ]), array([-0.678886, -0.094709,  1.49139 , -0.638902, -0.443982])]\n",
      "\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "print(dat.first())\n",
    "print()\n",
    "print(dat.take(3))\n",
    "print()\n",
    "print(dat.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c9421077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-9.70826   0.832708 -3.945197 -1.719718 -4.781526]\n",
      "100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.0970826 ,  0.00832708, -0.03945197, -0.01719718, -0.04781526])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat = file.map(str_to_vec)\n",
    "arr, num = dat.map(lambda x: (x,1)).reduce(lambda x,y:(x[0]+y[0],x[1]+y[1]))\n",
    "print(arr)\n",
    "print(num)\n",
    "arr/num"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3d25f0ed",
   "metadata": {},
   "source": [
    "**参考R：tidyverse中的pipeline操作！**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6cfaea25",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/mat_np.txt\r\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:304)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:244)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332)\r\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:208)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:292)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:288)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:292)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:288)\r\n\tat org.apache.spark.api.python.PythonRDD.getPartitions(PythonRDD.scala:55)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:292)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:288)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1021)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1020)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: java.io.IOException: Input path does not exist: file:/mat_np.txt\r\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:278)\r\n\t... 34 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[47], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m n, xsum \u001b[39m=\u001b[39m sc\u001b[39m.\u001b[39;49mtextFile(\u001b[39m\"\u001b[39;49m\u001b[39m/mat_np.txt\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39;49m\\\n\u001b[0;32m      2\u001b[0m     \u001b[39mmap\u001b[39;49m(str_to_vec)\u001b[39m.\u001b[39;49m\\\n\u001b[0;32m      3\u001b[0m     \u001b[39mmap\u001b[39;49m(\u001b[39mlambda\u001b[39;49;00m x:(\u001b[39m1\u001b[39;49m,x))\u001b[39m.\u001b[39;49m\\\n\u001b[1;32m----> 4\u001b[0m     reduce(\u001b[39mlambda\u001b[39;49;00m x,y:(x[\u001b[39m0\u001b[39;49m]\u001b[39m+\u001b[39;49my[\u001b[39m0\u001b[39;49m],x[\u001b[39m1\u001b[39;49m]\u001b[39m+\u001b[39;49my[\u001b[39m1\u001b[39;49m]))\n\u001b[0;32m      5\u001b[0m xsum\u001b[39m/\u001b[39mn\n",
      "File \u001b[1;32mD:\\spark-3.3.2-bin-hadoop3\\python\\pyspark\\rdd.py:1250\u001b[0m, in \u001b[0;36mRDD.reduce\u001b[1;34m(self, f)\u001b[0m\n\u001b[0;32m   1247\u001b[0m         \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m   1248\u001b[0m     \u001b[39myield\u001b[39;00m reduce(f, iterator, initial)\n\u001b[1;32m-> 1250\u001b[0m vals \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmapPartitions(func)\u001b[39m.\u001b[39;49mcollect()\n\u001b[0;32m   1251\u001b[0m \u001b[39mif\u001b[39;00m vals:\n\u001b[0;32m   1252\u001b[0m     \u001b[39mreturn\u001b[39;00m reduce(f, vals)\n",
      "File \u001b[1;32mD:\\spark-3.3.2-bin-hadoop3\\python\\pyspark\\rdd.py:1197\u001b[0m, in \u001b[0;36mRDD.collect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1195\u001b[0m \u001b[39mwith\u001b[39;00m SCCallSiteSync(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontext):\n\u001b[0;32m   1196\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mctx\u001b[39m.\u001b[39m_jvm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1197\u001b[0m     sock_info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mctx\u001b[39m.\u001b[39;49m_jvm\u001b[39m.\u001b[39;49mPythonRDD\u001b[39m.\u001b[39;49mcollectAndServe(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jrdd\u001b[39m.\u001b[39;49mrdd())\n\u001b[0;32m   1198\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(_load_from_socket(sock_info, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[1;32mD:\\spark-3.3.2-bin-hadoop3\\python\\lib\\py4j-0.10.9.5-src.zip\\py4j\\java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1315\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[0;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[0;32m   1325\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[1;32mD:\\spark-3.3.2-bin-hadoop3\\python\\pyspark\\sql\\utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m    189\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 190\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39ma, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\n\u001b[0;32m    191\u001b[0m     \u001b[39mexcept\u001b[39;00m Py4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    192\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mD:\\spark-3.3.2-bin-hadoop3\\python\\lib\\py4j-0.10.9.5-src.zip\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/mat_np.txt\r\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:304)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:244)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332)\r\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:208)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:292)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:288)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:292)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:288)\r\n\tat org.apache.spark.api.python.PythonRDD.getPartitions(PythonRDD.scala:55)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:292)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:288)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1021)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1020)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: java.io.IOException: Input path does not exist: file:/mat_np.txt\r\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:278)\r\n\t... 34 more\r\n"
     ]
    }
   ],
   "source": [
    "n, xsum = sc.textFile(\"/mat_np.txt\").\\\n",
    "    map(str_to_vec).\\\n",
    "    map(lambda x:(1,x)).\\\n",
    "    reduce(lambda x,y:(x[0]+y[0],x[1]+y[1]))\n",
    "xsum/n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f5aefeb8",
   "metadata": {},
   "source": [
    "### 4. RDD 分区"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ba24f1a4",
   "metadata": {},
   "source": [
    "RDD 的一个重要功能在于可以分区（分块），从而支持分布式计算。查看 RDD 的分区数：\n",
    "- 如果逐条处理内容，则没有充分利用分布计算机的本身的计算（空间存储）性能，且会增加频繁通信的通信成本和其他运行成本\n",
    "- 时间复杂度和空间复杂度的Tradeoff（空间换时间）\n",
    "- 通过对于内存的配置，极大地利用了本地计算机的局域存储资源计算资源等\n",
    "- 这些分块后的内容是不是也可以并行计算？是的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1214a326",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file.getNumPartitions() # 根据数据的大小自动划定分区"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fd3d0021",
   "metadata": {},
   "source": [
    "还可以手动指定分区数，从而支持更高的并行度。注意调用 `repartition()` 函数不改变原有 RDD，要使用分区后的 RDD，需要重新赋值："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e4819bc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "file_p10 = file.repartition(10)\n",
    "print(file.getNumPartitions())\n",
    "print(file_p10.getNumPartitions())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1582f8a5",
   "metadata": {},
   "source": [
    "我们可以按分区对数据进行转换，例如将每个分区的数据转成 Numpy 矩阵。需要使用的函数是 `mapPartitions()`，其接收一个函数作为参数，该函数将对每个分区的**迭代器**进行变换。某些分区可能会是空集，需要做特殊处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2c697eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iter[str] => Iter[matrix]\n",
    "def part_to_mat(iterator):\n",
    "    # Iter[str] => Iter[np.array]\n",
    "    iter_arr = map(str_to_vec, iterator)\n",
    "\n",
    "\n",
    "    # Iter[np.array] => list(np.array)\n",
    "    dat = list(iter_arr) #dat是由向量（nparray）作为元素组成的列表（list）\n",
    "\n",
    "# 有的分区可能是空分区\n",
    "    # list(np.array) => matrix\n",
    "    if len(dat) < 1:  # Test zero iterator\n",
    "        mat = np.array([])\n",
    "    else:\n",
    "        mat = np.vstack(dat) \n",
    "\n",
    "    # matrix => Iter[matrix]\n",
    "    yield mat # yield可以认为是return[mat]，返回的不是mat本身，而是包含这个的容器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e9000313",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1 = np.array([1,2,3])\n",
    "v2 = np.array([3,4,5])\n",
    "vs = np.vstack([v1,v2])\n",
    "type(vs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8286bcdc",
   "metadata": {},
   "source": [
    "变换后的结果依然是一个 RDD，但此时元素类型变成了矩阵。\n",
    "\n",
    "***- 感觉是，分区一旦运行了那个命令，系统就自动把数据分好块了。如果对这些块做操作，那么就是mapPartition，如果直接map就还是单纯的对每行进行操作***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4facbf14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.PipelinedRDD'>\n",
      "\n",
      "[]\n",
      "\n",
      "[array([], dtype=float64), array([[-1.085631,  0.997345,  0.282978, -1.506295, -0.5786  ],\n",
      "       [ 1.651437, -2.426679, -0.428913,  1.265936, -0.86674 ],\n",
      "       [-0.678886, -0.094709,  1.49139 , -0.638902, -0.443982],\n",
      "       [-0.434351,  2.20593 ,  2.186786,  1.004054,  0.386186],\n",
      "       [ 0.737369,  1.490732, -0.935834,  1.175829, -1.253881],\n",
      "       [-0.637752,  0.907105, -1.428681, -0.140069, -0.861755],\n",
      "       [-0.255619, -2.798589, -1.771533, -0.699877,  0.927462],\n",
      "       [-0.173636,  0.002846,  0.688223, -0.879536,  0.283627],\n",
      "       [-0.805367, -1.727669, -0.3909  ,  0.573806,  0.338589],\n",
      "       [-0.01183 ,  2.392365,  0.412912,  0.978736,  2.238143]]), array([[-1.294085, -1.038788,  1.743712, -0.798063,  0.029683],\n",
      "       [ 1.069316,  0.890706,  1.754886,  1.495644,  1.069393],\n",
      "       [-0.772709,  0.794863,  0.314272, -1.326265,  1.417299],\n",
      "       [ 0.807237,  0.04549 , -0.233092, -1.198301,  0.199524],\n",
      "       [ 0.468439, -0.831155,  1.162204, -1.097203, -2.1231  ],\n",
      "       [ 1.039727, -0.403366, -0.12603 , -0.837517, -1.605963],\n",
      "       [ 1.255237, -0.688869,  1.660952,  0.807308, -0.314758],\n",
      "       [-1.085902, -0.732462, -1.212523,  2.087113,  0.164441],\n",
      "       [ 1.150206, -1.267352,  0.181035,  1.177862, -0.335011],\n",
      "       [ 1.031114, -1.084568, -1.363472,  0.379401, -0.379176]])]\n",
      "\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "dat_p10 = file_p10.mapPartitions(part_to_mat)\n",
    "# 原先是包含100行向量的rdd，转换后是包含10个矩阵的rdd\n",
    "print(type(dat_p10))\n",
    "print()\n",
    "print(dat_p10.first()) #第一个分区是空的，请留意这种边缘情况\n",
    "print()\n",
    "print(dat_p10.take(3))\n",
    "print()\n",
    "print(dat_p10.count())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "587fdbb0",
   "metadata": {},
   "source": [
    "我们可以用 `filter()` 过滤掉空的分区："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ecfacf71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.PipelinedRDD'>\n",
      "\n",
      "[[-1.085631  0.997345  0.282978 -1.506295 -0.5786  ]\n",
      " [ 1.651437 -2.426679 -0.428913  1.265936 -0.86674 ]\n",
      " [-0.678886 -0.094709  1.49139  -0.638902 -0.443982]\n",
      " [-0.434351  2.20593   2.186786  1.004054  0.386186]\n",
      " [ 0.737369  1.490732 -0.935834  1.175829 -1.253881]\n",
      " [-0.637752  0.907105 -1.428681 -0.140069 -0.861755]\n",
      " [-0.255619 -2.798589 -1.771533 -0.699877  0.927462]\n",
      " [-0.173636  0.002846  0.688223 -0.879536  0.283627]\n",
      " [-0.805367 -1.727669 -0.3909    0.573806  0.338589]\n",
      " [-0.01183   2.392365  0.412912  0.978736  2.238143]]\n",
      "\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "dat_p10_nonempty = dat_p10.filter(lambda x: x.shape[0] > 0)\n",
    "\n",
    "print(type(dat_p10_nonempty))\n",
    "print()\n",
    "print(dat_p10_nonempty.first())\n",
    "print()\n",
    "print(dat_p10_nonempty.count())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8beec4f5",
   "metadata": {},
   "source": [
    "### 5. RDD 操作案例 - 求列和"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "299cd239",
   "metadata": {},
   "source": [
    "np.array 版本的 RDD 求矩阵的列和："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "34cfe8fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-9.70826 ,  0.832708, -3.945197, -1.719718, -4.781526])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat.reduce(lambda x1, x2: x1 + x2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "074e9dc5",
   "metadata": {},
   "source": [
    "从输入矩阵文件开始，将操作串联："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0f55faa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-9.70826 ,  0.832708, -3.945197, -1.719718, -4.781526])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file.map(str_to_vec).reduce(lambda x1, x2: x1 + x2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e9226f55",
   "metadata": {},
   "source": [
    "使用分区版本的 RDD，先在每个分区上求列和："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9f16bb47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([-1.694266,  0.948677,  0.106428,  1.133682,  0.169049]),\n",
       " array([ 3.66858 , -4.315501,  3.881944,  0.689979, -1.877668]),\n",
       " array([-5.599247, -2.846053,  2.978673,  5.934997,  0.565914]),\n",
       " array([-5.60762 ,  0.977661, -5.157818, -2.868979, -0.570433]),\n",
       " array([-1.430094,  1.021662, -1.322512, -7.564743, -5.2081  ]),\n",
       " array([ 0.883702,  0.571757, -3.832934, -1.569966,  1.929   ]),\n",
       " array([ 0.070685,  4.474505, -0.598978,  2.525312,  0.210712])]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_part = dat_p10_nonempty.map(lambda x: np.sum(x, axis=0))\n",
    "sum_part.collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "17068800",
   "metadata": {},
   "source": [
    "再将分区结果汇总："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a6f2c006",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-9.70826 ,  0.832708, -3.945197, -1.719718, -4.781526])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_part.reduce(lambda x1, x2: x1 + x2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "729a7d01",
   "metadata": {},
   "source": [
    "从输入矩阵文件开始，将操作串联："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a78a2944",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-9.70826 ,  0.832708, -3.945197, -1.719718, -4.781526])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file.repartition(10).\\\n",
    "    mapPartitions(part_to_mat).\\\n",
    "    filter(lambda x: x.shape[0] > 0).\\\n",
    "    map(lambda x: np.sum(x, axis=0)).\\\n",
    "    reduce(lambda x1, x2: x1 + x2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7e21b056",
   "metadata": {},
   "source": [
    "使用真实值检验："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "231de070",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-9.7082586 ,  0.83270703, -3.94519179, -1.71971787, -4.78152553])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(mat, axis=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9caaf2e7",
   "metadata": {},
   "source": [
    "### 6. RDD 操作案例 - 矩阵乘法"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "684edfb9",
   "metadata": {},
   "source": [
    "模拟数据和真实值："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e6dcb844",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.65326187,  0.43284335, -0.83326669,  1.65616556,  0.47393998, -1.20594195, -1.09926452,\n",
       "       -0.24483357, -0.58399139,  2.91984625, -1.22159268,  2.99167578,  0.04907967,  0.00526486,\n",
       "       -1.78033411, -1.03704672,  1.27253333,  0.0280204 ,  0.88785436,  0.03485989,  1.45756374,\n",
       "       -1.26733834,  0.89596346, -0.65027554,  1.24724097,  0.01338995, -0.45613812,  1.06057634,\n",
       "        0.33513133,  0.30420446, -1.8306843 ,  0.81135409,  0.8563569 , -0.59189289, -0.58993733,\n",
       "        0.85925493,  0.20665867, -2.07373852,  0.23232788, -2.69748055,  1.19285523, -0.22831252,\n",
       "       -0.75495708,  1.04599886, -0.59922216, -2.14049979, -0.68492854,  0.13322705,  0.11576237,\n",
       "       -1.07628496,  0.98308603,  2.28403745,  0.31327103,  0.97450293, -2.19087869, -1.38414598,\n",
       "       -2.06428815, -1.19693787, -2.20837322,  1.79393849,  0.37940968,  0.98364566,  2.12782768,\n",
       "        0.17228872, -1.42418937, -0.66160026,  0.20736396, -0.42352417, -1.83096405,  0.75557361,\n",
       "       -1.87660221, -1.93437067, -0.51802796,  0.70099077, -2.27776851, -0.17137795, -0.77013413,\n",
       "       -0.33715716, -0.46570004, -0.22885299,  0.07744646,  0.65965659,  1.30432415, -3.05410919,\n",
       "       -1.55812228, -0.35166363, -0.26695372,  1.71736731,  1.42907711,  0.74512303, -1.17590892,\n",
       "        1.28153134,  0.34006662,  1.1969479 ,  1.68259996, -2.70844742, -0.21291717,  2.74992919,\n",
       "       -2.1979523 ,  0.60576651])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(123)\n",
    "v = np.random.uniform(size=p)\n",
    "res = mat.dot(v)\n",
    "res"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c2037ad4",
   "metadata": {},
   "source": [
    "np.array 版 RDD："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f47de328",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-1.6532623588006552,\n",
       " 0.43284380839732095,\n",
       " -0.8332665412164197,\n",
       " 1.656165481369684,\n",
       " 0.47393996910183156,\n",
       " -1.2059426466010321,\n",
       " -1.0992643850859096,\n",
       " -0.24483374428360488,\n",
       " -0.5839915890709256,\n",
       " 2.9198462372629264]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res1 = dat.map(lambda x: x.dot(v)).collect()\n",
    "res1[:10]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f2acc05f",
   "metadata": {},
   "source": [
    "分区版 RDD："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "733872af",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([-1.65326236,  0.43284381, -0.83326654,  1.65616548,  0.47393997, -1.20594265, -1.09926439,\n",
       "        -0.24483374, -0.58399159,  2.91984624]),\n",
       " array([-1.22159275,  2.99167581,  0.04907979,  0.0052652 , -1.78033393, -1.03704719,  1.27253296,\n",
       "         0.02802034,  0.88785453,  0.03485997]),\n",
       " array([ 1.45756404, -1.26733862,  0.89596327, -0.65027561,  1.24724115,  0.01338989, -0.45613776,\n",
       "         1.06057673,  0.33513193,  0.30420455,  2.28403732,  0.31327091,  0.97450361, -2.19087935,\n",
       "        -1.38414658, -2.06428804, -1.19693768, -2.20837397,  1.79393855,  0.37941031]),\n",
       " array([-1.8306849 ,  0.81135346,  0.85635656, -0.59189308, -0.58993783,  0.8592545 ,  0.20665878,\n",
       "        -2.07373867,  0.23232755, -2.69748044,  0.9836457 ,  2.12782845,  0.17228866, -1.42418964,\n",
       "        -0.66160031,  0.20736295, -0.4235236 , -1.83096434,  0.75557361, -1.87660252]),\n",
       " array([ 1.19285543, -0.22831212, -0.75495698,  1.04599886, -0.59922233, -2.14049959, -0.68492885,\n",
       "         0.13322687,  0.11576229, -1.07628444, -1.93437101, -0.51802806,  0.70099126, -2.27776847,\n",
       "        -0.17137845, -0.77013423, -0.33715737, -0.46569988, -0.22885317,  0.07744686]),\n",
       " array([ 0.98308645,  0.65965705,  1.30432399, -3.05410973, -1.55812232, -0.35166336, -0.26695394,\n",
       "         1.7173679 ,  1.42907711,  0.74512261, -1.17590882]),\n",
       " array([ 1.28153159,  0.34006666,  1.19694819,  1.68260023, -2.70844726, -0.21291761,  2.74992875,\n",
       "        -2.1979517 ,  0.60576649])]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_part = dat_p10_nonempty.map(lambda x: x.dot(v)).collect()\n",
    "res_part\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "04bd0404",
   "metadata": {},
   "source": [
    "拼接分区结果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "335b21e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.65326236,  0.43284381, -0.83326654,  1.65616548,  0.47393997, -1.20594265, -1.09926439,\n",
       "       -0.24483374, -0.58399159,  2.91984624, -1.22159275,  2.99167581,  0.04907979,  0.0052652 ,\n",
       "       -1.78033393, -1.03704719,  1.27253296,  0.02802034,  0.88785453,  0.03485997,  1.45756404,\n",
       "       -1.26733862,  0.89596327, -0.65027561,  1.24724115,  0.01338989, -0.45613776,  1.06057673,\n",
       "        0.33513193,  0.30420455,  2.28403732,  0.31327091,  0.97450361, -2.19087935, -1.38414658,\n",
       "       -2.06428804, -1.19693768, -2.20837397,  1.79393855,  0.37941031, -1.8306849 ,  0.81135346,\n",
       "        0.85635656, -0.59189308, -0.58993783,  0.8592545 ,  0.20665878, -2.07373867,  0.23232755,\n",
       "       -2.69748044,  0.9836457 ,  2.12782845,  0.17228866, -1.42418964, -0.66160031,  0.20736295,\n",
       "       -0.4235236 , -1.83096434,  0.75557361, -1.87660252,  1.19285543, -0.22831212, -0.75495698,\n",
       "        1.04599886, -0.59922233, -2.14049959, -0.68492885,  0.13322687,  0.11576229, -1.07628444,\n",
       "       -1.93437101, -0.51802806,  0.70099126, -2.27776847, -0.17137845, -0.77013423, -0.33715737,\n",
       "       -0.46569988, -0.22885317,  0.07744686,  0.98308645,  0.65965705,  1.30432399, -3.05410973,\n",
       "       -1.55812232, -0.35166336, -0.26695394,  1.7173679 ,  1.42907711,  0.74512261, -1.17590882,\n",
       "        1.28153159,  0.34006666,  1.19694819,  1.68260023, -2.70844726, -0.21291761,  2.74992875,\n",
       "       -2.1979517 ,  0.60576649])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.concatenate(res_part)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2ea100bd",
   "metadata": {},
   "source": [
    "关闭 Spark 连接："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e9f36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
