层归一化（Layer Normalization）在深度学习网络中的位置通常取决于网络的类型和特定的应用场景。然而，有几个一般性的指导原则可以帮助确定层归一化的最佳位置：

1. **在激活函数之前或之后**：
   - 在许多情况下，层归一化被应用于激活函数之前。这是因为归一化有助于调节进入激活函数的数据分布，从而防止数据在经过非线性激活时发生饱和。
   - 然而，在某些网络架构中，如Transformer模型，层归一化通常被放置在激活函数之后。

2. **在全连接层或卷积层之后**：
   - 对于全连接网络，层归一化通常在每个全连接层之后执行。
   - 在卷积神经网络（CNN）中，层归一化可以用于卷积层之后，尤其是当批量大小很小时或者要求模型对批量大小不敏感时。

3. **在循环神经网络（RNN）中**：
   - 在RNN或其变种（如LSTM或GRU）中，层归一化可以应用于循环层的隐藏状态。这有助于稳定长期依赖关系的学习。

4. **顺序问题**：
   - 在某些架构中，如Transformer，层归一化的顺序和位置对性能有显著影响。例如，标准的Transformer模型中，层归一化通常在自注意力和前馈网络的每个子层之后使用，并且在残差连接之前。

5. **实验调整**：
   - 最后，层归一化的具体位置可能需要根据具体任务和网络架构进行实验性地调整。不同的位置可能会对网络的学习动态和最终性能产生不同的影响。

综上所述，层归一化的位置在不同类型的网络和不同的应用场景中可能有所不同，通常需要根据具体情况进行调整和优化。