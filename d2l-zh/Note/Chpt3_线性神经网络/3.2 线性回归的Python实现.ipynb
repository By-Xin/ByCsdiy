{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 从零开始的Python实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import random\n",
    "import torch\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 生成数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 数据描述：拟生成一个包含1000个样本，2个特征的数据集，即\n",
    "  $$\\bold{X}\\in\\mathbb{R}^{1000\\times2},~\\bold{w}=[2,-3.4]^T,~b=4.2$$\n",
    "- 生成规则：\n",
    "  $$ \\bold{y} = \\bold{X}\\bold{w} + b + \\bold{\\epsilon} $$\n",
    "  *where* $\\bold{X}\\sim \\mathcal{N}(0,1),~\\epsilon \\sim \\mathcal{N}(0,0.01)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Example] \n",
      "features: tensor([ 0.3647, -0.3397]) \n",
      "label: tensor([6.0735])\n"
     ]
    }
   ],
   "source": [
    "# ----- 生成数据集 ------\n",
    "def gen_data(w,b,sample_size):\n",
    "    \"\"\" Rules: y = Xw + b + eps \"\"\"\n",
    "    X = torch.normal( 0 , 1 , (sample_size,len(w)) ) # 第三个参数指定参数形状 \n",
    "    y = torch.matmul(X,w) + b\n",
    "    y += torch.normal(0,0.01,y.shape)\n",
    "    return X,y.reshape((-1,1)) \n",
    "\n",
    "true_w = torch.tensor([2,-3.4]) # 设定模拟的真实w\n",
    "true_b = 4.2 # 设定模拟的真实b\n",
    "\n",
    "features,labels = gen_data(true_w,true_b,1000) # 生成1000个样本，features是X，labels是y\n",
    "\n",
    "# ----- 结果测试 ------\n",
    "print(f'[Example] \\nfeatures:',features[0],'\\nlabel:',labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.5533,  1.1319],\n",
      "        [ 0.4434, -0.6582]]) tensor([[1.4726],\n",
      "        [7.3262]])\n"
     ]
    }
   ],
   "source": [
    "# ----- 读取数据 ------\n",
    "def data_iter(batch_size,features,labels):\n",
    "    \"\"\"生成一个小批量样本迭代器\"\"\"\n",
    "    sample_size = len(features)  #获取样本总数\n",
    "    indices = list(range(sample_size)) # 生成一个样本索引的列表\n",
    "    random.shuffle(indices) # 将样本索引列表打乱，然后只取出打乱后的前batch_size个索引\n",
    "\n",
    "    for i in range(0, sample_size, batch_size): # 这里的i相当于是一个batch的起始索引，每调用一次data_iter函数，就会向下确定一个这组batch的索引的起点标号\n",
    "        batch_indices = torch.tensor(\n",
    "            indices[i:min(i + batch_size, sample_size)] ) # 从起点i开始，抽取batch_size个样本，除非已经到达最后一个样本\n",
    "        yield features[batch_indices], labels[batch_indices] # 生成一个batch的样本\n",
    "    \n",
    "# ----- 结果测试 ------\n",
    "demo_batch_size = 2\n",
    "iter = data_iter(demo_batch_size,features,labels)\n",
    "print(*next(iter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 说明：上述python自带的默认迭代器的执行效率较低（例如其需要将数据全部加载到内存中等）。因而在实际使用中通常会用PyTorch等框架中提供的数据迭代器来更高效地读取数据。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 参数初始化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这里，我们初始化参数模型为：w为正态分布随机取样数据，b为0向量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.normal(0, 0.01, size=(2,1), requires_grad=True) # 见下方解释\n",
    "b = torch.zeros(1, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**说明**：\n",
    "- requires_grad=True 指示 PyTorch 在计算张量 w 的梯度时要跟踪它。也就是说，PyTorch 会记录所有与 w 相关的操作，以便在后续的反向传播过程中计算梯度。这对于训练神经网络非常重要，因为在训练过程中需要计算模型参数相对于损失函数的梯度，以便更新这些参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 补充：自动微分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**说明：**\n",
    "- 根据设计好的模型，系统会构建一个计算图 *(computational graph)*， 来跟踪计算是哪些数据通过哪些操作组合起来产生输出。\n",
    "- 自动微分使系统能够随后反向传播梯度。 这里，反向传播（backpropagate）意味着跟踪整个计算图，填充关于每个参数的偏导数 *（反向传播有关内容详见第四章）*。\n",
    "\n",
    "这里通过对$y = 2\\bold{x}^T\\bold{x}$求导，来说明自动微分的过程。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. 初始化x**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 2., 3.])\n"
     ]
    }
   ],
   "source": [
    "# 初始化 x\n",
    "import torch\n",
    "x = torch.arange(4.0)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. 初始化梯度容器**\n",
    "\n",
    "重要的是，我们不会在每次对一个参数求导时都分配新的内存。 因为我们经常会成千上万次地更新相同的参数，每次都分配新的内存可能很快就会将内存耗尽。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "x.requires_grad_(True)  # 等价于x=torch.arange(4.0,requires_grad=True)\n",
    "x.grad  # 默认值是None\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.计算y**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(28., grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y = 2 * torch.dot(x, x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. 计算梯度**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数学上：$ y = 2\\bold{x}^T\\bold{x}  = 2x_1^2 + 2x_2^2 + ... + 2x_n^2 $，因而有：${\\partial y}/{\\partial \\bold{x}} = [4x_1, 4x_2, ..., 4x_n]$。\n",
    "\n",
    "特别地，$y$这里在点$\\bold{x} = [0,1,2,3]^T$处取值，因此其梯度为$[0,4,8,12]^T$。这里通过PyTorch的自动微分功能来求解梯度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.,  4.,  8., 12.])\n"
     ]
    }
   ],
   "source": [
    "y.backward() # 通过调用反向传播函数来自动计算y关于x每个分量的梯度\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "再举一例，设$y = \\sum x_i$，则有$\\partial y/\\partial \\bold{x} = [1,1,...,1]^T$。\n",
    "在PyTorch中，有："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "x.grad.zero_() # 清除x的梯度(否则默认会累加)\n",
    "y = x.sum()\n",
    "y.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 定义模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里涉及到Python的*广播机制*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linreg(X, w, b):  #@save\n",
    "    \"\"\"线性回归模型\"\"\"\n",
    "    return torch.matmul(X, w) + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 定义Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squared_loss(y_hat, y):  #@save\n",
    "    \"\"\"均方损失\"\"\"\n",
    "    return (y_hat - y.reshape(y_hat.shape)) ** 2 / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 定义优化算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此处为Mini-batch SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGD(params, lr, batch_size):  #@save\n",
    "    \"\"\"小批量随机梯度下降\n",
    "    params: 模型参数\n",
    "    lr: 学习率\n",
    "    batch_size: 批量大小\n",
    "    \"\"\"\n",
    "    with torch.no_grad(): # 见下文详细介绍\n",
    "        for param in params: # 随机梯度下降是对样本取样，但是每次iter都会更新全部参数\n",
    "            # GD的更新公式是：param := param - lr * param.grad\n",
    "            param -= lr * param.grad / batch_size # lr/batch_size是针对batch-size来规范学习率\n",
    "            param.grad.zero_()  # 梯度清零"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**说明：**\n",
    "\n",
    "1. `torch.no_grad():`的作用是在该语句块中，不会对`requires_grad=True`的tensor进行求导.\n",
    "2. `with ... `的作用是添加了一个上下文管理器，它可以创建一个上下文，执行一些代码，并在代码块结束时自动清理资源；在这里with torch.no_grad(): 创建了一个上下文，它告诉PyTorch在这个上下文中不要计算梯度。当代码块结束时，PyTorch会自动恢复梯度计算。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7 模型训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "大致的训练流程概括如下：\n",
    "\n",
    "- 初始化模型参数\n",
    "  \n",
    "- 重复一下训练，直到收敛：\n",
    "  - 计算梯度\n",
    "  $$ g:= \\nabla_{\\boldsymbol{\\theta}}\\frac{1}{|\\mathcal{B}|}\\sum_{i\\in\\mathcal{B}}L(f(\\boldsymbol{x}_i;\\boldsymbol{\\theta}),y_i) $$\n",
    "  - 更新参数\n",
    "  $$ \\boldsymbol{\\theta} := \\boldsymbol{\\theta} - \\eta g $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中，每一个迭代周期就成为一个epoch。 我们通过调用data_iter()将整个数据集进行遍历（这里假设batch的划分是可以正好整除的）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 超参数与模型设定\n",
    "batch_size = 3\n",
    "lr = 0.03\n",
    "num_epochs = 10\n",
    "net = linreg\n",
    "loss = squared_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 说明：（这里linreg,squared_loss都是上文提到的函数，由于上面#@save所以这里不用再单独定义）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.000054\n",
      "epoch 2, loss 0.000054\n",
      "epoch 3, loss 0.000055\n",
      "epoch 4, loss 0.000053\n",
      "epoch 5, loss 0.000054\n",
      "epoch 6, loss 0.000054\n",
      "epoch 7, loss 0.000054\n",
      "epoch 8, loss 0.000054\n",
      "epoch 9, loss 0.000053\n",
      "epoch 10, loss 0.000054\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs): # 对于规定的epoch数，进行迭代\n",
    "\n",
    "    # 迭代过程：抽样，根据样本进行GD更新\n",
    "    for X, y in data_iter(batch_size, features, labels): # 通过data_iter函数，每次迭代都会生成一个batch的样本\n",
    "        l = loss(net(X, w, b), y)  # 计算损失, loss=squared_loss, net=linreg\n",
    "        l.sum().backward()  # 求和并反向传播，详见说明1\n",
    "        SGD([w, b], lr, batch_size)  # 使用参数的梯度更新参数\n",
    "\n",
    "    # 输出每次epoch的迭代结果（损失）\n",
    "    with torch.no_grad(): # 防止梯度累加\n",
    "        train_l = loss(net(features, w, b), labels) # 这里的w,b是正在迭代更新的参数，features与labels是全局的（而非batch）\n",
    "        print(f'epoch {epoch + 1}, loss {float(train_l.mean()):f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**说明：**\n",
    "1. `l.sum().backward()`中，求和是因为，通过`loss`得到的损失函数是针对每个sample都有一个损失，故求和得到一个标量（这在统计中也是这样操作的）。这里也联系到，求导和求和的可交换性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.8 结果评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w的估计误差: tensor([0.0011, 0.0007], grad_fn=<SubBackward0>)\n",
      "b的估计误差: tensor([0.0006], grad_fn=<RsubBackward1>)\n"
     ]
    }
   ],
   "source": [
    "print(f'w的估计误差: {true_w - w.reshape(true_w.shape)}')\n",
    "print(f'b的估计误差: {true_b - b}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
