{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size = 20\n",
      "Vocab = ['about', 'and', 'apples', 'bananas', 'books', 'cartoons', 'cat', 'eat', 'fish', 'i', 'john', 'like', 'likes', 'loves', 'movies', 'python', 'read', 'the', 'to', 'watch']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# 固定随机种子，保证演示可复现\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# 一点简短的 toy corpus\n",
    "corpus = [\n",
    "    \"i like to eat apples and bananas\",\n",
    "    \"i like to watch movies and cartoons\",\n",
    "    \"the cat likes to eat fish\",\n",
    "    \"john loves to read books about python\"\n",
    "]\n",
    "\n",
    "# 1) 分词\n",
    "tokenized_sentences = [sent.lower().split() for sent in corpus]\n",
    "\n",
    "# 2) 汇总到一个 list，得到所有单词序列\n",
    "all_tokens = []\n",
    "for tokens in tokenized_sentences:\n",
    "    all_tokens.extend(tokens)\n",
    "\n",
    "# 3) 构造词表\n",
    "word_counter = Counter(all_tokens)\n",
    "vocab = sorted(word_counter.keys())  \n",
    "word2idx = {w: i for i, w in enumerate(vocab)}\n",
    "idx2word = {i: w for w, i in word2idx.items()}\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "print(f\"Vocabulary size = {vocab_size}\")\n",
    "print(\"Vocab =\", vocab)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 生成 (center, outside) Skip-gram 数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total skip-gram pairs: 84\n",
      "Example pairs (center_idx, outside_idx): [(9, 11), (9, 18), (11, 9), (11, 18), (11, 7), (18, 9), (18, 11), (18, 7), (18, 2), (7, 11)]\n"
     ]
    }
   ],
   "source": [
    "def make_skipgram_data(tokenized_sentences, word2idx, window_size=2):\n",
    "    \"\"\"\n",
    "    根据 tokenized_sentences，生成 (center, outside) 对儿列表。\n",
    "    window_size 表示上下文窗口大小 (左右各 window_size 个).\n",
    "    \"\"\"\n",
    "    pairs = []\n",
    "    for tokens in tokenized_sentences:\n",
    "        token_ids = [word2idx[w] for w in tokens]\n",
    "        length = len(token_ids)\n",
    "        for i, center_id in enumerate(token_ids):\n",
    "            start = max(i - window_size, 0)\n",
    "            end = min(i + window_size + 1, length)\n",
    "            for j in range(start, end):\n",
    "                if j != i:\n",
    "                    outside_id = token_ids[j]\n",
    "                    pairs.append((center_id, outside_id))\n",
    "    return pairs\n",
    "\n",
    "window_size = 2\n",
    "skipgram_pairs = make_skipgram_data(tokenized_sentences, word2idx, window_size)\n",
    "print(f\"Total skip-gram pairs: {len(skipgram_pairs)}\")\n",
    "print(\"Example pairs (center_idx, outside_idx):\", skipgram_pairs[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中, 这个 pair (center, outside) 是我们的训练数据, center 是我们的输入, outside 是我们的输出. 在训练的时候, 我们希望通过输入 center 来预测输出 outside."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 定义模型：SkipGramFullSoftmax\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **输入 (center_word)**：从 `in_embed` 查到中心词向量 $\\mathbf{v}_c$，形状是 $(B, d)$（如果一次喂 batch_size=B）。  \n",
    "2. **输出 logits**：对全部 `out_embed.weight` 做一次矩阵乘法。  \n",
    "   - `out_embed.weight` 的形状是 $(V, d)$  \n",
    "   - $\\mathbf{v}_c$ 的形状是 $(B, d)$  \n",
    "   - 矩阵乘法后得到 $(B, V)$ 的 logits 矩阵。  \n",
    "3. 对这 $(B, V)$ 矩阵做 **CrossEntropyLoss**，目标是 `outside_word_ids`（形状 $(B,)$），表示“正解在词表中哪个词的位置”。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGramFullSoftmax(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "        # 中心词 embedding\n",
    "        self.in_embed = nn.Embedding(vocab_size, embed_dim)\n",
    "        # 上下文词 embedding\n",
    "        self.out_embed = nn.Embedding(vocab_size, embed_dim)\n",
    "        # 这里的embedding就类似于one-hot编码.\n",
    "        \n",
    "        # 可以自行初始化，也可以用默认的\n",
    "        nn.init.uniform_(self.in_embed.weight, a=-0.5, b=0.5)\n",
    "        nn.init.uniform_(self.out_embed.weight, a=-0.5, b=0.5)\n",
    "        \n",
    "    def forward(self, center_word_ids, outside_word_ids):\n",
    "        \"\"\"\n",
    "        输入:\n",
    "            center_word_ids: (batch_size,) 里面是中心词的索引\n",
    "            outside_word_ids: (batch_size,) 里面是真实上下文词(正样本)的索引\n",
    "        返回:\n",
    "            loss: 这批样本的平均损失 (标量)\n",
    "        \"\"\"\n",
    "        # 1) 查表得到中心词向量: 形状 (batch_size, embed_dim)\n",
    "        center_embed = self.in_embed(center_word_ids)  # (B, d)\n",
    "        \n",
    "        # 2) 计算与 out_embed.weight 的内积，得到 logits: 形状 (B, V)\n",
    "        #    center_embed: (B, d)\n",
    "        #    out_embed.weight: (V, d)\n",
    "        #    => logits = center_embed @ out_embed.weight.T => (B, V)\n",
    "        logits = torch.matmul(center_embed, self.out_embed.weight.t())\n",
    "        \n",
    "        # 3) 用 CrossEntropyLoss 计算损失\n",
    "        #    CrossEntropyLoss = log_softmax + NLL\n",
    "        #    这里 outside_word_ids.shape=(B,), logits.shape=(B, V)\n",
    "        loss_fn = nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "        loss = loss_fn(logits, outside_word_ids)\n",
    "        \n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 训练优化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500, Avg Loss = 3.0109\n",
      "Epoch 2/500, Avg Loss = 2.9995\n",
      "Epoch 3/500, Avg Loss = 2.9884\n",
      "Epoch 4/500, Avg Loss = 2.9778\n",
      "Epoch 5/500, Avg Loss = 2.9674\n",
      "Epoch 6/500, Avg Loss = 2.9573\n",
      "Epoch 7/500, Avg Loss = 2.9474\n",
      "Epoch 8/500, Avg Loss = 2.9377\n",
      "Epoch 9/500, Avg Loss = 2.9281\n",
      "Epoch 10/500, Avg Loss = 2.9187\n",
      "Epoch 11/500, Avg Loss = 2.9094\n",
      "Epoch 12/500, Avg Loss = 2.9002\n",
      "Epoch 13/500, Avg Loss = 2.8910\n",
      "Epoch 14/500, Avg Loss = 2.8818\n",
      "Epoch 15/500, Avg Loss = 2.8726\n",
      "Epoch 16/500, Avg Loss = 2.8635\n",
      "Epoch 17/500, Avg Loss = 2.8543\n",
      "Epoch 18/500, Avg Loss = 2.8450\n",
      "Epoch 19/500, Avg Loss = 2.8357\n",
      "Epoch 20/500, Avg Loss = 2.8263\n",
      "Epoch 21/500, Avg Loss = 2.8168\n",
      "Epoch 22/500, Avg Loss = 2.8071\n",
      "Epoch 23/500, Avg Loss = 2.7974\n",
      "Epoch 24/500, Avg Loss = 2.7876\n",
      "Epoch 25/500, Avg Loss = 2.7776\n",
      "Epoch 26/500, Avg Loss = 2.7674\n",
      "Epoch 27/500, Avg Loss = 2.7571\n",
      "Epoch 28/500, Avg Loss = 2.7466\n",
      "Epoch 29/500, Avg Loss = 2.7360\n",
      "Epoch 30/500, Avg Loss = 2.7251\n",
      "Epoch 31/500, Avg Loss = 2.7140\n",
      "Epoch 32/500, Avg Loss = 2.7028\n",
      "Epoch 33/500, Avg Loss = 2.6913\n",
      "Epoch 34/500, Avg Loss = 2.6797\n",
      "Epoch 35/500, Avg Loss = 2.6678\n",
      "Epoch 36/500, Avg Loss = 2.6557\n",
      "Epoch 37/500, Avg Loss = 2.6434\n",
      "Epoch 38/500, Avg Loss = 2.6308\n",
      "Epoch 39/500, Avg Loss = 2.6180\n",
      "Epoch 40/500, Avg Loss = 2.6050\n",
      "Epoch 41/500, Avg Loss = 2.5918\n",
      "Epoch 42/500, Avg Loss = 2.5783\n",
      "Epoch 43/500, Avg Loss = 2.5646\n",
      "Epoch 44/500, Avg Loss = 2.5507\n",
      "Epoch 45/500, Avg Loss = 2.5366\n",
      "Epoch 46/500, Avg Loss = 2.5223\n",
      "Epoch 47/500, Avg Loss = 2.5078\n",
      "Epoch 48/500, Avg Loss = 2.4930\n",
      "Epoch 49/500, Avg Loss = 2.4781\n",
      "Epoch 50/500, Avg Loss = 2.4631\n",
      "Epoch 51/500, Avg Loss = 2.4479\n",
      "Epoch 52/500, Avg Loss = 2.4325\n",
      "Epoch 53/500, Avg Loss = 2.4170\n",
      "Epoch 54/500, Avg Loss = 2.4015\n",
      "Epoch 55/500, Avg Loss = 2.3858\n",
      "Epoch 56/500, Avg Loss = 2.3702\n",
      "Epoch 57/500, Avg Loss = 2.3545\n",
      "Epoch 58/500, Avg Loss = 2.3389\n",
      "Epoch 59/500, Avg Loss = 2.3232\n",
      "Epoch 60/500, Avg Loss = 2.3077\n",
      "Epoch 61/500, Avg Loss = 2.2923\n",
      "Epoch 62/500, Avg Loss = 2.2771\n",
      "Epoch 63/500, Avg Loss = 2.2621\n",
      "Epoch 64/500, Avg Loss = 2.2471\n",
      "Epoch 65/500, Avg Loss = 2.2326\n",
      "Epoch 66/500, Avg Loss = 2.2183\n",
      "Epoch 67/500, Avg Loss = 2.2043\n",
      "Epoch 68/500, Avg Loss = 2.1905\n",
      "Epoch 69/500, Avg Loss = 2.1770\n",
      "Epoch 70/500, Avg Loss = 2.1638\n",
      "Epoch 71/500, Avg Loss = 2.1509\n",
      "Epoch 72/500, Avg Loss = 2.1382\n",
      "Epoch 73/500, Avg Loss = 2.1259\n",
      "Epoch 74/500, Avg Loss = 2.1138\n",
      "Epoch 75/500, Avg Loss = 2.1020\n",
      "Epoch 76/500, Avg Loss = 2.0903\n",
      "Epoch 77/500, Avg Loss = 2.0791\n",
      "Epoch 78/500, Avg Loss = 2.0680\n",
      "Epoch 79/500, Avg Loss = 2.0571\n",
      "Epoch 80/500, Avg Loss = 2.0465\n",
      "Epoch 81/500, Avg Loss = 2.0360\n",
      "Epoch 82/500, Avg Loss = 2.0259\n",
      "Epoch 83/500, Avg Loss = 2.0160\n",
      "Epoch 84/500, Avg Loss = 2.0062\n",
      "Epoch 85/500, Avg Loss = 1.9968\n",
      "Epoch 86/500, Avg Loss = 1.9873\n",
      "Epoch 87/500, Avg Loss = 1.9783\n",
      "Epoch 88/500, Avg Loss = 1.9695\n",
      "Epoch 89/500, Avg Loss = 1.9606\n",
      "Epoch 90/500, Avg Loss = 1.9520\n",
      "Epoch 91/500, Avg Loss = 1.9437\n",
      "Epoch 92/500, Avg Loss = 1.9356\n",
      "Epoch 93/500, Avg Loss = 1.9276\n",
      "Epoch 94/500, Avg Loss = 1.9197\n",
      "Epoch 95/500, Avg Loss = 1.9123\n",
      "Epoch 96/500, Avg Loss = 1.9046\n",
      "Epoch 97/500, Avg Loss = 1.8973\n",
      "Epoch 98/500, Avg Loss = 1.8902\n",
      "Epoch 99/500, Avg Loss = 1.8831\n",
      "Epoch 100/500, Avg Loss = 1.8764\n",
      "Epoch 101/500, Avg Loss = 1.8698\n",
      "Epoch 102/500, Avg Loss = 1.8631\n",
      "Epoch 103/500, Avg Loss = 1.8567\n",
      "Epoch 104/500, Avg Loss = 1.8505\n",
      "Epoch 105/500, Avg Loss = 1.8445\n",
      "Epoch 106/500, Avg Loss = 1.8385\n",
      "Epoch 107/500, Avg Loss = 1.8326\n",
      "Epoch 108/500, Avg Loss = 1.8267\n",
      "Epoch 109/500, Avg Loss = 1.8212\n",
      "Epoch 110/500, Avg Loss = 1.8155\n",
      "Epoch 111/500, Avg Loss = 1.8102\n",
      "Epoch 112/500, Avg Loss = 1.8049\n",
      "Epoch 113/500, Avg Loss = 1.7997\n",
      "Epoch 114/500, Avg Loss = 1.7946\n",
      "Epoch 115/500, Avg Loss = 1.7898\n",
      "Epoch 116/500, Avg Loss = 1.7850\n",
      "Epoch 117/500, Avg Loss = 1.7800\n",
      "Epoch 118/500, Avg Loss = 1.7757\n",
      "Epoch 119/500, Avg Loss = 1.7711\n",
      "Epoch 120/500, Avg Loss = 1.7669\n",
      "Epoch 121/500, Avg Loss = 1.7623\n",
      "Epoch 122/500, Avg Loss = 1.7580\n",
      "Epoch 123/500, Avg Loss = 1.7541\n",
      "Epoch 124/500, Avg Loss = 1.7501\n",
      "Epoch 125/500, Avg Loss = 1.7461\n",
      "Epoch 126/500, Avg Loss = 1.7420\n",
      "Epoch 127/500, Avg Loss = 1.7383\n",
      "Epoch 128/500, Avg Loss = 1.7347\n",
      "Epoch 129/500, Avg Loss = 1.7312\n",
      "Epoch 130/500, Avg Loss = 1.7275\n",
      "Epoch 131/500, Avg Loss = 1.7242\n",
      "Epoch 132/500, Avg Loss = 1.7206\n",
      "Epoch 133/500, Avg Loss = 1.7176\n",
      "Epoch 134/500, Avg Loss = 1.7143\n",
      "Epoch 135/500, Avg Loss = 1.7110\n",
      "Epoch 136/500, Avg Loss = 1.7081\n",
      "Epoch 137/500, Avg Loss = 1.7052\n",
      "Epoch 138/500, Avg Loss = 1.7022\n",
      "Epoch 139/500, Avg Loss = 1.6994\n",
      "Epoch 140/500, Avg Loss = 1.6965\n",
      "Epoch 141/500, Avg Loss = 1.6939\n",
      "Epoch 142/500, Avg Loss = 1.6912\n",
      "Epoch 143/500, Avg Loss = 1.6887\n",
      "Epoch 144/500, Avg Loss = 1.6861\n",
      "Epoch 145/500, Avg Loss = 1.6835\n",
      "Epoch 146/500, Avg Loss = 1.6811\n",
      "Epoch 147/500, Avg Loss = 1.6786\n",
      "Epoch 148/500, Avg Loss = 1.6765\n",
      "Epoch 149/500, Avg Loss = 1.6743\n",
      "Epoch 150/500, Avg Loss = 1.6721\n",
      "Epoch 151/500, Avg Loss = 1.6699\n",
      "Epoch 152/500, Avg Loss = 1.6678\n",
      "Epoch 153/500, Avg Loss = 1.6659\n",
      "Epoch 154/500, Avg Loss = 1.6640\n",
      "Epoch 155/500, Avg Loss = 1.6617\n",
      "Epoch 156/500, Avg Loss = 1.6601\n",
      "Epoch 157/500, Avg Loss = 1.6581\n",
      "Epoch 158/500, Avg Loss = 1.6565\n",
      "Epoch 159/500, Avg Loss = 1.6548\n",
      "Epoch 160/500, Avg Loss = 1.6533\n",
      "Epoch 161/500, Avg Loss = 1.6513\n",
      "Epoch 162/500, Avg Loss = 1.6495\n",
      "Epoch 163/500, Avg Loss = 1.6479\n",
      "Epoch 164/500, Avg Loss = 1.6466\n",
      "Epoch 165/500, Avg Loss = 1.6450\n",
      "Epoch 166/500, Avg Loss = 1.6434\n",
      "Epoch 167/500, Avg Loss = 1.6421\n",
      "Epoch 168/500, Avg Loss = 1.6404\n",
      "Epoch 169/500, Avg Loss = 1.6393\n",
      "Epoch 170/500, Avg Loss = 1.6380\n",
      "Epoch 171/500, Avg Loss = 1.6371\n",
      "Epoch 172/500, Avg Loss = 1.6352\n",
      "Epoch 173/500, Avg Loss = 1.6342\n",
      "Epoch 174/500, Avg Loss = 1.6328\n",
      "Epoch 175/500, Avg Loss = 1.6317\n",
      "Epoch 176/500, Avg Loss = 1.6307\n",
      "Epoch 177/500, Avg Loss = 1.6293\n",
      "Epoch 178/500, Avg Loss = 1.6284\n",
      "Epoch 179/500, Avg Loss = 1.6276\n",
      "Epoch 180/500, Avg Loss = 1.6265\n",
      "Epoch 181/500, Avg Loss = 1.6252\n",
      "Epoch 182/500, Avg Loss = 1.6246\n",
      "Epoch 183/500, Avg Loss = 1.6232\n",
      "Epoch 184/500, Avg Loss = 1.6224\n",
      "Epoch 185/500, Avg Loss = 1.6210\n",
      "Epoch 186/500, Avg Loss = 1.6206\n",
      "Epoch 187/500, Avg Loss = 1.6195\n",
      "Epoch 188/500, Avg Loss = 1.6186\n",
      "Epoch 189/500, Avg Loss = 1.6180\n",
      "Epoch 190/500, Avg Loss = 1.6170\n",
      "Epoch 191/500, Avg Loss = 1.6159\n",
      "Epoch 192/500, Avg Loss = 1.6152\n",
      "Epoch 193/500, Avg Loss = 1.6146\n",
      "Epoch 194/500, Avg Loss = 1.6137\n",
      "Epoch 195/500, Avg Loss = 1.6125\n",
      "Epoch 196/500, Avg Loss = 1.6123\n",
      "Epoch 197/500, Avg Loss = 1.6117\n",
      "Epoch 198/500, Avg Loss = 1.6108\n",
      "Epoch 199/500, Avg Loss = 1.6098\n",
      "Epoch 200/500, Avg Loss = 1.6096\n",
      "Epoch 201/500, Avg Loss = 1.6088\n",
      "Epoch 202/500, Avg Loss = 1.6080\n",
      "Epoch 203/500, Avg Loss = 1.6071\n",
      "Epoch 204/500, Avg Loss = 1.6067\n",
      "Epoch 205/500, Avg Loss = 1.6060\n",
      "Epoch 206/500, Avg Loss = 1.6054\n",
      "Epoch 207/500, Avg Loss = 1.6053\n",
      "Epoch 208/500, Avg Loss = 1.6041\n",
      "Epoch 209/500, Avg Loss = 1.6040\n",
      "Epoch 210/500, Avg Loss = 1.6036\n",
      "Epoch 211/500, Avg Loss = 1.6027\n",
      "Epoch 212/500, Avg Loss = 1.6023\n",
      "Epoch 213/500, Avg Loss = 1.6019\n",
      "Epoch 214/500, Avg Loss = 1.6011\n",
      "Epoch 215/500, Avg Loss = 1.6010\n",
      "Epoch 216/500, Avg Loss = 1.5998\n",
      "Epoch 217/500, Avg Loss = 1.5992\n",
      "Epoch 218/500, Avg Loss = 1.5990\n",
      "Epoch 219/500, Avg Loss = 1.5988\n",
      "Epoch 220/500, Avg Loss = 1.5984\n",
      "Epoch 221/500, Avg Loss = 1.5978\n",
      "Epoch 222/500, Avg Loss = 1.5970\n",
      "Epoch 223/500, Avg Loss = 1.5970\n",
      "Epoch 224/500, Avg Loss = 1.5966\n",
      "Epoch 225/500, Avg Loss = 1.5960\n",
      "Epoch 226/500, Avg Loss = 1.5957\n",
      "Epoch 227/500, Avg Loss = 1.5950\n",
      "Epoch 228/500, Avg Loss = 1.5948\n",
      "Epoch 229/500, Avg Loss = 1.5946\n",
      "Epoch 230/500, Avg Loss = 1.5937\n",
      "Epoch 231/500, Avg Loss = 1.5933\n",
      "Epoch 232/500, Avg Loss = 1.5934\n",
      "Epoch 233/500, Avg Loss = 1.5927\n",
      "Epoch 234/500, Avg Loss = 1.5928\n",
      "Epoch 235/500, Avg Loss = 1.5923\n",
      "Epoch 236/500, Avg Loss = 1.5914\n",
      "Epoch 237/500, Avg Loss = 1.5914\n",
      "Epoch 238/500, Avg Loss = 1.5914\n",
      "Epoch 239/500, Avg Loss = 1.5912\n",
      "Epoch 240/500, Avg Loss = 1.5905\n",
      "Epoch 241/500, Avg Loss = 1.5904\n",
      "Epoch 242/500, Avg Loss = 1.5903\n",
      "Epoch 243/500, Avg Loss = 1.5895\n",
      "Epoch 244/500, Avg Loss = 1.5892\n",
      "Epoch 245/500, Avg Loss = 1.5893\n",
      "Epoch 246/500, Avg Loss = 1.5890\n",
      "Epoch 247/500, Avg Loss = 1.5887\n",
      "Epoch 248/500, Avg Loss = 1.5882\n",
      "Epoch 249/500, Avg Loss = 1.5878\n",
      "Epoch 250/500, Avg Loss = 1.5876\n",
      "Epoch 251/500, Avg Loss = 1.5870\n",
      "Epoch 252/500, Avg Loss = 1.5870\n",
      "Epoch 253/500, Avg Loss = 1.5871\n",
      "Epoch 254/500, Avg Loss = 1.5868\n",
      "Epoch 255/500, Avg Loss = 1.5868\n",
      "Epoch 256/500, Avg Loss = 1.5857\n",
      "Epoch 257/500, Avg Loss = 1.5859\n",
      "Epoch 258/500, Avg Loss = 1.5854\n",
      "Epoch 259/500, Avg Loss = 1.5853\n",
      "Epoch 260/500, Avg Loss = 1.5852\n",
      "Epoch 261/500, Avg Loss = 1.5848\n",
      "Epoch 262/500, Avg Loss = 1.5849\n",
      "Epoch 263/500, Avg Loss = 1.5840\n",
      "Epoch 264/500, Avg Loss = 1.5846\n",
      "Epoch 265/500, Avg Loss = 1.5833\n",
      "Epoch 266/500, Avg Loss = 1.5837\n",
      "Epoch 267/500, Avg Loss = 1.5831\n",
      "Epoch 268/500, Avg Loss = 1.5830\n",
      "Epoch 269/500, Avg Loss = 1.5826\n",
      "Epoch 270/500, Avg Loss = 1.5827\n",
      "Epoch 271/500, Avg Loss = 1.5828\n",
      "Epoch 272/500, Avg Loss = 1.5829\n",
      "Epoch 273/500, Avg Loss = 1.5824\n",
      "Epoch 274/500, Avg Loss = 1.5824\n",
      "Epoch 275/500, Avg Loss = 1.5821\n",
      "Epoch 276/500, Avg Loss = 1.5817\n",
      "Epoch 277/500, Avg Loss = 1.5809\n",
      "Epoch 278/500, Avg Loss = 1.5811\n",
      "Epoch 279/500, Avg Loss = 1.5813\n",
      "Epoch 280/500, Avg Loss = 1.5815\n",
      "Epoch 281/500, Avg Loss = 1.5804\n",
      "Epoch 282/500, Avg Loss = 1.5809\n",
      "Epoch 283/500, Avg Loss = 1.5807\n",
      "Epoch 284/500, Avg Loss = 1.5804\n",
      "Epoch 285/500, Avg Loss = 1.5803\n",
      "Epoch 286/500, Avg Loss = 1.5799\n",
      "Epoch 287/500, Avg Loss = 1.5802\n",
      "Epoch 288/500, Avg Loss = 1.5794\n",
      "Epoch 289/500, Avg Loss = 1.5795\n",
      "Epoch 290/500, Avg Loss = 1.5792\n",
      "Epoch 291/500, Avg Loss = 1.5795\n",
      "Epoch 292/500, Avg Loss = 1.5797\n",
      "Epoch 293/500, Avg Loss = 1.5794\n",
      "Epoch 294/500, Avg Loss = 1.5793\n",
      "Epoch 295/500, Avg Loss = 1.5783\n",
      "Epoch 296/500, Avg Loss = 1.5788\n",
      "Epoch 297/500, Avg Loss = 1.5777\n",
      "Epoch 298/500, Avg Loss = 1.5781\n",
      "Epoch 299/500, Avg Loss = 1.5777\n",
      "Epoch 300/500, Avg Loss = 1.5779\n",
      "Epoch 301/500, Avg Loss = 1.5779\n",
      "Epoch 302/500, Avg Loss = 1.5778\n",
      "Epoch 303/500, Avg Loss = 1.5771\n",
      "Epoch 304/500, Avg Loss = 1.5769\n",
      "Epoch 305/500, Avg Loss = 1.5774\n",
      "Epoch 306/500, Avg Loss = 1.5769\n",
      "Epoch 307/500, Avg Loss = 1.5770\n",
      "Epoch 308/500, Avg Loss = 1.5772\n",
      "Epoch 309/500, Avg Loss = 1.5762\n",
      "Epoch 310/500, Avg Loss = 1.5760\n",
      "Epoch 311/500, Avg Loss = 1.5770\n",
      "Epoch 312/500, Avg Loss = 1.5765\n",
      "Epoch 313/500, Avg Loss = 1.5762\n",
      "Epoch 314/500, Avg Loss = 1.5761\n",
      "Epoch 315/500, Avg Loss = 1.5762\n",
      "Epoch 316/500, Avg Loss = 1.5763\n",
      "Epoch 317/500, Avg Loss = 1.5761\n",
      "Epoch 318/500, Avg Loss = 1.5757\n",
      "Epoch 319/500, Avg Loss = 1.5755\n",
      "Epoch 320/500, Avg Loss = 1.5759\n",
      "Epoch 321/500, Avg Loss = 1.5754\n",
      "Epoch 322/500, Avg Loss = 1.5755\n",
      "Epoch 323/500, Avg Loss = 1.5755\n",
      "Epoch 324/500, Avg Loss = 1.5752\n",
      "Epoch 325/500, Avg Loss = 1.5751\n",
      "Epoch 326/500, Avg Loss = 1.5752\n",
      "Epoch 327/500, Avg Loss = 1.5750\n",
      "Epoch 328/500, Avg Loss = 1.5750\n",
      "Epoch 329/500, Avg Loss = 1.5748\n",
      "Epoch 330/500, Avg Loss = 1.5745\n",
      "Epoch 331/500, Avg Loss = 1.5748\n",
      "Epoch 332/500, Avg Loss = 1.5742\n",
      "Epoch 333/500, Avg Loss = 1.5748\n",
      "Epoch 334/500, Avg Loss = 1.5740\n",
      "Epoch 335/500, Avg Loss = 1.5739\n",
      "Epoch 336/500, Avg Loss = 1.5743\n",
      "Epoch 337/500, Avg Loss = 1.5739\n",
      "Epoch 338/500, Avg Loss = 1.5738\n",
      "Epoch 339/500, Avg Loss = 1.5745\n",
      "Epoch 340/500, Avg Loss = 1.5742\n",
      "Epoch 341/500, Avg Loss = 1.5736\n",
      "Epoch 342/500, Avg Loss = 1.5739\n",
      "Epoch 343/500, Avg Loss = 1.5736\n",
      "Epoch 344/500, Avg Loss = 1.5736\n",
      "Epoch 345/500, Avg Loss = 1.5733\n",
      "Epoch 346/500, Avg Loss = 1.5735\n",
      "Epoch 347/500, Avg Loss = 1.5730\n",
      "Epoch 348/500, Avg Loss = 1.5734\n",
      "Epoch 349/500, Avg Loss = 1.5728\n",
      "Epoch 350/500, Avg Loss = 1.5727\n",
      "Epoch 351/500, Avg Loss = 1.5732\n",
      "Epoch 352/500, Avg Loss = 1.5733\n",
      "Epoch 353/500, Avg Loss = 1.5729\n",
      "Epoch 354/500, Avg Loss = 1.5725\n",
      "Epoch 355/500, Avg Loss = 1.5732\n",
      "Epoch 356/500, Avg Loss = 1.5727\n",
      "Epoch 357/500, Avg Loss = 1.5724\n",
      "Epoch 358/500, Avg Loss = 1.5727\n",
      "Epoch 359/500, Avg Loss = 1.5725\n",
      "Epoch 360/500, Avg Loss = 1.5724\n",
      "Epoch 361/500, Avg Loss = 1.5714\n",
      "Epoch 362/500, Avg Loss = 1.5729\n",
      "Epoch 363/500, Avg Loss = 1.5715\n",
      "Epoch 364/500, Avg Loss = 1.5717\n",
      "Epoch 365/500, Avg Loss = 1.5716\n",
      "Epoch 366/500, Avg Loss = 1.5721\n",
      "Epoch 367/500, Avg Loss = 1.5720\n",
      "Epoch 368/500, Avg Loss = 1.5719\n",
      "Epoch 369/500, Avg Loss = 1.5720\n",
      "Epoch 370/500, Avg Loss = 1.5717\n",
      "Epoch 371/500, Avg Loss = 1.5713\n",
      "Epoch 372/500, Avg Loss = 1.5721\n",
      "Epoch 373/500, Avg Loss = 1.5712\n",
      "Epoch 374/500, Avg Loss = 1.5713\n",
      "Epoch 375/500, Avg Loss = 1.5711\n",
      "Epoch 376/500, Avg Loss = 1.5717\n",
      "Epoch 377/500, Avg Loss = 1.5710\n",
      "Epoch 378/500, Avg Loss = 1.5711\n",
      "Epoch 379/500, Avg Loss = 1.5719\n",
      "Epoch 380/500, Avg Loss = 1.5712\n",
      "Epoch 381/500, Avg Loss = 1.5709\n",
      "Epoch 382/500, Avg Loss = 1.5713\n",
      "Epoch 383/500, Avg Loss = 1.5704\n",
      "Epoch 384/500, Avg Loss = 1.5713\n",
      "Epoch 385/500, Avg Loss = 1.5711\n",
      "Epoch 386/500, Avg Loss = 1.5711\n",
      "Epoch 387/500, Avg Loss = 1.5714\n",
      "Epoch 388/500, Avg Loss = 1.5708\n",
      "Epoch 389/500, Avg Loss = 1.5706\n",
      "Epoch 390/500, Avg Loss = 1.5710\n",
      "Epoch 391/500, Avg Loss = 1.5710\n",
      "Epoch 392/500, Avg Loss = 1.5707\n",
      "Epoch 393/500, Avg Loss = 1.5706\n",
      "Epoch 394/500, Avg Loss = 1.5703\n",
      "Epoch 395/500, Avg Loss = 1.5702\n",
      "Epoch 396/500, Avg Loss = 1.5702\n",
      "Epoch 397/500, Avg Loss = 1.5703\n",
      "Epoch 398/500, Avg Loss = 1.5703\n",
      "Epoch 399/500, Avg Loss = 1.5704\n",
      "Epoch 400/500, Avg Loss = 1.5696\n",
      "Epoch 401/500, Avg Loss = 1.5706\n",
      "Epoch 402/500, Avg Loss = 1.5703\n",
      "Epoch 403/500, Avg Loss = 1.5703\n",
      "Epoch 404/500, Avg Loss = 1.5700\n",
      "Epoch 405/500, Avg Loss = 1.5702\n",
      "Epoch 406/500, Avg Loss = 1.5700\n",
      "Epoch 407/500, Avg Loss = 1.5700\n",
      "Epoch 408/500, Avg Loss = 1.5700\n",
      "Epoch 409/500, Avg Loss = 1.5696\n",
      "Epoch 410/500, Avg Loss = 1.5696\n",
      "Epoch 411/500, Avg Loss = 1.5704\n",
      "Epoch 412/500, Avg Loss = 1.5702\n",
      "Epoch 413/500, Avg Loss = 1.5692\n",
      "Epoch 414/500, Avg Loss = 1.5690\n",
      "Epoch 415/500, Avg Loss = 1.5697\n",
      "Epoch 416/500, Avg Loss = 1.5702\n",
      "Epoch 417/500, Avg Loss = 1.5699\n",
      "Epoch 418/500, Avg Loss = 1.5687\n",
      "Epoch 419/500, Avg Loss = 1.5692\n",
      "Epoch 420/500, Avg Loss = 1.5696\n",
      "Epoch 421/500, Avg Loss = 1.5698\n",
      "Epoch 422/500, Avg Loss = 1.5693\n",
      "Epoch 423/500, Avg Loss = 1.5695\n",
      "Epoch 424/500, Avg Loss = 1.5687\n",
      "Epoch 425/500, Avg Loss = 1.5694\n",
      "Epoch 426/500, Avg Loss = 1.5699\n",
      "Epoch 427/500, Avg Loss = 1.5689\n",
      "Epoch 428/500, Avg Loss = 1.5693\n",
      "Epoch 429/500, Avg Loss = 1.5690\n",
      "Epoch 430/500, Avg Loss = 1.5687\n",
      "Epoch 431/500, Avg Loss = 1.5687\n",
      "Epoch 432/500, Avg Loss = 1.5690\n",
      "Epoch 433/500, Avg Loss = 1.5682\n",
      "Epoch 434/500, Avg Loss = 1.5691\n",
      "Epoch 435/500, Avg Loss = 1.5687\n",
      "Epoch 436/500, Avg Loss = 1.5692\n",
      "Epoch 437/500, Avg Loss = 1.5693\n",
      "Epoch 438/500, Avg Loss = 1.5691\n",
      "Epoch 439/500, Avg Loss = 1.5694\n",
      "Epoch 440/500, Avg Loss = 1.5681\n",
      "Epoch 441/500, Avg Loss = 1.5691\n",
      "Epoch 442/500, Avg Loss = 1.5685\n",
      "Epoch 443/500, Avg Loss = 1.5681\n",
      "Epoch 444/500, Avg Loss = 1.5683\n",
      "Epoch 445/500, Avg Loss = 1.5686\n",
      "Epoch 446/500, Avg Loss = 1.5679\n",
      "Epoch 447/500, Avg Loss = 1.5692\n",
      "Epoch 448/500, Avg Loss = 1.5693\n",
      "Epoch 449/500, Avg Loss = 1.5681\n",
      "Epoch 450/500, Avg Loss = 1.5686\n",
      "Epoch 451/500, Avg Loss = 1.5684\n",
      "Epoch 452/500, Avg Loss = 1.5685\n",
      "Epoch 453/500, Avg Loss = 1.5687\n",
      "Epoch 454/500, Avg Loss = 1.5689\n",
      "Epoch 455/500, Avg Loss = 1.5683\n",
      "Epoch 456/500, Avg Loss = 1.5683\n",
      "Epoch 457/500, Avg Loss = 1.5681\n",
      "Epoch 458/500, Avg Loss = 1.5685\n",
      "Epoch 459/500, Avg Loss = 1.5684\n",
      "Epoch 460/500, Avg Loss = 1.5683\n",
      "Epoch 461/500, Avg Loss = 1.5683\n",
      "Epoch 462/500, Avg Loss = 1.5679\n",
      "Epoch 463/500, Avg Loss = 1.5687\n",
      "Epoch 464/500, Avg Loss = 1.5681\n",
      "Epoch 465/500, Avg Loss = 1.5682\n",
      "Epoch 466/500, Avg Loss = 1.5680\n",
      "Epoch 467/500, Avg Loss = 1.5680\n",
      "Epoch 468/500, Avg Loss = 1.5679\n",
      "Epoch 469/500, Avg Loss = 1.5681\n",
      "Epoch 470/500, Avg Loss = 1.5679\n",
      "Epoch 471/500, Avg Loss = 1.5682\n",
      "Epoch 472/500, Avg Loss = 1.5677\n",
      "Epoch 473/500, Avg Loss = 1.5681\n",
      "Epoch 474/500, Avg Loss = 1.5676\n",
      "Epoch 475/500, Avg Loss = 1.5683\n",
      "Epoch 476/500, Avg Loss = 1.5677\n",
      "Epoch 477/500, Avg Loss = 1.5684\n",
      "Epoch 478/500, Avg Loss = 1.5670\n",
      "Epoch 479/500, Avg Loss = 1.5680\n",
      "Epoch 480/500, Avg Loss = 1.5679\n",
      "Epoch 481/500, Avg Loss = 1.5680\n",
      "Epoch 482/500, Avg Loss = 1.5674\n",
      "Epoch 483/500, Avg Loss = 1.5684\n",
      "Epoch 484/500, Avg Loss = 1.5678\n",
      "Epoch 485/500, Avg Loss = 1.5678\n",
      "Epoch 486/500, Avg Loss = 1.5680\n",
      "Epoch 487/500, Avg Loss = 1.5677\n",
      "Epoch 488/500, Avg Loss = 1.5678\n",
      "Epoch 489/500, Avg Loss = 1.5682\n",
      "Epoch 490/500, Avg Loss = 1.5672\n",
      "Epoch 491/500, Avg Loss = 1.5679\n",
      "Epoch 492/500, Avg Loss = 1.5678\n",
      "Epoch 493/500, Avg Loss = 1.5678\n",
      "Epoch 494/500, Avg Loss = 1.5678\n",
      "Epoch 495/500, Avg Loss = 1.5681\n",
      "Epoch 496/500, Avg Loss = 1.5677\n",
      "Epoch 497/500, Avg Loss = 1.5669\n",
      "Epoch 498/500, Avg Loss = 1.5674\n",
      "Epoch 499/500, Avg Loss = 1.5678\n",
      "Epoch 500/500, Avg Loss = 1.5679\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 8  # 词向量维度\n",
    "model = SkipGramFullSoftmax(vocab_size, embed_dim)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "num_epochs = 500\n",
    "pairs_list = skipgram_pairs[:]  # 拷贝一份\n",
    "for epoch in range(num_epochs):\n",
    "    random.shuffle(pairs_list)  # 每个 epoch 打乱\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    for (center_id, outside_id) in pairs_list:\n",
    "        center_tensor = torch.LongTensor([center_id])\n",
    "        outside_tensor = torch.LongTensor([outside_id])\n",
    "        \n",
    "        loss = model(center_tensor, outside_tensor)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    avg_loss = total_loss / len(pairs_list)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Avg Loss = {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Top similar words to 'i']\n",
      "   watch    cos_sim = 0.7528\n",
      "   eat      cos_sim = 0.6275\n",
      "   john     cos_sim = 0.5144\n",
      "\n",
      "[Top similar words to 'eat']\n",
      "   bananas  cos_sim = 0.6609\n",
      "   i        cos_sim = 0.6275\n",
      "   cat      cos_sim = 0.4998\n",
      "\n",
      "[Top similar words to 'movies']\n",
      "   like     cos_sim = 0.4661\n",
      "   bananas  cos_sim = 0.3990\n",
      "   apples   cos_sim = 0.3901\n",
      "\n",
      "[Top similar words to 'python']\n",
      "   read     cos_sim = 0.6595\n",
      "   about    cos_sim = 0.5029\n",
      "   books    cos_sim = 0.4482\n",
      "\n",
      "[Top similar words to 'cat']\n",
      "   eat      cos_sim = 0.4998\n",
      "   likes    cos_sim = 0.4874\n",
      "   the      cos_sim = 0.2974\n"
     ]
    }
   ],
   "source": [
    "def get_embedding(model, word):\n",
    "    \"\"\"获取某个word在 in_embed 中的向量。\"\"\"\n",
    "    idx = word2idx[word]\n",
    "    emb = model.in_embed.weight[idx].detach().numpy()\n",
    "    return emb\n",
    "\n",
    "def cosine_sim(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b) + 1e-9)\n",
    "\n",
    "def most_similar_words(model, query_word, top_k=3):\n",
    "    \"\"\"找与 query_word 最相似的 top_k 词。\"\"\"\n",
    "    if query_word not in word2idx:\n",
    "        return []\n",
    "    query_emb = get_embedding(model, query_word)\n",
    "    sims = []\n",
    "    for w in vocab:\n",
    "        if w == query_word:\n",
    "            continue\n",
    "        emb = get_embedding(model, w)\n",
    "        sim_score = cosine_sim(query_emb, emb)\n",
    "        sims.append((w, sim_score))\n",
    "    sims.sort(key=lambda x: x[1], reverse=True)\n",
    "    return sims[:top_k]\n",
    "\n",
    "test_words = [\"i\", \"eat\", \"movies\", \"python\", \"cat\"]\n",
    "for w in test_words:\n",
    "    if w not in word2idx:\n",
    "        print(f\"Word '{w}' not in vocab, skip.\")\n",
    "        continue\n",
    "    print(f\"\\n[Top similar words to '{w}']\")\n",
    "    for candidate, score in most_similar_words(model, w):\n",
    "        print(f\"   {candidate:<8} cos_sim = {score:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
