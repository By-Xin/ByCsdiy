{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a1368f0",
   "metadata": {},
   "source": [
    "# 作业5：RNN 生成模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997082ed",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "以 `data/names.txt` 中的英文名作为训练集，利用 RNN 或 LSTM 等方法对字母序列数据进行建模，然后使用拟合的模型随机生成20个名字。本次作业为开放式，不指定各类超参数（如网络结构、学习率、迭代次数等），但需提供必要的输出和诊断结果支持你的选择（如模型是否收敛、效果评价等）。\n",
    "\n",
    "提示：可以参照 `lec12-rnn-generation.zip` 中的代码，但注意英文名不需要像中文那样构建字典，因为可以直接使用26个字母作为字典。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 处理数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-04T11:42:04.645497700Z",
     "start_time": "2023-12-04T11:42:00.191807700Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import itertools\n",
    "import collections\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-04T11:42:08.783637Z",
     "start_time": "2023-12-04T11:42:08.623557100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abbas', 'abbey', 'abbott', 'abdi', 'abel']\n"
     ]
    }
   ],
   "source": [
    "# load txt file\n",
    "\n",
    "def read_txt_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        content = file.read().split()\n",
    "        return content\n",
    "\n",
    "dat = read_txt_file('data/names.txt')\n",
    "print(dat[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-04T11:42:23.907980700Z",
     "start_time": "2023-12-04T11:42:23.892846700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '<EOS>']\n"
     ]
    }
   ],
   "source": [
    "# construct dictionary\n",
    "charset_size = 27 # 26 letters  + 1 <EOS>\n",
    "dictionary = list('abcdefghijklmnopqrstuvwxyz') + ['<EOS>'] \n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-04T11:42:26.701001Z",
     "start_time": "2023-12-04T11:42:26.639429500Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "          [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]]),\n",
       " array([4, 6]),\n",
       " tensor([[ 4,  0],\n",
       "         [14,  2],\n",
       "         [13,  7],\n",
       "         [26,  4],\n",
       "         [26, 11],\n",
       "         [26, 26]]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# names to tensor\n",
    "def char2index(char):\n",
    "    \"\"\"Transform a character to its index in the dictionary\n",
    "    Args:\n",
    "        char (str): a character\n",
    "        \n",
    "    Returns:\n",
    "       int: the index of the character in the dictionary\n",
    "        \n",
    "    \"\"\"\n",
    "    return dictionary.index(char.lower()) \n",
    "\n",
    "def names2tensor(NameList):\n",
    "    \"\"\"Transform a list of names to one-hot tensor\n",
    "    Args:\n",
    "        NameList (array): a list of names\n",
    "        \n",
    "    Returns:\n",
    "        tensor: a tensor of shape (LongestNameLength, NumberOfNames, charset_size=27), storing the one-hot representation of names\n",
    "        array: a numpy array of shape (NumberOfNames), storing each name's length\n",
    "        target: a tensor of shape (LongestNameLength, NumberOfNames), storing the index of the next letter\n",
    "        \n",
    "    \"\"\"\n",
    "    names_num = len(NameList) # number of names\n",
    "    names_lens = [len(name) for name in NameList] # a list storing each name's length\n",
    "    max_name_len = max(names_lens) # the longest name's length\n",
    "    \n",
    "    tensor = torch.zeros(max_name_len, names_num, charset_size) # (each char in a name, each name, one-hot vector)\n",
    "    target = torch.zeros(max_name_len, names_num, dtype=int) + charset_size - 1 # initialize with <EOS>\n",
    "    \n",
    "    for name_i in range(names_num): # for each name(idx) in data set\n",
    "        name = NameList[name_i] # get the name\n",
    "        for char_i in range(names_lens[name_i]): # for each char(idx) in the name\n",
    "            # set tensor\n",
    "            tensor[char_i, name_i, char2index(name[char_i])] = 1 # set the corresponding one-hot vector\n",
    "            # set target\n",
    "            if char_i < names_lens[name_i] - 1: # if not the last char (here note that python index starts from 0)\n",
    "                target[char_i, name_i] = char2index(name[char_i + 1]) # target for name_i, char_i is char_i+1\n",
    "                \n",
    "    return tensor, np.array(names_lens), target\n",
    "\n",
    "# test names2tensor\n",
    "names2tensor([\"leon\",\"rachel\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 构建LSTM模型类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "![](https://michael-1313341240.cos.ap-shanghai.myqcloud.com/202312042025464.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters Definition:\n",
    "\n",
    "- Suppose there are $h$ hidden units, batch size is $n$, number of inputs is $d$. Thus, $X_t \\in \\mathbb{R}^{n\\times h}, H_t = \\mathbb{R}^{n\\times h}.$\n",
    "- Define Gates (at time $t$):\n",
    "    - Input gate ($I_t \\in \\mathbb{R}^{n\\times h}$) : $I_t = \\text{sigmoid}(X_t W_{xi} + H_{t-1} W_{hi} + b_i)$\n",
    "    - Forget gate ($F_t \\in \\mathbb{R}^{n\\times h}$) : $F_t = \\text{sigmoid}({X_t W_{xf} + H_{t-1} W_{hf} + b_f)}$\n",
    "    - Output gate ($O_t \\in  \\mathbb{R}^{n\\times h}$) : $O_t = \\text{sigmoid}{X_t W_{xo} + H_{t-1} W_{ho} + b_o)}$\n",
    "  \n",
    "  where $W_{x,\\cdot} \\in \\mathbb{R}^{d\\times h}$, $W_{h,\\cdot} \\in \\mathbb{R}^{h\\times h}$, $b_{\\cdot} \\in \\mathbb{R}^{1\\times h}$.\n",
    "\n",
    "- Define Candidate Memory Cell $\\tilde C$:\n",
    "\n",
    "$$\n",
    "  \\tilde C_t = \\text{tanh}(X_t W_{xc} + H_{t-1} W_{hc} + b_c)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "> *Reference*\n",
    ">\n",
    "> *1. Dive Into Deep Learning (https://zh.d2l.ai/chapter_recurrent-modern/lstm.html)*\n",
    ">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LSTM\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(LSTM,self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size #?\n",
    "        \n",
    "        self.X2F = nn.Linear(in_features = input_size + hidden_size,\n",
    "                            out_features = hidden_size )\n",
    "        self.X2I = nn.Linear(in_features = input_size + hidden_size,\n",
    "                            out_features = hidden_size )\n",
    "        self.X2O = nn.Linear(in_features = input_size + hidden_size,\n",
    "                            out_features = hidden_size )\n",
    "        \n",
    "        self.X2Ct = nn.Linear(in_features = input_size + hidden_size,\n",
    "                            out_features = hidden_size )\n",
    "        \n",
    "        self.O2O = nn.Linear(in_features = hidden_size,\n",
    "                            out_features = hidden_size)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.logsoftmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, input, hidden, MemCell):\n",
    "        \n",
    "        input_combined = torch.cat((input,hidden),1)     #? how\n",
    "        \n",
    "        ForgetGate = torch.sigmoid(self.X2F(input_combined))   \n",
    "        InputGate = torch.sigmoid(self.X2I(input_combined))\n",
    "        OutputGate = torch.sigmoid(self.X2O(input_combined))\n",
    "        \n",
    "        CandidateMemCell = torch.tanh(self.X2Ct(input_combined))\n",
    "        \n",
    "        MemCell = ForgetGate * MemCell + InputGate * CandidateMemCell\n",
    "        \n",
    "        hidden = OutputGate * torch.tanh(MemCell)\n",
    "        print(f\"hidden:{hidden.shape}\")\n",
    "        \n",
    "        output = self.O2O(hidden)\n",
    "        output = self.dropout(output)\n",
    "        output = self.logsoftmax(output)\n",
    "        \n",
    "        return output, hidden, MemCell\n",
    "    \n",
    "    def init_hidden(self, batch_size, device):\n",
    "        return torch.zeros((batch_size, self.hidden_size), device = device)\n",
    "\n",
    "    def init_MemCell(self, batch_size, device):\n",
    "        return torch.zeros((batch_size, self.hidden_size), device = device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Test Demo*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "n_hidden = 64\n",
    "lstm = LSTM(charset_size, n_hidden)\n",
    "def name2tensor(name):\n",
    "    \"\"\"将名字转换为 one-hot 编码的张量\"\"\"\n",
    "    tensor = torch.zeros(len(name), 1, charset_size) #一个tensor（其实是2d矩阵的感觉），第一个维度是名字的长度（名字中的各个字符），第二个维度是1，第三个维度是每个字符的onehot\n",
    "    for i, char in enumerate(name): #enmuerate\n",
    "        tensor[i, 0, char2index(char)] = 1\n",
    "    return tensor\n",
    "\n",
    "input = name2tensor(\"leon\")\n",
    "#print(input)\n",
    "hidden = lstm.init_hidden(batch_size=1, device='cpu')\n",
    "MemCell = lstm.init_MemCell(batch_size=1, device='cpu')\n",
    "\n",
    "print(input[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden:torch.Size([1, 64])\n",
      "tensor([[-4.2074, -4.2082, -4.0750, -4.1639, -4.0779, -4.0199, -4.2261, -4.1309,\n",
      "         -4.2845, -4.1639, -4.2804, -4.0654, -4.0371, -4.2899, -4.1065, -4.1218,\n",
      "         -4.1017, -4.0127, -4.1988, -4.0327, -4.2638, -4.1167, -4.1900, -4.1590,\n",
      "         -4.2082, -4.1802, -4.2928, -4.1544, -4.0681, -4.1971, -4.2131, -4.1241,\n",
      "         -4.1018, -4.1639, -4.2799, -4.0572, -4.2967, -4.1796, -4.1924, -4.1639,\n",
      "         -4.2341, -4.2030, -4.0965, -4.0390, -4.0517, -4.3073, -4.0404, -4.1639,\n",
      "         -4.1319, -4.2506, -4.2211, -4.3108, -4.1111, -4.0193, -4.2164, -4.1751,\n",
      "         -4.0710, -4.2590, -4.2866, -4.2244, -4.0737, -4.0793, -4.1215, -4.3062]],\n",
      "       grad_fn=<LogSoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "output, next_hidden, next_MemCell  = lstm(input[0], hidden, MemCell)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Train LSTM***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seed \n",
    "np.random.seed(123)\n",
    "torch.random.manual_seed(123)\n",
    "\n",
    "# set device\n",
    "device = torch.device(\"cpu\") \n",
    "\n",
    "# set hyper-params\n",
    "train = dat\n",
    "train_size = len(train)\n",
    "n_hidden = 256\n",
    "n_epoch = 10\n",
    "batch_size = 256\n",
    "learning_rate = 0.001\n",
    "\n",
    "# create LSTM\n",
    "\n",
    "## Initialization\n",
    "lstm = LSTM(input_size = charset_size, hidden_size = n_hidden) # create lstm\n",
    "lstm = lstm.to(device=device) # set gpu/cpu\n",
    "\n",
    "## Optimization\n",
    "opt = torch.optim.Adam(lstm.parameters(), lr=learning_rate)\n",
    "\n",
    "## Loss Function\n",
    "lossfn = nn.NLLLoss(reduction='none') # ? reduction='none'\n",
    "losses = []\n",
    "\n",
    "## Training Structure\n",
    "\n",
    "train_ind = np.arange(train_size) # train_ind for mini-batch shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "t1 = time.time() # timing \n",
    "\n",
    "for epoch in range(n_epoch): # start training\n",
    "    np.random.shuffle(train_ind) # shuffle\n",
    "    \n",
    "    for batch_i in range(0, train_size, batch_size): # batch_i: i'th batch start index\n",
    "        \n",
    "        # create minibatch_i\n",
    "        index_list = train_ind[batch_i:(batch_i+batch_size)] # get i'th batch's index list\n",
    "        minibatch = [train[id]  for id in index_list ] # from index get i'th minibatch list\n",
    "        minibatch_size = len(minibatch) # get i'th minibatch size\n",
    "        \n",
    "        # transform input name: char -> tensor\n",
    "        input, actual_len, target = names2tensor(minibatch) # input: each name's one-hot tensor\n",
    "        input = input.to(device=device) # transfer input to device\n",
    "        target = target.to(device=device) # transfer input to device\n",
    "        max_name_len = input.shape[0] # by definition of names2tensor\n",
    "        \n",
    "        # initialization \n",
    "        hidden = lstm.init_hidden(minibatch_size).to(device=device) # initialize hidden layer\n",
    "        MemCell = lstm.init_MemCell(minibatch_size).to(device=device) # initialize MemCell\n",
    "        \n",
    "        # compute loss function\n",
    "        loss = 0.0 # initialize loss function\n",
    "        \n",
    "        for char_i in range(max_name_len): # put data into LSTM char by char\n",
    "            output, hidden, MemCell = lstm(input[char_i], hidden, MemCell) #input[char_i]\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "n_hidden = 64\n",
    "lstm = LSTM(charset_size, n_hidden)\n",
    "def name2tensor(name):\n",
    "    \"\"\"将名字转换为 one-hot 编码的张量\"\"\"\n",
    "    tensor = torch.zeros(len(name), 1, charset_size) #一个tensor（其实是2d矩阵的感觉），第一个维度是名字的长度（名字中的各个字符），第二个维度是1，第三个维度是每个字符的onehot\n",
    "    for i, char in enumerate(name): #enmuerate\n",
    "        tensor[i, 0, char2index(char)] = 1\n",
    "    return tensor\n",
    "\n",
    "input = name2tensor(\"leon\")\n",
    "#print(input)\n",
    "hidden = lstm.init_hidden(batch_size=1, device='cpu')\n",
    "MemCell = lstm.init_MemCell(batch_size=1, device='cpu')\n",
    "\n",
    "print(input[0])\n",
    "#output, next_hidden, next_MemCell  = lstm(input[0], hidden, MemCell)\n",
    "#print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
