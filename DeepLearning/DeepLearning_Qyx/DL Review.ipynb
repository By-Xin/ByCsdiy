{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DL 复习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 目录"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lect 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lect 2 线性回归，二分类回归，损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 线性回归\n",
    "\n",
    "- 在线性回归中， 如果假定误差是正态，极大似然可以推导出OLS，即MSE准则\n",
    "- 若假定误差是Laplace分布，极大似然可以推导出MAE准则"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 线性二分类\n",
    "\n",
    "$$Y|x \\sim Bernoulli(\\rho(\\beta'x))$$\n",
    "\n",
    "- $\\rho(\\beta'x)$把$\\beta'x$映射到$(0,1)$，表示$Y=1$的概率\n",
    "- 当$\\rho(x)=e^x/(e^x+1)$时，称为logistic 回归\n",
    "\n",
    "***Lositics 回归***\n",
    "- 损失函数\n",
    "  $$\\begin{aligned} \n",
    "  L(\\beta)&=n^{-1}\\sum_{i=1}^n l(\\rho(\\beta'x_i),y_i) \n",
    "  \\\\\n",
    "  &= n^{-1}\\sum_{i=1}^n \\left[ y_i\\log \\rho(\\beta'x_i) + (1-y_i)\\log(1-\\rho(\\beta'x_i)) \\right]\n",
    "  \\end{aligned}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistics MLE loss function\n",
    "\n",
    "def loss_fn_logistic(bhat, x, y):\n",
    "    rhohat = sigmoid(torch.matmul(x,bhat))\n",
    "    loss = -torch.sum(y*torch.log(rhohat) + (1-y) * torch.log(1-rhohat))/y.shape[0]\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 其他损失函数\n",
    "\n",
    "- KL 散度\n",
    "- 最小化KL散度等价于MLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lect 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 线性多分类\n",
    "\n",
    "$$Y|x \\sim Multinomial(\\rho(Wx))$$\n",
    "\n",
    "- $x_i \\in \\mathbb{R}^p$: 每个obs的数据包括p个特征\n",
    "- $y_i \\in \\mathbb{R}^k$: 每个obs的数据包括k个类别，且此处是one-hot编码，即$y_i$只有一个元素为1，其余为0\n",
    "- $W \\in \\mathbb{R}^{k \\times p}$: 负责将$p$维的特征映射到$k$维的类别\n",
    "- $\\rho(Wx)$: 将$Wx$映射到$(0,1)^k$，表示每个类别的概率\n",
    "\n",
    "***Softmax 回归***\n",
    "- 当取$\\rho(Wx)=\\frac{e^{Wx}}{\\sum_{j=1}^k e^{Wx_j}}$时，称为Softmax回归"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
