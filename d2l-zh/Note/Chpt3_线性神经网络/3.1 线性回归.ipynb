{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 线性回归到神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以认为，线性回归是一个**单层神经网络**，如下图所示。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://michael-1313341240.cos.ap-shanghai.myqcloud.com/202309101523332.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中，输入层$x_1,...,x_d$是各个特征，输出层$o_1$是预测值。\n",
    "\n",
    "我们称这样的一个神经网络为一个**单层神经网络**。\n",
    "\n",
    "同时，由于每个输入与每个输出都相连，因此我们将这个输出层称为**全连接层**。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 随机梯度下降"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们这里讨论**小批量随机梯度下降（mini-batch stochastic gradient descent）**\n",
    "\n",
    "- 其大致思想为：  \n",
    "  在每次迭代中，抽取一个小批量$\\mathcal{B}$，其样本量是固定的（记为$|\\mathcal{B}|$）。我们根据这个小样本来计算梯度，然后通过正常的梯度下降法更新参数。\n",
    "  \n",
    "- 具体算法为：\n",
    "  $$ \\bold{w} := \\bold{w} - \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} \\nabla_{\\bold{w}} L^{(i)}(\\bold{w},b) = \\bold{w} - \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} \\bold{x}^{(i)} \\left(  \\bold{w}^T\\bold{x}^{(i)} +b - y^{(i)} \\right) \n",
    "  \\\\ b := b - \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} \\nabla_{b} L^{(i)}(\\bold{w},b) = b - \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} \\left(  \\bold{w}^T\\bold{x}^{(i)}+b - y^{(i)} \\right) $$\n",
    "  $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*注意这里的Mini-Batch是相当于对行进行抽样！每次只是通过一个小样本来进行梯度更新*"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
