{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b7f6b61f-b170-4a78-bcea-7de8ce8a7257",
   "metadata": {},
   "source": [
    "# 分布式计算 总复习\n",
    "\n",
    "**Python, PySpark & 数值计算基础**\n",
    "\n",
    "- lect2：PySpark初始化, map & reduce\n",
    "- lect3：Python中的函数式编程（思想）\n",
    "- lect4：函数式编程（MapReduce的实现）& 弹幕\n",
    "- lect5,6：Numpy, RDD & 数值计算原则 \n",
    "\n",
    "**机器学习基础**\n",
    "\n",
    "- lect7：ML-（分布式）矩阵乘法&线性回归\n",
    "- lect8：岭回归、共轭梯度、logistics（理论）\n",
    "\n",
    "**优化算法系列**\n",
    "\n",
    "- lect9: logistics的梯降法、牛顿法实现\n",
    "- lect10：logistics的L-BFGS实现; 数值稳定算法，缓存，混洗\n",
    "- lect11：ADMM理论（以LAD，Lasso为例的理论推导）,soft_thresholding\n",
    "- lect12：LAD与Lasso的ADMM实现（非RDD）, Cholesky分解"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1d47e286-b15f-44f3-9905-5eea96fd7a42",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Lect2：PySpark初始化, map & reduce"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5e1ae3d9-a432-4009-8312-b4b1cd94bd83",
   "metadata": {},
   "source": [
    "### 课堂实例：联合国文字的处理"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fe614864-6075-46b9-81eb-3f3b8fcd0765",
   "metadata": {},
   "source": [
    "- PySpark配置和启动"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "886d2f4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x000001C30E48B2B0>\n",
      "<SparkContext master=local[*] appName=Reading Text>\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "# 本地模式\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"Reading Text\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "# sc.setLogLevel(\"ERROR\")\n",
    "print(spark)\n",
    "print(sc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3508d561-eb15-42f8-8da8-814bc35d46a5",
   "metadata": {},
   "source": [
    "- PySpark中文件读取，查看，筛选\n",
    "\n",
    "    `sc.textFile(\"address\")`\n",
    "\n",
    "    `file.count()`\n",
    "\n",
    "    `file.take()`\n",
    "\n",
    "    `file.filter()`\n",
    "\n",
    "    `file.map()`\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5998d574-3360-4a66-9165-62747b63757d",
   "metadata": {},
   "source": [
    "### Slides大纲"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "41077ded",
   "metadata": {},
   "source": [
    "\n",
    "#### 1. 并行计算基本概念 \n",
    "- Amdahl 定律：并行加速程度计算公式\n",
    "  $$S_{latency}(s)=\\frac{1}{(1-p)+p/s}$$\n",
    "  $S$：整体提升倍数，$s$：可并行部分加速比，$p$：可并行部分所占时间的比例\n",
    "\n",
    "#### 2. 并行计算 vs 分布式计算\n",
    "\n",
    "#### 3. Apache Spark简介与安装\n",
    "- Hadoop 分布式计算框架\n",
    "- HDFS 分布式文件系统\n",
    "- MapReduce\n",
    "- Spark：Hadoop改进\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dcbf01e1",
   "metadata": {},
   "source": [
    "#### 4. Spark运行模式\n",
    "- 单机模式\n",
    "- 集群模式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fed6245",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "# 本地模式\n",
    "spark = SparkSession.builder.\\\n",
    "    master(\"local[*]\").\\\n",
    "    appName(\"PySpark RDD\").\\\n",
    "    getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "print(spark)\n",
    "print(sc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3449f5ce-af35-4bf5-9af6-d30d750a231a",
   "metadata": {},
   "source": [
    "## Lect3：Python中的函数式编程（思想）"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9cd8fe81",
   "metadata": {},
   "source": [
    "### 课堂实例：迭代器、计算方差、随机抽样、MapReduce"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e2d06f36",
   "metadata": {},
   "source": [
    "- 迭代器的创建与访问：\n",
    "  - `iter()`\n",
    "  - `next(IT)`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8a2c8493",
   "metadata": {},
   "source": [
    "- 一次遍历计算方差：\n",
    "\n",
    "  $$(n-1)S=\\sum_i (x_i-\\bar{x})^2=\\sum_i x_i^2-n\\bar{x}^2$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "34ba573b",
   "metadata": {},
   "source": [
    "- 随机抽样:\n",
    "  - `for`循环中一次遍历两个循环\n",
    "    - `for a,b in zip(A,B):`\n",
    "  - 遍历的同时获取索引\n",
    "    - ```python\n",
    "      for i, (w, v) in enumerate(zip(wvec, vvec)):\n",
    "      print(f\"i is {i}, w is {w}, v is {v}\")\n",
    "      ```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5076d19c",
   "metadata": {},
   "source": [
    "- Reduce:\n",
    "  - syntax:\n",
    "    ```python\n",
    "        import functools\n",
    "        functools.reduce(FUNCTION,ITER,OPTIONAL)\n",
    "    ```\n",
    "    `OPTIONAL` 是一个可选选项，用来设定reduce的初始数值（例如在累乘时要设置初值为1）\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d19ca650",
   "metadata": {},
   "source": [
    "- Filter:\n",
    "  - syntax:\n",
    "    - `filter(IS_FUNCTION,ITER)`\n",
    "\n",
    "      其中`IS_FUNCTION`是一个需要返回`TRUE/FALSE`的函数"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "57fdc6e8",
   "metadata": {},
   "source": [
    "- Map:\n",
    "  - syntax:\n",
    "    - `map(FUNCTION,ITER)`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d1e9088a",
   "metadata": {},
   "source": [
    "- islice （按照指定长度截断迭代器）:\n",
    "  - syntax:\n",
    "    - ```python\n",
    "        import itertools\n",
    "        itertools.islice(ITER,NUM)\n",
    "        ```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c0b99553",
   "metadata": {},
   "source": [
    "- `lambda`函数"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e22d8fd2",
   "metadata": {},
   "source": [
    "### slides 讲解\n",
    "\n",
    "- 函数式编程\n",
    "  - 概念、特点\n",
    "  - 核心思想\n",
    "  - 迭代器：YOLO！\n",
    "  - 迭代器全部取出为列表：`lst=list(it)`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ec040c89",
   "metadata": {},
   "source": [
    "## Lect4：函数式编程（MapReduce的实现）& 弹幕"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b0ef3fc4",
   "metadata": {},
   "source": [
    "- 样本方差计算\n",
    "    ```python\n",
    "    num,sum,sq_sum = data.map(lambda x: (1,x,x*x)).reduce(lambda x,y: (x[0]+y[0],x[1]+y[1],x[2]+y[2]))\n",
    "    mean = sum / num\n",
    "    sample_var = (sq_sum+num*mean*mean-2*mean*sum)/(num-1)\n",
    "    print(sample_var)\n",
    "    ```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c6121c7e",
   "metadata": {},
   "source": [
    "## Lect5,6：Numpy, RDD & 数值计算原则"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a05360a9",
   "metadata": {},
   "source": [
    "### 5.1 Numpy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f78b7c82",
   "metadata": {},
   "source": [
    "#### 基础操作\n",
    "\n",
    "- `np.array([1,2,3],[3,4,5])` 定义数列（矩阵）\n",
    "\n",
    "- `np.linspace(start= 1 , stop=5 , num= 12 )`  生成连续数列\n",
    "\n",
    "- `np.arange(12)` 生成[0,1,...,12]的array\n",
    "\n",
    "- `np.reshape(VEC, (3,4))` / `vec.reshape(3,4)`\n",
    "\n",
    "- `np.ones((3,2))` !不是生成单位矩阵，而是全1矩阵\n",
    "\n",
    "- `np.zeros((2,3))` 零矩阵\n",
    "\n",
    "- `np.eyes(5)` 只需要输入一个数字，即可生成对应单位阵\n",
    "\n",
    "- `np.diag([1,3,5])` 对角矩阵\n",
    "\n",
    "- `vec[-1]` 表示倒数第1个元素\n",
    "\n",
    "- `vec[start:end]` 左闭右开\n",
    "\n",
    "- `mat[:,:2]` 表示全部行，前两列（注意python中下标从1开始）\n",
    "\n",
    "- `np.random.uniform(low = , high = , size = )` 均匀分布\n",
    "\n",
    "- `np.random.normal(loc = 2 , scale = 3, size = (2,5))` 正态分布 *loc为均值，scale为标准差（非方差）*\n",
    "\n",
    "- `np.log()` `np.exp()` 对数与指数\n",
    "\n",
    "- `np.sum(MAT, axis = 0)` 矩阵的求和，其中`axis=0` 表示对每个列求一个和\n",
    "\n",
    "- `np.mean(MAT, axis = 1)` 矩阵的平均，其中`axis = 1` 表示对每个行求一个平均\n",
    "\n",
    "\n",
    ">汇总可以按行或者按列进行，这由`axis`参数决定。（联想到矩阵元素下标$a_{ij}$是先行后列，故这里`axis`是0行1列）0表示运算时第一个维度（行）在变化，1表示运算时第二个维度（列）在变化。再次提醒，Python中以0表示第一个元素！\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6f2d2587",
   "metadata": {},
   "source": [
    "#### 线性代数\n",
    "\n",
    "- `MAT.transpose()` 转置\n",
    "\n",
    "- `MAT_A.dot(MAT_B)` / `np.matmul(MAT_A,MAT_B)` 矩阵乘法\n",
    "\n",
    "- `np.linalg.inv(MAT)` 矩阵的逆(不推荐)\n",
    "\n",
    "- `np.linalg.solve(B,d)` 解线性方程组 (返回$B^{-1}d$)\n",
    "\n",
    "- `evals, evecs = np.linalg.eigh(MAT)` 求特征值，特征向量\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "721e58c3",
   "metadata": {},
   "source": [
    "### 5.2 RDD"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c7dee9d0",
   "metadata": {},
   "source": [
    "PySpark 初始化\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb463a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "# 本地模式\n",
    "spark = SparkSession.builder.\\\n",
    "    master(\"local[*]\").\\\n",
    "    appName(\"PySpark RDD\").\\\n",
    "    getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "print(spark)\n",
    "print(sc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ae5700db",
   "metadata": {},
   "source": [
    "#### RDD的创建\n",
    "\n",
    "- `DAT = sc.parallelize(VEC)` 将一个数据(i.e. VEC)转化为rdd对象(i.e. DAT)\n",
    "\n",
    "- `DAT.collect()` 收集所有内容\n",
    "\n",
    "- `DAT.count()` 返回的实质为RDD的迭代次数\n",
    "\n",
    "- `DAT.take()` / `DAT.first()` 取RDD前几个next的内容\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "81786367",
   "metadata": {},
   "source": [
    "#### RDD 对.txt文件的处理\n",
    "\n",
    "- `file = sc.textFile(\"ADDRESS\")` 读入文本文件\n",
    "\n",
    "- `print(*(file.take(5)), sep=\"\\n\")` 对前五行的打印\n",
    "\n",
    "- 将string类型数据转换成numpy数据：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef53b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# str => np.array\n",
    "def str_to_vec(line):\n",
    "    # 分割字符串\n",
    "    str_vec = line.split(\"\\t\")\n",
    "    # 让 Numpy 进行类型转换\n",
    "    return np.array(str_vec, dtype=float)\n",
    "DAT = file.map(str_to_vec)    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4d182af0",
   "metadata": {},
   "source": [
    "#### RDD的分区\n",
    "\n",
    "- `file.getNumPartitions()` 获得自动分区数\n",
    "\n",
    "- `file.repartition(NUM)` 自定义分区数\n",
    "\n",
    "- 对于切分完的分区，我们希望把一个分区中的数据按照一个矩阵来理解，故：\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdc1f48",
   "metadata": {},
   "outputs": [],
   "source": [
    " # str => np.array\n",
    "def str_to_vec(line):\n",
    "    # 分割字符串\n",
    "    str_vec = line.split(\"\\t\")\n",
    "    # 让 Numpy 进行类型转换\n",
    "    return np.array(str_vec, dtype=float)\n",
    "DAT = file.map(str_to_vec)    \n",
    " # Iter[str] => Iter[matrix]\n",
    "def part_to_mat(iterator):\n",
    "    # Iter[str] => Iter[np.array]\n",
    "    iter_arr = map(str_to_vec, iterator)\n",
    "\n",
    "\n",
    "    # Iter[np.array] => list(np.array)\n",
    "    dat = list(iter_arr) #dat是由向量（nparray）作为元素组成的列表（list）\n",
    "\n",
    "    # 有的分区可能是空分区\n",
    "    # list(np.array) => matrix\n",
    "    if len(dat) < 1:  # Test zero iterator\n",
    "        mat = np.array([])\n",
    "    else:\n",
    "        mat = np.vstack(dat) \n",
    "\n",
    "    # matrix => Iter[matrix]\n",
    "    yield mat # yield可以认为是return[mat]，返回的不是mat本身，而是包含这个的容器\n",
    "\n",
    "dat_p10 = file_p10.mapPartitions(part_to_mat)\n",
    "\n",
    "dat_p10_nonempty = dat_p10.filter(lambda x: x.shape[0] > 0)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "151ca829",
   "metadata": {},
   "source": [
    "### 5.3 数值计算原则"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4ca2a91b",
   "metadata": {},
   "source": [
    "#### 原则1：矩阵相乘，小维度优先\n",
    "\n",
    "- 经验法则：对于更一般的矩阵乘法 $A_{m\\times n}B_{n\\times p}C_{p\\times r}$，如果 $n\\approx p$ 且 $m>r$，则优先计算 $BC$，反之优先计算 $AB$。\n",
    "\n",
    "- 对于一个$A_{n\\times p }\\times B_{p\\times q}$的矩阵乘法，时间复杂度为$O(npq)$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "744037e7",
   "metadata": {},
   "source": [
    "#### 原则2：尽量避免显式矩阵求逆"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "03a61f01",
   "metadata": {},
   "source": [
    "常见矩阵运算复杂度整理：\n",
    "\n",
    "（假设$A,B : n\\times n, b :n\\times1$）\n",
    "\n",
    "矩阵乘法：\n",
    "- $AB:O(n^3)$\n",
    "- $Ab:O(n^2)$\n",
    "\n",
    "矩阵的逆：\n",
    "- $A^{-1}:O(n^3)$\n",
    "- $A^{-1}b:O(n^3)$ （但从实证方面看比求逆效率更高）\n",
    "\n",
    "矩阵的其他运算：\n",
    "- $|A|, eign(A) :O(n^3) $\n",
    "- 特别的：上/下三角矩阵的行列式$O(n)$ （行列式为对角线的乘积）\n",
    "- $||A||_p^2 : O(n^2)$\n",
    "- $ A+b1^T : O(n^2)$ 相当于将$b$广播\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "99a2d523",
   "metadata": {},
   "source": [
    "#### 原则3：利用矩阵的特殊结构\n",
    "\n",
    "- 对于对角矩阵，在存储时应当以向量存储对角线元素即可，事实上在参与计算中也可以通过广播机制直接以向量参与运算：\n",
    "  - 假设$D=diag(d_1,...,d_n)$为对角线元素，$A$为正常矩阵，则$DA$相当于对第i列乘以$d_i$，$AD$相当于对第i行乘以$d_i$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "08c8df77",
   "metadata": {},
   "source": [
    "#### 原则4：尽可能将显示循环转化为矩阵计算\n",
    "\n",
    "- 虽然理论复杂度相似，但实证上，考虑通信成本等，矩阵运算更快"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688a28cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 初始化pyspark\n",
    "\n",
    "import findspark\n",
    "findspark.init(\"/Users/xinby/Library/Spark\")\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "# 本地模式\n",
    "spark = SparkSession.builder.\\\n",
    "    master(\"local[*]\").\\\n",
    "    appName(\"PySpark RDD\").\\\n",
    "    getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "print(spark)\n",
    "print(sc)\n",
    "\n",
    "# 2. 生成数据\n",
    "\n",
    "import numpy as np\n",
    "np.set_printoptions(linewidth=100)\n",
    "\n",
    "np.random.seed(123)\n",
    "n = 100\n",
    "p = 5\n",
    "mat = np.random.normal(size=(n, p))\n",
    "np.savetxt(\"mat_np.txt\", mat, fmt=\"%f\", delimiter=\"\\t\")\n",
    "\n",
    "# 3. 读取数据至RDD并分区\n",
    "\n",
    "file = sc.textFile(\"mat_np.txt\")\n",
    "file_p10 = file.repartition(10)\n",
    "# str => np.array\n",
    "def str_to_vec(line):\n",
    "    # 分割字符串\n",
    "    str_vec = line.split(\"\\t\")\n",
    "    # 将每一个元素从字符串变成数值型\n",
    "    num_vec = map(lambda s: float(s), str_vec)\n",
    "    # 创建 Numpy 向量\n",
    "    return np.fromiter(num_vec, dtype=float)\n",
    "\n",
    "# Iter[str] => Iter[matrix]\n",
    "def part_to_mat(iterator):\n",
    "    # Iter[str] => Iter[np.array]\n",
    "    iter_arr = map(str_to_vec, iterator)\n",
    "\n",
    "    # Iter[np.array] => list(np.array)\n",
    "    dat = list(iter_arr)\n",
    "\n",
    "    # list(np.array) => matrix\n",
    "    if len(dat) < 1:  # Test zero iterator\n",
    "        mat = np.array([])\n",
    "    else:\n",
    "        mat = np.vstack(dat)\n",
    "\n",
    "    # matrix => Iter[matrix]\n",
    "    yield mat\n",
    "\n",
    "dat = file_p10.mapPartitions(part_to_mat).filter(lambda x: x.shape[0] > 0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "77d7e765",
   "metadata": {},
   "source": [
    "## Lect 7：ML-（分布式）矩阵乘法&线性回归（OLS）"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bdd0567c",
   "metadata": {},
   "source": [
    "### 7.1 矩阵乘法（RDD）"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "67b01e79",
   "metadata": {},
   "source": [
    "#### 0. *准备工作*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "77afeaf0",
   "metadata": {},
   "source": [
    "#### 1. 矩阵乘法$Xv$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3e9ec525",
   "metadata": {},
   "source": [
    "**原理**\n",
    "\n",
    "假设：$ X \\in \\R^{n\\times p}, v \\in \\R^{p}$\n",
    "\n",
    "将$X$按照行进行分块（包含所有列，但行不一定是一行），记为：$X=[X_1;...;X_m]^T$, $X_i \\in \\R^{n_i\\times p}$\n",
    "\n",
    "故$Xv=[X_1v;...;X_mv]^T$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5982fffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "v = np.random.uniform(size=p)\n",
    "\n",
    "# 这里不是reduce是因为不是要把最后分块进行加和，而是想要最后的拼接\n",
    "res_part = dat.map(lambda x: x.dot(v)).collect() \n",
    "print(res_part,type(res_part))\n",
    "# 这个结果里，一个array就是前面的一个分块\n",
    "np.concatenate(res_part)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "151ba342",
   "metadata": {},
   "source": [
    "#### 2. 矩阵乘法 $X'X$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fa086321",
   "metadata": {},
   "source": [
    "**原理**\n",
    "\n",
    "同理进行分块，则最终的结果为：$X'X=X_1'X_1+\\dots+X_m'X_m$ (由于这里的假定$X\\in\\R^{n\\times p}, n>>p$，故最终的大小为$p\\times p$，是可以存储在内存中的)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b786aeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = dat.map(lambda x: x.T.dot(x)).reduce(lambda x, y: x + y) \n",
    "# x代表了前面的累积结果，y代表了最新一项"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6ee9b0fe",
   "metadata": {},
   "source": [
    "#### 3. 矩阵乘法 $X'v$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bd8b96f9",
   "metadata": {},
   "source": [
    "**原理**\n",
    "\n",
    "这里设定$X,v$的行数相同，故可以同时进行拆分(在具体操作时认为是在一个大矩阵中)，$X'v = X_1'v_1+...+X_m'v_m$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db32e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = mat[:, :-1]\n",
    "v = mat[:, -1]\n",
    "def Xitv(part):\n",
    "    '''\n",
    "    在定义这个map函数的时候，由于整体是一个大矩阵，但是在内部需要将X和v区分开来\n",
    "    '''\n",
    "    Xi = part[:, :-1]\n",
    "    vi = part[:, -1]\n",
    "    return Xi.transpose().dot(vi)\n",
    "\n",
    "res = dat.map(Xitv).reduce(lambda x, y: x + y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "da44bd44",
   "metadata": {},
   "source": [
    "### 7.2 线性回归（OLS） （RDD）"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8d33f0ff",
   "metadata": {},
   "source": [
    "**理想OLS的理论模型与假设：**\n",
    "\n",
    "$$y = \\beta_0 + \\beta'x +\\epsilon$$\n",
    "\n",
    "其中 $X\\in \\R^{n\\times p}$，这里要求$n>>p$\n",
    "\n",
    "$$\\hat\\beta = (X'X)^{-1}X'y$$\n",
    "\n",
    "- 若要包含截距项，则$X$的第一列为1\n",
    "\n",
    "**计算过程**\n",
    "1. 首先计算$X'X,X'y$\n",
    "2. 然后计算$(X'X)^{-1}X'y$\n",
    "   \n",
    "事实上，由矩阵分块原理，可以只计算一次：$X'[X;y]$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ca6bdc27",
   "metadata": {},
   "source": [
    "#### 实例：RDD实现 （hw4）"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e9fbe523",
   "metadata": {},
   "source": [
    "##### 参数估计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62044956",
   "metadata": {},
   "outputs": [],
   "source": [
    "xt_xy = data_partition_nonempty.\\\n",
    "    map(lambda x: np.hstack((np.ones((np.shape(x)[0],1)),x))).\\\n",
    "    map(lambda x: x[:,:-1].transpose().dot(x) ).\\\n",
    "    reduce (lambda x,y: x+y)\n",
    "\n",
    "xt_x = xt_xy[:,:-1]\n",
    "xt_y = xt_xy[:,-1]\n",
    "\n",
    "hat_beta = np.linalg.solve(xt_x,xt_y)\n",
    "print(hat_beta)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d14e83c2",
   "metadata": {},
   "source": [
    "##### R_sq"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4b8e6b84",
   "metadata": {},
   "source": [
    "相关公式：\n",
    "$$SSR = \\sum (y_i-\\hat y_i)^2 = ||Y-X\\hat\\beta||_2$$\n",
    "$$SST = \\sum (y_i - \\bar y)^2 = \\sum y_i^2+n\\bar y^2-2n\\bar y \\sum y_i $$\n",
    "$$ R^2 = 1 - SSR/SST$$\n",
    "\n",
    "假设：\n",
    "1. 已知$\\hat\\beta, [X,Y]$\n",
    "2. 数据经过mapPartition在多个矩阵中存储\n",
    "   \n",
    "计算过程：\n",
    "1. 扩充$X := [1,X]$\n",
    "2. 计算 $Y-X\\hat\\beta$，稍后对其进行平方求和\n",
    "3. 计算 $\\sum y_i, \\sum y_i^2$\n",
    "4. reduce，根据上述Rsquare公式进行整合计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581186e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_y, sum_y_sq, ssr, num = data_partition_nonempty.\\\n",
    "    map(lambda x: np.hstack((np.ones((np.shape(x)[0],1)),x))).\\\n",
    "    map(lambda x: (np.sum(x[:,-1]),np.sum(x[:,-1]**2),np.sum(x[:,-1]-x[:,:-1].dot(hat_beta)**2,axis=0),np.shape(x)[0])).\\\n",
    "    reduce(lambda x,y:(x[0]+y[0],x[1]+y[1],x[2]+y[2],x[3]+y[3]) )\n",
    "print(sum_y,sum_y_sq, ssr, num)\n",
    "y_bar = sum_y/num\n",
    "sst = sum_y_sq + n*y_bar**2 -2*num*y_bar*sum_y\n",
    "R_sq = 1 - ssr / sst\n",
    "print(f\"R^2: {R_sq}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3ffd4b26",
   "metadata": {},
   "source": [
    "## Lect8：岭回归、共轭梯度法、logistics（理论）"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "017372da",
   "metadata": {},
   "source": [
    "### 8.2 共轭梯度法"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "81b3e23b",
   "metadata": {},
   "source": [
    "**算法目的**\n",
    "\n",
    "求解$Ax=b$（只要$A_{n \\times n}$是正定的，n步内即求得精确解）\n",
    "\n",
    "- 时间复杂度最差为$O(n^3)$\n",
    "\n",
    "*具体算法细节参见codes*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8b906e03",
   "metadata": {},
   "source": [
    "**CG-Python实现**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3280686",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cg(A, b, x0, eps=1e-3, print_progress=False):\n",
    "    ''' 已知Ax=b中的A与b,求解x\n",
    "    A: n*n\n",
    "    b: n*1\n",
    "    x0: 一个初始迭代内容,可以用np.zeros(shape=m)等初始化\n",
    "    eps: 精度\n",
    "    print_progress: 是否打印迭代过程\n",
    "    '''\n",
    "    m = b.shape[0]\n",
    "    # 初始解（注意此处应该复制x0，否则程序退出时会修改x0）\n",
    "    x = np.copy(x0)\n",
    "    # 初始残差向量\n",
    "    r = b - np.dot(A, x)\n",
    "    # 初始共轭梯度\n",
    "    p = r\n",
    "\n",
    "    for k in range(m):\n",
    "        # 矩阵乘法\n",
    "        Ap = np.dot(A, p)\n",
    "        rr = r.dot(r)\n",
    "        alpha = rr / p.dot(Ap)\n",
    "        # 更新解\n",
    "        x += alpha * p\n",
    "        # 计算新残差向量\n",
    "        rnew = r - alpha * Ap\n",
    "        # 测试是否收敛\n",
    "        norm = np.linalg.norm(rnew)\n",
    "        if print_progress:\n",
    "            print(f\"Iter {k}, residual norm = {norm}\")\n",
    "        if norm < eps:\n",
    "            break\n",
    "        beta = rnew.dot(rnew) / rr\n",
    "        # 更新共轭梯度\n",
    "        p = rnew + beta * p\n",
    "        # 更新残差向量\n",
    "        r = rnew\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e36a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 更通用的共轭梯度法：ABSTRACTION!\n",
    "\n",
    "def cg(Afn, b, x0, eps=1e-3, print_progress=False, **Afn_args):\n",
    "    ''' 已知Ax=b中的A与b,求解x\n",
    "    在cg的实际计算中,我们永远只关心A与其他向量的乘法结果,\n",
    "    而并不关心A本身的数值,因此可以通过函数调用的方式来实现\n",
    "    Afn: 传入一个函数，该函数应当形如 PROD(X,MATRIX),\n",
    "         其中大写字母表示可以自定义的名称\n",
    "         该函数将返回MAT.dot(X)\n",
    "         其优势在于比如处理对角矩阵时可以通过广播节省计算\n",
    "    b, x0, eps, print_progress: 与cg函数相同\n",
    "    Afn_args: 传入Afn的所有其他参数(可以是多个,按序给出即可),譬如MATRIX=A (A就是cg中的那个A)\n",
    "    '''\n",
    "    m = b.shape[0]\n",
    "    # 初始解（注意此处应该复制x0，否则程序退出时会修改x0）\n",
    "    x = np.copy(x0)\n",
    "    # 初始残差向量\n",
    "    r = b - Afn(x, **Afn_args)\n",
    "    # 初始共轭梯度\n",
    "    p = r\n",
    "\n",
    "    for k in range(m):\n",
    "        # 矩阵乘法\n",
    "        Ap = Afn(p, **Afn_args)\n",
    "        rr = r.dot(r)\n",
    "        alpha = rr / p.dot(Ap)\n",
    "        # 更新解\n",
    "        x += alpha * p\n",
    "        # 计算新残差向量\n",
    "        rnew = r - alpha * Ap\n",
    "        # 测试是否收敛\n",
    "        norm = np.linalg.norm(rnew)\n",
    "        if print_progress:\n",
    "            print(f\"Iter {k}, residual norm = {norm}\")\n",
    "        if norm < eps:\n",
    "            break\n",
    "        beta = rnew.dot(rnew) / rr\n",
    "        # 更新共轭梯度\n",
    "        p = rnew + beta * p\n",
    "        # 更新残差向量\n",
    "        r = rnew\n",
    "\n",
    "    return x\n",
    "\n",
    "def mat_prod(x, mat):\n",
    "    return mat.dot(x)\n",
    "\n",
    "def diag_mat_prod(x, diag_elements):\n",
    "    return diag_elements * x  # 逐元素相乘，非矩阵乘法\n",
    "\n",
    "sol = cg(diag_mat_prod, b, x0=np.zeros(shape=m), print_progress=True, diag_elements=np.diagonal(A))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5e3f49ad",
   "metadata": {},
   "source": [
    "### 8.1 岭回归"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7d04bd32",
   "metadata": {},
   "source": [
    "**理论**\n",
    "\n",
    "- 当 $n<p$ 时，$X'X$ 不可逆，此时最小二乘**没有唯一解** （事实上是有无数组解使得$\\min S_c=0$，即可以完美拟合）【并不是说不存在解！！】\n",
    "- 此时的OLS并不影响预测性，但是并不影响解释性\n",
    "- 此时我们可以采用岭回归的方法，其在最小二乘损失函数的基础上加入一个惩罚项 \n",
    "\n",
    "$$Loss = ||Y-X\\beta||_2+\\lambda ||\\beta||^2$$\n",
    "\n",
    "$$\\hat\\beta_\\lambda = (X^TX+\\lambda I)^{-1}X^TY$$\n",
    "\n",
    "- 但注意到 $X'X+\\lambda I$ 是一个高维的矩阵($p\\times p$)，难以直接进行求解。因此我们采用共轭梯度法"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "60228ce9",
   "metadata": {},
   "source": [
    "**Python 实现**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7b00f312",
   "metadata": {},
   "source": [
    "在岭回归中，我们需要求解 $\\hat\\beta_\\lambda = (X^TX+\\lambda I)^{-1}X^TY$\n",
    "\n",
    "参照cg的形式，也就是对于方程$Ax=b$，求解$x$：其中$A=X^TX+\\lambda I, b=X^TY$\n",
    "\n",
    "观察cg的算法以及前面“更通用的cg”中介绍的，对A的矩阵乘法可以abstracted成一些函数的形式以提高效率。具体而言，也就是在cg中要实现一些形如$Av = (X^TX+\\lambda I)v \\equiv X^TXv+\\lambda v$的乘法\n",
    "\n",
    "因此重点是思考如何高效计算$X^TXv$:\n",
    "\n",
    "若将$X$按行分块为$X=(X_1;X_2;...;X_m)'$, 则$X'(Xv) = X_1'X_1v_1+...+X_m'X_mv_m$, 而这便可以分布式实现。\n",
    "\n",
    "对于rdd中的每一个分块，计算$X_i'(X_iv_i)$，然后将各个分块的结果reduce即可（注意乘法的顺序）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271ea132",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xtxv(part, v):\n",
    "    '''该函数是非分布式,其含义只是单纯输入一个X,v,输出X.TX.v\n",
    "    part: 从第二列开始是X的数据(第一列为y)\n",
    "    '''\n",
    "    x = part[:, 1:]\n",
    "    return x.transpose().dot(x.dot(v))\n",
    "\n",
    "def ridge_prod(v, lam, rdd):\n",
    "    '''计算X'Xv+lam*v,其中X'Xv是分布式计算的\n",
    "    v,lam: ridge回归的参数\n",
    "    rdd: 每个rdd为数据的一个分块,形式上相当于一个分块矩阵,对于每个分块矩阵计算一个矩阵乘法,\n",
    "         最终再用reduce加总起来\n",
    "    ridge_prod: 返回X'Xv+lam*v\n",
    "    '''\n",
    "    first_term = rdd.map(lambda part: xtxv(part, v)).reduce(lambda x, y: x + y)\n",
    "    second_term = lam * v\n",
    "    return first_term + second_term\n",
    "\n",
    "def cg(Afn, b, x0, eps=1e-3, print_progress=False, **Afn_args):\n",
    "    m = b.shape[0]\n",
    "    # 初始解（注意此处应该复制x0，否则程序退出时会修改x0）\n",
    "    x = np.copy(x0)\n",
    "    # 初始残差向量\n",
    "    r = b - Afn(x, **Afn_args)\n",
    "    # 初始共轭梯度\n",
    "    p = r\n",
    "\n",
    "    for k in range(m):\n",
    "        # 矩阵乘法\n",
    "        Ap = Afn(p, **Afn_args)\n",
    "        rr = r.dot(r)\n",
    "        alpha = rr / p.dot(Ap)\n",
    "        # 更新解\n",
    "        x += alpha * p\n",
    "        # 计算新残差向量\n",
    "        rnew = r - alpha * Ap\n",
    "        # 测试是否收敛\n",
    "        norm = np.linalg.norm(rnew)\n",
    "        if print_progress:\n",
    "            print(f\"Iter {k}, residual norm = {norm}\")\n",
    "        if norm < eps:\n",
    "            break\n",
    "        beta = rnew.dot(rnew) / rr\n",
    "        # 更新共轭梯度\n",
    "        p = rnew + beta * p\n",
    "        # 更新残差向量\n",
    "        r = rnew\n",
    "\n",
    "    return x\n",
    "    \n",
    "b = dat.map(lambda part: part[:, 1:].transpose().dot(part[:, 0])).reduce(lambda x, y: x + y)\n",
    "lam = 0.01 * n\n",
    "sol = cg(ridge_prod, b, x0=np.zeros(shape=p), eps=1e-3, print_progress=True, lam=lam, rdd=dat)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "544f358c",
   "metadata": {},
   "source": [
    "### 8.3 Logistics 理论"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e936753e",
   "metadata": {},
   "source": [
    "$$Y|x \\sim Bernoulli(\\rho(\\beta^T x))$$\n",
    "\n",
    "Sigmoid: $\\rho(x) = \\frac{1}{1+e^{-x}}$,表示$y=1$的概率\n",
    "\n",
    "故通过MLE：\n",
    "$$l = \\sum \\log P(Y_i =y_i) = \\sum[ y_i \\log p_i + ( 1 - y_i) \\log (1 - p_i)]$$\n",
    "where $p_i = \\rho(x_i' \\beta)$\n",
    "\n",
    "而求解使得$l$最大的$\\beta$，需要通过数值解法，这也是后面几节的重点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c764e9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_to_vector(line):\n",
    "    vector = line.split(\"\\t\")\n",
    "    vector = np.append(vector,1.0) #可以通过这个代码添加一行1\n",
    "    return np.array(vector, dtype=float)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a9ad7697",
   "metadata": {},
   "source": [
    "## Lect9: logistics的梯降法、牛顿法实现"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8ca0c646",
   "metadata": {},
   "source": [
    "**优化目标函数**\n",
    "\n",
    "$$\\argmin \\beta = l= \\sum[ y_i \\log p_i + ( 1 - y_i) \\log (1 - p_i)]$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5b3ff998",
   "metadata": {},
   "source": [
    "### 9.1 梯降法"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4a113a08",
   "metadata": {},
   "source": [
    "**理论**\n",
    "\n",
    "$$\\beta^{new} = \\beta^{old}-\\alpha \\frac{\\partial l}{\\partial \\beta}|_{\\beta=\\beta^{old}} =\\beta^{old}-\\alpha\\cdot n^{-1}X'(prob-y)$$\n",
    "其中$prob=\\rho(X\\beta^{old})$\n",
    "\n",
    "梯降法一次迭代的时间复杂度为$O(np)$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "02dbce4e",
   "metadata": {},
   "source": [
    "**Python实现**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52385e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import expit, logit #expit为sigmoid函数\n",
    "\n",
    "def compute_obj_grad(part_mat, beta_old):\n",
    "    '''计算目标函数值和梯度(针对logistics),\n",
    "    不是梯降法主函数,而是通过调用它来计算梯降法的关键数据\n",
    "    ----[INPUT]----\n",
    "    part_mat:rdd,是原始数据的一个分块矩阵,第一列是y,其余列是X\n",
    "    beta_old:np.array,当前的beta值\n",
    "\n",
    "    ----[OUTPUT]----\n",
    "    ni:该分块的样本量\n",
    "    obj:目标函数值\n",
    "    grad:梯度(即目标函数在当前beta处的导数值)\n",
    "    '''\n",
    "    # 提取 X 和 y\n",
    "    y = part_mat[:, 0]\n",
    "    x = part_mat[:, 1:]\n",
    "    # X * beta\n",
    "    xb = x.dot(beta_old)\n",
    "    # rho(X * beta)\n",
    "    prob = expit(xb)\n",
    "    # 目标函数：sum(y * log(prob) + (1 - y) * log(1 - prob))\n",
    "    obj = -np.sum(y * np.log(prob + 1e-8) + (1.0 - y) * np.log(1.0 - prob + 1e-8))\n",
    "    # 梯度：X'(prob - y)\n",
    "    grad = x.transpose().dot(prob - y)\n",
    "    # 该分块的样本量\n",
    "    ni = x.shape[0]\n",
    "    return ni, obj, grad\n",
    "\n",
    "#### 初始化 ####\n",
    "# 根据数据动态获取维度，p为x变量个数\n",
    "p = dat.first().shape[1] - 1\n",
    "# beta 初始化为 0 向量\n",
    "beta_hat2 = np.zeros(p)\n",
    "# 记录目标函数值\n",
    "objvals = []\n",
    "# 最大迭代次数\n",
    "maxit = 100\n",
    "# 步长\n",
    "step_size = 10.00\n",
    "# 收敛条件\n",
    "eps = 1e-5\n",
    "\n",
    "dat.cache() #缓存机制\n",
    "\n",
    "#### 迭代 ####\n",
    "for i in range(maxit):\n",
    "    # 完整数据的样本量和梯度是各分区的加和（因为导数是具有线性性的）\n",
    "    n, objfn, grad = dat.map(lambda part: compute_obj_grad(part, beta_hat2)).\\\n",
    "        reduce(lambda x, y: (x[0] + y[0], x[1] + y[1], x[2] + y[2]))\n",
    "    objfn /= n\n",
    "    grad /= n\n",
    "    # 计算新 beta\n",
    "    beta_new = beta_hat2 - step_size * grad\n",
    "    # 计算梯度的变化\n",
    "    grad_norm = np.linalg.norm(grad)\n",
    "    beta_norm = np.linalg.norm(beta_hat2)\n",
    "    print(f\"Iteration {i}, objfn = {objfn}, grad_norm = {grad_norm}, beta_norm = {beta_norm}\")\n",
    "    objvals.append(objfn)\n",
    "    # 如果梯度值较小，退出循环\n",
    "    if grad_norm < eps or grad_norm < eps * beta_norm:\n",
    "        break\n",
    "    # 更新 beta\n",
    "    beta_hat2 = beta_new"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2bc0dbc2",
   "metadata": {},
   "source": [
    "### 9.2 牛顿法"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9f9b2f98",
   "metadata": {},
   "source": [
    "**理论**\n",
    "\n",
    "按照牛顿法general的公式对$l$求导，可以得到针对logistics的牛顿法迭代公式：\n",
    "\n",
    "$$\\beta^{new} = (X'WX)^{-1}X'Wz = (X'WX)^{-1}X'W(X\\beta^{old}+W^{-1}(y-prob))$$\n",
    "\n",
    "其中:\n",
    "\n",
    "1. $prob=\\rho(X\\beta^{old})$\n",
    "\n",
    "2. $W$是一个对角矩阵，$W_{ii}=prob_i(1-prob_i)$\n",
    "\n",
    "因此又转化为求解$X'WX,X'Wz$，再解线性方程组的问题，并且其中只有$z$是参与迭代的。\n",
    "\n",
    "*！不要显式存储对角矩阵W，使用广播实现*\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bdfa0898",
   "metadata": {},
   "source": [
    "**Python实现**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac62009",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_stats(part_mat, beta_old):\n",
    "    # 提取 X 和 y\n",
    "    y = part_mat[:, 0]\n",
    "    x = part_mat[:, 1:]\n",
    "    \n",
    "    # X * beta\n",
    "    xb = x.dot(beta_old)\n",
    "\n",
    "    # rho(X * beta)\n",
    "    prob = expit(xb)\n",
    "\n",
    "    # W 的对角线元素\n",
    "    w = prob * (1.0 - prob) + 1e-6\n",
    "\n",
    "    # X'W，数组广播操作，避免生成完整的 W\n",
    "    xtw = x.transpose() * w\n",
    "\n",
    "    # X'WX\n",
    "    xtwx = xtw.dot(x)\n",
    "\n",
    "    # X'Wz\n",
    "    z = xb + (y - prob) / w\n",
    "\n",
    "    xtwz = xtw.dot(z)\n",
    "\n",
    "    # 目标函数：sum(y * log(prob) + (1 - y) * log(1 - prob))\n",
    "    objfn = -np.sum(y * np.log(prob + 1e-8) + (1.0 - y) * np.log(1.0 - prob + 1e-8))\n",
    "    return xtwx, xtwz, objfn\n",
    "\n",
    "# 根据数据动态获取维度，不要使用之前模拟时的变量\n",
    "p = dat.first().shape[1] - 1\n",
    "# beta 初始化为 0 向量\n",
    "beta_hat = np.zeros(p)\n",
    "# 记录目标函数值\n",
    "objvals = []\n",
    "# 最大迭代次数\n",
    "maxit = 30\n",
    "# 收敛条件\n",
    "eps = 1e-6\n",
    "\n",
    "for i in range(maxit):\n",
    "    # 完整数据的 X'WX 和 X'Wz 是各分区的加和\n",
    "    xtwx, xtwz, objfn = dat.map(lambda part: compute_stats(part, beta_hat)).\\\n",
    "        reduce(lambda x, y: (x[0] + y[0], x[1] + y[1], x[2] + y[2]))\n",
    "    # 计算新 beta\n",
    "    beta_new = np.linalg.solve(xtwx, xtwz)\n",
    "    # 计算 beta 的变化\n",
    "    resid = np.linalg.norm(beta_new - beta_hat)\n",
    "    print(f\"Iteration {i}, objfn = {objfn}, resid = {resid}\")\n",
    "    objvals.append(objfn)\n",
    "    # 如果 beta 几乎不再变化，退出循环\n",
    "    if resid < eps:\n",
    "        break\n",
    "    # 更新 beta\n",
    "    beta_hat = beta_new"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1d994efb",
   "metadata": {},
   "source": [
    "## Lect10：logistics的L-BFGS实现，数值稳定算法，缓存，混洗"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e4baf8f9",
   "metadata": {},
   "source": [
    "### 10.1 L-BFGS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "02d8bda1",
   "metadata": {},
   "source": [
    "**Python实现**\n",
    "\n",
    "`scipy.optimize.minimize`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1685084a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "def compute_obj_grad(part_mat, beta_old):\n",
    "    # 提取 X 和 y\n",
    "    y = part_mat[:, 0]\n",
    "    x = part_mat[:, 1:]\n",
    "    # X * beta\n",
    "    xb = x.dot(beta_old)\n",
    "    # rho(X * beta)\n",
    "    prob = expit(xb)\n",
    "    # 目标函数：sum(y * log(prob) + (1 - y) * log(1 - prob))\n",
    "    obj = -np.sum(y * np.log(prob + 1e-8) + (1.0 - y) * np.log(1.0 - prob + 1e-8))\n",
    "    # 梯度：X'(prob - y)\n",
    "    grad = x.transpose().dot(prob - y)\n",
    "    # 该分块的样本量\n",
    "    ni = x.shape[0]\n",
    "    return ni, obj, grad\n",
    "\n",
    "def logistic_obj_grad(beta, *args):\n",
    "    dat = args[0]\n",
    "    n, objfn, grad = dat.map(lambda part: compute_obj_grad(part, beta)).\\\n",
    "        reduce(lambda x, y: (x[0] + y[0], x[1] + y[1], x[2] + y[2]))\n",
    "    objfn /= n\n",
    "    grad /= n\n",
    "    return objfn, grad\n",
    "\n",
    "res = minimize(logistic_obj_grad, #l-bfgs需要提供目标函数和梯度值 \\\n",
    "    beta_init, #要优化变量的初始值 \\ \n",
    "    args=(dat,), #QUESTION：这里的dat一定是y和x拼接成的吗？函数会自动判别吗？\\\n",
    "    method=\"L-BFGS-B\", \\\n",
    "    jac=True, \\\n",
    "    options={\"iprint\": 1})\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1998e599",
   "metadata": {},
   "source": [
    "### 10.2 数值稳定算法"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e2272400",
   "metadata": {},
   "source": [
    "#### 问题1：Sigmoid函数"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "276ecc45",
   "metadata": {},
   "source": [
    "**问题分析**\n",
    "\n",
    "Sigmoid:\n",
    "$$\\rho(x) = \\frac{1}{1+e^{-x}}$$\n",
    "\n",
    "当$x$很小时，$\\exp(x)$很大，造成数值溢出\n",
    "\n",
    "**问题处理**\n",
    "\n",
    "解法1：\n",
    "\n",
    "$ x\\ge 0,\\quad sigmoid(x)=\\frac{1}{1+e^{-x}}$\n",
    "\n",
    "$ x<0,\\quad sigmoid(x)=\\frac{e^x}{1+e^x}$\n",
    "\n",
    "解法2：\n",
    "\n",
    "直接调用：`scipy.special.expit()`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e65f103d",
   "metadata": {},
   "source": [
    "#### 问题2：Softplus函数"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1df5e29a",
   "metadata": {},
   "source": [
    "**问题分析**\n",
    "\n",
    "在计算obj_fn的时候，需要计算：\n",
    "\n",
    "$$\\sum[ y_i \\log p_i + ( 1 - y_i) \\log (1 - p_i)]$$\n",
    "\n",
    "即\n",
    "\n",
    "$\\log(\\rho(x)) = \\log(\\frac{1}{1+e^{-x}}) = x-\\log(1+e^x)$\n",
    "\n",
    "$\\log(1-\\rho(x)) = -\\log(1+e^x)$\n",
    "\n",
    "因此核心就在于计算\n",
    "\n",
    "$$\\operatorname{softplus} = \\log(1+e^x)$$\n",
    "\n",
    "依然是在取指数的时候出现问题，故分类讨论：\n",
    "\n",
    "$x\\ge0, \\operatorname{softplus}=x+\\log(1+e^{-x})$\n",
    "\n",
    "$x<0, \\operatorname{softplus}=\\log(1+e^x)$\n",
    "\n",
    "因此对数部分可以统一为：\n",
    "\n",
    "$$\\log(1+e^{-|x|})$$\n",
    "\n",
    "**问题处理**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7527c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softplus(x):\n",
    "    log_num = np.log(1+np.exp(-np.abs(x)))\n",
    "    ans = np.where(x>=0, x+log_num, log_num)\n",
    "    return ans"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a3e48448",
   "metadata": {},
   "source": [
    "### 10.3 RDD缓存"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "15cf009b",
   "metadata": {},
   "source": [
    "**缓存机制**\n",
    "\n",
    "在利用 Spark 对 RDD 进行操作时，通常会叠加很多次变换（ map 、 filter 等等）。理论上每次取数据时都要重复整个流程。为了避免重复操作，可以将中间的某些结果进行缓存；即将变换后的计算结果存进内存，下次需要数据时直接从内存调取。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3e42be",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = sc.textFile(\"data/logistic.txt\")\n",
    "dat = file.mapPartitions(part_to_mat).filter(lambda x: x.shape[0] > 0)\n",
    "\n",
    "##### RDD 缓存 #####\n",
    "dat.cache()\n",
    "####################"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cdd99243",
   "metadata": {},
   "source": [
    "**增加内存上限**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fb7924",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "# 本地模式\n",
    "spark = SparkSession.builder.\\\n",
    "    master(\"local[*]\").\\\n",
    "    config(\"spark.executor.memory\", \"2g\").\\\n",
    "    config(\"spark.driver.memory\", \"2g\").\\\n",
    "    appName(\"Logistic Regression\").\\\n",
    "    getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "# sc.setLogLevel(\"ERROR\")\n",
    "print(spark)\n",
    "print(sc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1235762b",
   "metadata": {},
   "source": [
    "### 10.4 RDD混洗"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ee7658a3",
   "metadata": {},
   "source": [
    "**Shuffling概念**\n",
    "\n",
    "- `repartition()`会触发shuffle\n",
    "- 如果想要减少分区数，可以调用`coalesce()`避免shuffle\n",
    "- 若需要增加分区数，则混洗不可避免，此时要注意问题是否具有排列不变性（行打乱是否有影响）\n",
    "  - 不依赖顺序的算法：排列不变性，通常是一些汇总计算例如计算方差、回归系数、似然函数等\n",
    "  - 依赖排序等算法：通常是与观测相关的计算，例如观测的预测值等\n",
    "- 若无法避免shuffling，但也是排序依赖的，可以用`rdd.zipWithIndex()`来加入索引id，后续对齐"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4ed27b48",
   "metadata": {},
   "source": [
    "## Lect11：ADMM理论（以LAD，Lasso为例的理论推导,soft_thresholding）"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b8a6d95a",
   "metadata": {},
   "source": [
    "### 11.1 ADMM的理论"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c4d52530",
   "metadata": {},
   "source": [
    "**优化目标：**\n",
    "\n",
    "$$\\min_{x,z} f(x)+g(z) \\\\s.t. Ax+Bz = c$$\n",
    "\n",
    "其中 $A_{p\\times n},x_{n\\times 1},B_{p\\times m},z_{m\\times 1},c_{p\\times 1}$ （p可以认为是约束的个数）； $f,g$为凸函数\n",
    "\n",
    "**ADMM实现过程**\n",
    "\n",
    "\n",
    "1. 首先引入增广函数:\n",
    "\n",
    "$$L_\\rho(x,z,y) = f(x)+g(z)+y'(Ax+bz-c)\\\\+(\\rho/2) ||Ax+Bz-c||_2^2$$\n",
    "\n",
    "其中$\\rho$为任意给定正数，$y_{p\\times 1}$为辅助变量\n",
    "\n",
    "\n",
    "2. 进行迭代\n",
    "\n",
    "$$x^{k+1} = \\argmin_x L_\\rho(x,z^k,y^k) \\\\ z^{k+1} = \\argmin_z L_\\rho(x^{k+1},z,y^k) \\\\ y^{k+1} = y^k + \\rho(Ax^{k+1}+Bz^{k+1}-c)$$\n",
    "\n",
    "3. 设置停止条件\n",
    "\n",
    "对偶问题残差：$s^{k+1} = \\rho A'B(z^{k+1} - z^k)$\n",
    "\n",
    "原问题残差：$r^{k+1} = Ax^{k+1}+Bz^{k+1}-c$\n",
    "\n",
    "当两个残差的范数在某次迭代后都小于某个阈值时，停止迭代。\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bb6f4eb4",
   "metadata": {},
   "source": [
    "### 11.2 ADMM例1：LAD（理论）"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "869d1d33",
   "metadata": {},
   "source": [
    "**LAD概念**\n",
    "\n",
    "- 对异常值稳健\n",
    "- 损失函数非光滑但为凸函数\n",
    "\n",
    "**优化目标：**\n",
    "\n",
    "$$ \\min _x \\left\\| Ax-b \\right\\| _1 $$\n",
    "\n",
    "**ADMM对应**\n",
    "\n",
    "令$f(\\cdot)=0,g(\\cdot)=\\left\\| \\cdot \\right\\|， B=-I, c=b$，则\n",
    "\n",
    "$$\\min g(z)\\\\s.t. Ax-z=b$$\n",
    "\n",
    "**迭代过程**\n",
    "\n",
    "$$x^{k+1} := (A'A)^{-1}A'(b+z^k-u^k) \\\\ z^{k+1} := S_{1/\\rho}(Ax^{k+1}-b+u^k) \\\\ u^{k+1} := u^k + Ax^{k+1} - z^{k+1} - b$$\n",
    "其中$S_{\\kappa}(a)$ - Soft-thresholding operator:\n",
    "\n",
    "$$S_{\\kappa}(a) = \\begin{cases} a-\\kappa, & a>\\kappa \\\\ 0, & -\\kappa\\le a\\le \\kappa \\\\ a+\\kappa, & a<-\\kappa \\end{cases}$$\n",
    "\n",
    "一种紧凑的表达是 $S_{\\kappa}(a)=\\mathrm{sign}(a)\\cdot\\max\\{0,|a|-\\kappa\\}$。\n",
    "\n",
    "*上述迭代中的$x$即为所求*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0a8c3f3f",
   "metadata": {},
   "source": [
    "### 11.3 ADMM例2：Lasso（理论）\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7157cff0",
   "metadata": {},
   "source": [
    "\n",
    "**Lasso概念**\n",
    "\n",
    "- 起到变量选择\n",
    "\n",
    "**优化目标：**\n",
    "\n",
    "$$ \\min _x \\frac12 \\left\\| Ax-b \\right\\| _2^2 + \\lambda \\left\\| x \\right\\| _1 $$\n",
    "\n",
    "**ADMM对应**\n",
    "\n",
    "令$f(x)=\\frac12 \\left\\| Ax-b \\right\\| _2^2,g(z)=\\lambda \\left\\| z \\right\\| _1$,则\n",
    "\n",
    "$$\\min f(x)+g(z)\\\\s.t. ~ x-z=0$$\n",
    "\n",
    "**迭代过程**\n",
    "\n",
    "$$x^{k+1} := (A'A+\\rho I)^{-1}(A'b+\\rho(z^k-u^k)) \\\\ z^{k+1} := S_{\\lambda/\\rho}(x^{k+1}+u^k) \\\\ u^{k+1} := u^k + x^{k+1} - z^{k+1} $$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c0009f5b",
   "metadata": {},
   "source": [
    "## Lect12：LAD与Lasso的ADMM实现（非RDD）,Cholesky分解"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9827ea5d",
   "metadata": {},
   "source": [
    "### 12.1 Soft-Thresholding 与 Cholesky分解"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f7dd6ebd",
   "metadata": {},
   "source": [
    "#### Soft-Thresholding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b7ab1778",
   "metadata": {},
   "source": [
    "$$\n",
    "S_{\\kappa}(a)=\\begin{cases}\n",
    "a-\\kappa, & a>\\kappa\\\\\n",
    "0, & |a|\\le\\kappa\\\\\n",
    "a+\\kappa, & a<-\\kappa\n",
    "\\end{cases},\n",
    "$$\n",
    "\n",
    "一种紧凑的表达是 $S_{\\kappa}(a)=\\mathrm{sign}(a)\\cdot\\max\\{0,|a|-\\kappa\\}$。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12d5f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_thresholding(a, k):\n",
    "    return np.sign(a) * np.maximum(0.0, np.abs(a) - k)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7a7e6a51",
   "metadata": {},
   "source": [
    "#### Cholesky Decomposition"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a42e9b18",
   "metadata": {},
   "source": [
    "**Cholesky分解理论**\n",
    "\n",
    "当$M$是正定的，定存在分解$M=LL'$，其中$L$是下三角矩阵\n",
    "\n",
    "如果在一个迭代中，需要循环计算$Mx=b$，则可以将解线性方程组的过程分成两步：1. 首先对$M$进行Cholesky分解 2. 解上三角的线性方程组（解这样的方程组的时间复杂度为$O(n^2)$）。\n",
    "\n",
    "此外，Cholesky分解的结果（由于$M$矩阵是不变的），因此只需要在迭代外部计算一次即可，在循环中即可快速的通过解上三角的线性方程组来求解，大大减少了时间复杂度。\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "93732de2",
   "metadata": {},
   "source": [
    "**Cholesky python实现**\n",
    "\n",
    "假设要求解: $M^{-1}v$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698e5e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.linalg import cho_factor, cho_solve\n",
    "\n",
    "# 首先，在循环外可以一次性进行Cholesky分解 \n",
    "c, lower = cho_factor(M) # 注意cho_factor一次会返回两个值，在应用中直接按照(c,lower)合并成一个元组统一处理即可\n",
    "\n",
    "# 在循环内，就可以把原先的np.linalg.solve替换成cho_solve\n",
    "res = cho_solve((c, lower), v)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "422c0943",
   "metadata": {},
   "source": [
    "### 12.2 ADMM-LAD python 实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa09f5bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.19230848 -0.28642899 -0.89053513  2.35251214  0.66217182  0.14198784 -0.43247972 -1.11299057\n",
      " -0.01374415 -0.38485577]\n",
      "[-1.24096967 -0.31294679 -0.84894679  2.37795259  0.65750062  0.21308689 -0.49097031 -1.0815104\n",
      "  0.00480111 -0.36079657]\n"
     ]
    }
   ],
   "source": [
    "# 模拟生成数据\n",
    "import numpy as np\n",
    "np.set_printoptions(linewidth=100)\n",
    "np.random.seed(123)\n",
    "n = 1000\n",
    "p = 10\n",
    "A = np.random.normal(size=(n, p))\n",
    "xtrue = np.random.normal(size=p)\n",
    "b = A.dot(xtrue) + np.random.normal(size=n)\n",
    "\n",
    "# soft-thresholding\n",
    "from scipy.linalg import cho_factor, cho_solve\n",
    "def soft_thresholding(a, k):\n",
    "    return np.sign(a) * np.maximum(0.0, np.abs(a) - k)\n",
    "\n",
    "# 初始化数值\n",
    "x = np.zeros(p)\n",
    "z = np.zeros(n)\n",
    "u = np.zeros(n)\n",
    "rho = 1.0 # rho 理论可以是任意正数\n",
    "max_iter = 10000\n",
    "tol = 0.001\n",
    "\n",
    "# Cholesky 分解中间变量\n",
    "AtA = A.T.dot(A)\n",
    "c, lower = cho_factor(AtA)\n",
    "\n",
    "# ADMM迭代\n",
    "for i in range(max_iter):\n",
    "    # x 更新\n",
    "    xnew = cho_solve((c,lower),A.T.dot(b+z-u))\n",
    "   #xnew = np.linalg.solve(A.T.dot(A), A.T.dot(b + z - u))\n",
    "    # z 更新\n",
    "    Axnew = A.dot(xnew)\n",
    "    znew = soft_thresholding(Axnew - b + u, 1.0 / rho)\n",
    "    # u 更新\n",
    "    unew = u + Axnew - znew - b\n",
    "    # 计算残差大小\n",
    "    resid_r_norm = np.linalg.norm(unew - u)\n",
    "    resid_s_norm = rho * np.linalg.norm(A.T.dot(znew - z))\n",
    "    # 更新 x、z 和 u 的取值\n",
    "    x = xnew\n",
    "    z = znew\n",
    "    u = unew\n",
    "    # 打印残差信息，判断是否收敛\n",
    "    if i % 100 == 0:\n",
    "        continue\n",
    "        #print(f\"Iteration {i}, ||r|| = {resid_r_norm:.6f}, ||s|| = {resid_s_norm:.6f}\")\n",
    "    if resid_r_norm <= tol and resid_s_norm <= tol:\n",
    "        break\n",
    "\n",
    "print(x)\n",
    "print(xtrue)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bbf8c35b",
   "metadata": {},
   "source": [
    "### 12.3 ADMM-Lasso python 实现"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c1d704d9",
   "metadata": {},
   "source": [
    "用 $M\\in\\mathbb{R}^{n\\times p}$ 表示自变量矩阵，$b\\in\\mathbb{R}^n$ 表示因变量向量 （因此$M,b$都是已知的数据），要估计的回归系数为 $x\\in\\mathbb{R}^p$。于是 Lasso 的目标函数为 $$\\frac{1}{2}\\Vert Mx-b\\Vert^2+\\lambda \\Vert x\\Vert_1,$$\n",
    "其迭代公式为\n",
    "$$\n",
    "\\begin{align*}\n",
    "x^{k+1} & =(M'M+\\rho I)^{-1}(M'b+\\rho(z^{k}-u^{k}))\\\\\n",
    "z^{k+1} & =S_{\\lambda/\\rho}(x^{k+1}+u^{k})\\\\\n",
    "u^{k+1} & =u^{k}+x^{k+1}-z^{k+1},\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b3a6ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.11993425e+00 -7.75763650e-01  1.81091384e+00  1.75943298e+00  1.31142956e+00 -4.01742626e-01\n",
      " -6.19590384e-01 -4.85078523e-01  1.00971039e+00  6.85745010e-01 -1.12150786e-03 -2.96475270e-02\n",
      "  2.38870164e-02  3.56066046e-03  2.70006717e-02 -1.08262799e-02  2.68398062e-02 -3.71308977e-02\n",
      "  5.30794866e-03  1.36178951e-02  3.84465382e-02  1.20218888e-02  1.69123155e-02  6.43126083e-02\n",
      " -2.09899563e-02  1.60904388e-02  2.59800827e-03 -1.09607367e-02  8.29531241e-03 -1.18662502e-03]\n",
      "[-1.05417044 -0.78301134  1.82790084  1.7468072   1.3282585  -0.43277314 -0.6686141  -0.47208845\n",
      "  1.05554064  0.67905585  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "# 模拟数据\n",
    "import numpy as np\n",
    "np.set_printoptions(linewidth=100)\n",
    "np.random.seed(123)\n",
    "n = 1000\n",
    "p = 30\n",
    "nz = 10\n",
    "M = np.random.normal(size=(n, p))\n",
    "xtrue = np.random.normal(size=nz)\n",
    "xtrue = np.concatenate((xtrue, np.zeros(p - nz)))# 真实的 x 只有前10个元素非零，其余均为0\n",
    "b = M.dot(xtrue) + np.random.normal(size=n)\n",
    "\n",
    "# soft-thresholding\n",
    "def soft_thresholding(a, k):\n",
    "    return np.sign(a) * np.maximum(0.0, np.abs(a) - k)\n",
    "\n",
    "# 参数及超参数设置\n",
    "rho = 1.0\n",
    "lam = 0.01*n\n",
    "max_iter = 10000\n",
    "kappa = lam / rho\n",
    "tol = 0.01\n",
    "\n",
    "# 迭代元素初始化\n",
    "\n",
    "MtM  = M.transpose().dot(M)\n",
    "Mtb = M.transpose().dot(b)\n",
    "I = np.eye(p)\n",
    "c, lower = cho_factor(MtM+rho*I)\n",
    "\n",
    "x = np.zeros(p)\n",
    "z = np.zeros(p)\n",
    "u = np.zeros(p)\n",
    "\n",
    "resid_r = -999\n",
    "resid_s = -999\n",
    "\n",
    "for iter in range(max_iter):\n",
    "    xnew = cho_solve((c,lower),Mtb+rho*(z-u))\n",
    "    znew = soft_thresholding(xnew+u,kappa)\n",
    "    unew = u + xnew - znew\n",
    "    \n",
    "    resid_r = np.linalg.norm(xnew-znew)\n",
    "    resid_s = - rho*np.linalg.norm(znew-z)\n",
    "\n",
    "    x = xnew\n",
    "    z = znew\n",
    "    u = unew\n",
    "\n",
    "    # 打印残差信息，判断是否收敛\n",
    "    if iter % 200 == 0:\n",
    "        #print(f\"Iteration {iter}, ||r|| = {resid_r:.6f}, ||s|| = {resid_s:.6f}\")\n",
    "        continue\n",
    "    if resid_r <= tol and resid_s <= tol:\n",
    "        #print(f\"Iteration {iter}, ||r|| = {resid_r:.6f}, ||s|| = {resid_s:.6f}\")\n",
    "        break\n",
    "\n",
    "print(x)\n",
    "print(xtrue)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
