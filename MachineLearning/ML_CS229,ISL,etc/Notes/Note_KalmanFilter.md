# Kalman Filtering

## 简介

### 模型背景

假设有一组数据 $\{X_1, X_2, \ldots, X_n\}$，我们想要估计某一个未知的参数 $\Theta$. 这个参数是未知的, 但是我们假设其在某种意义上取决于数据的分布. 

一般而言, 我们对于这个参数 $\Theta$ 的认知是通过一个系统 $\mathcal{H}$ 来实现的 (可以认为是一个广义的函数). 这个系统 $\mathcal{H}$ 既可能是线性的, 也可能是非线性的. 我们认为这个系统会输入数据 $X(n)$, 并得到输出 $Y(n)$:
$$
X(n) \xrightarrow{\mathcal{H}} Y(n)
$$
我们希望通过对于输入和输出的整理来获得对于系统 $\mathcal{H}$ 的深入的认知. 


### 前 Kalman Filtering 的黑箱系统

在 Kalman Filter 之前, 我们是以一个黑箱的方式来处理这个系统 $\mathcal{H}$. 我们并不知道其中的具体的细节, 只知道输入$X(n)$ 和输出 $Y(n)$. 一般而言, 我们假定输出 $Y(n)$ 是输出 $X(n)$ 的一个函数:
$$
Y(n) = f(X(n), \Theta) 
$$
甚至进一步认为是一个线性函数:
$$
Y(n) = \Theta^\top X(n)
$$
而在这个假设下, 我们可以通过最小二乘法来估计参数 $\Theta$. 这也被称为 Wiener Filter.

### 黑箱系统的局限性与 Kalman Filtering 的核心思想


但是这种黑箱的假设明显过于简单. 
- 事实上在现实生活中, 我们对于这个系统 $\mathcal{H}$ 也并不是完全无知的. 我们可能会有一些来源于其他经验的先验知识. 因此我们希望能够把这些先验知识也加入到我们的模型中 (而不是单纯的一个线性回归). 
- 此外, 并且黑箱的设定还会假设这个系统是不变的. 但是在现实生活中, 系统往往是会随着时间的推移而变化的 (运动是绝对的). 
  
因此我们需要一个动态的模型来描述这个系统 $\mathcal{H}$. 对于任何一个系统, 一定会有内部的发展演化 (Inner Evolution). 而从这种内部演化会部分地反映到人类冰山一角之观测上 (Observation / Measurement). 我们对于这种演化的观测能力是有限的. 这也是人类认知能力的体现与局限. 我们只能通过有限的观测来推测这个系统的演化. 

Kalman Filter 的核心思想也就是在于: **我们能否在有限的观测下, 通过对系统内部演化的建模, 来推测系统的演化?**

### Kalman Filtering 的建模

因此为了对于系统 $\mathcal{H}$ 进行建模, 我们引入了两个概念: State Space (状态空间) 和 Observation Space (观测空间). 对应地, 我们可以用两个方程来描述系统的演化:
$$
\begin{aligned}
&\text{State:} \quad &X_n &= f(X_{n-1}, V_n) \\
&\text{Observation:} \quad &Y_n &= g(X_n, W_n)
\end{aligned}
$$
其中$X_n$ 是系统的状态, $Y_n$ 是观测值, $V_n$ 和 $W_n$ 分别是状态和观测的噪声. 

这样的两个方程就刻画了我们对于系统的动态演化的认知. 
- 我们认为, 每个系统都会有其内部的状态和变化规律. 这些内部状态的维度可能很高, 它们可能是这个系统最本质的刻画 (即 *State*). 
- 但是我们能够看到、真正观测到的, 只是这个系统的状态的某种投影 / 某一方面 / 某个冰山一角 (即 *Observation*). 

这就是我们所说的状态空间和观测空间. 我们希望在有限的观测下, 形成对于状态的认知, 以更好地理解系统的演化. 将状态和观测分离开再结合起来, 是最大的进步. 

此外, 我们对于状态噪声 $V_n$ 和 观测噪声 $W_n$ 的引入也是突破性的. 状态噪声描绘了发展的不确定性, 而观测噪声则描绘了观测的误差. 这两者的引入使得我们对于系统的演化有了更深刻的理解.

## Kalman Filtering 的数学模型

事实上, 在 Kalman Filtering 中, 我们不需要对平稳性(stationarity) 进行假设. 因此我们可以让这个系统是一个时变的 (即 $f$ 和 $g$ 是时变的, 根据每个时刻 $n$ 来变化):
$$
\begin{aligned}
&\text{State:} \quad &X_n &= f_n(X_{n-1}, V_n) \\
&\text{Observation:} \quad &Y_n &= g_n(X_n, W_n)
\end{aligned}
$$

进一步, 我们依然选择线性化来简化模型 (这样的处理也是合理的. 这在某种意义上相当于 Taylor 展开的一阶近似):
$$
\begin{aligned}
&\text{State:} \quad &X_n &= F_n X_{n-1} + V_n \\
&\text{Observation:} \quad &Y_n &= G_n X_n + W_n
\end{aligned}
$$
其中, $\{X_k\}\in\mathbb{R}^m$ 是系统的状态, $\{Y_k\}\in\mathbb{R}^d$ 是观测值. 二者的维度不同也是合理的. 并且往往我们会有 $m\gg d$. 对应地, $F_n\in\mathbb{R}^{m\times m}$ 和 $G_n\in\mathbb{R}^{d\times m}$ 分别是状态转移矩阵和观测矩阵. 进一步假设噪声 $V_n$ 和 $W_n$ 零均值的 White Noise.

---

对于 Kalman Filtering, 我们希望能够从观测值 $Y_n$ 中来推测系统的状态 $X_n$. 并且当我们如果已经完成了$n-1$时刻的估计, 我们希望 recursively (递归) 地来完成 $n$ 时刻的估计. 具体地, 如同两条腿走路一般, 我们可以分为两个步骤:
1. **预测 (Prediction)**: 通过 $\{Y_1, Y_2, \ldots, Y_{n-1}\}$ 的观测来预测 $\{X_1, X_2, \ldots, X_{n}\}$的状态. 
2. **矫正 (Correction)**: 当我们获得了 $Y_n$ 的观测后, 我们希望能够根据 $\{Y_1, Y_2, \ldots, Y_{n}\}$ 来更新我们对于 $\{X_1, X_2, \ldots, X_{n}\}$ 的估计.

这种两步走的策略体现的是 ***Prediction-Correction*** 的思想. 而这种思想本质上就是**Tracking (跟踪)**