{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Decision Boundary / Separating Hyperplane"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**回忆Logistics Regression**\n",
    "\n",
    "logistics的拟合函数是：$h_\\theta(x) = g(\\theta^Tx)$.\n",
    "\n",
    "若以正常的阈值$p=0.5$作为分界来看，$\\theta^Tx>0$与否便构成了一个决策边界，或称separating hyperplane. 数据点距离该决策平面越远，则判断其属于某一类别的confidence越高."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 绘制sigmoid函数图像\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    " \n",
    "z = np.arange(-10, 10, 0.1)\n",
    "phi_z = sigmoid(z)\n",
    "\n",
    "plt.plot(z, phi_z)\n",
    "plt.axvline(0.0, color='k') # 在x=0处画一条垂直的线\n",
    "plt.axhspan(0.0, 1.0, facecolor='1.0', edgecolor='k', ls='dashed') # 在y=0和y=1处画一条水平的虚线\n",
    "plt.axhline(y=0.5, ls='dashed', color='k') # 在y=0.5处画一条水平的虚线\n",
    "plt.yticks([0.0, 0.5, 1.0]) # 设置y轴的刻度\n",
    "plt.ylim(-0.1, 1.1) # 设置y轴的范围\n",
    "plt.xlabel('z')\n",
    "plt.ylabel('sigmoid (z)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Some Notations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在本小节，我们将主要讨论一个**二元分类问题**，采用**linear classifier**. \n",
    "\n",
    "记标签为$y\\in \\{-1,1\\}$，相关特征为$x$，模型参数为$\\omega, \\beta$，构建的拟合模型为:\n",
    "$$ h_{\\omega, b}(x) = g(\\omega^Tx + b)$$\n",
    "其中$g(z) = \\text{sign}(z)$，即$g(z) = 1$ if $z\\geq 0$，$g(z) = -1$ if $z<0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Functional & Geometic Margins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Functional Margin $\\gamma$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functional Margin 作用 ：\n",
    "\n",
    "度量预测的*正确性*与*置信度*。具体而言，Func Margin为正说明预测正确，其绝对值越大说明预测越置信。\n",
    "\n",
    "> 注：其实由下面的构造形式可知，正的Func Margin意味着$y^{(i)}$和$\\omega^Tx^{(i)}$同号，即预测正确。\n",
    "\n",
    "#### Functional Margin 定义公式：\n",
    "  \n",
    "  $$\\hat{\\gamma}^{(i)} = y^{(i)}(\\omega^Tx^{(i)} + b)$$\n",
    "\n",
    "#### Functional Margin 存在的问题\n",
    "\n",
    "缺少normalization。同倍改变$\\omega$和$b$，Func Margin会变大，但并不会改变实际的决策边界提高预测效果。\n",
    "\n",
    "#### 训练集的Functional Margin\n",
    "\n",
    "给定训练集$S = \\{(x^{(i)}, y^{(i)}); i = 1, ..., m\\}$，则训练集的Func Margin（记为$\\hat\\gamma$）为所有样本的Func Margin的最小值，即\n",
    "\n",
    "$$\\hat\\gamma = \\min_{i=1, ..., m} \\hat\\gamma^{(i)}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Geometric Margin $\\hat\\gamma$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现给出如图所示的决策边界$\\omega^Tx+b=0$：\n",
    "\n",
    "![](https://michael-1313341240.cos.ap-shanghai.myqcloud.com/202308131632970.png)\n",
    "\n",
    "在上图中，我们可以给出如下事实陈述：\n",
    "\n",
    "- 由解析几何知识可知，向量$\\omega$定与超平面$\\omega^Tx+b=0$垂直（正交），即$\\omega$是超平面的法向量\n",
    "- 记$A$为$y^{(i)}=1$的一样本点（其坐标可表示为向量$x^{(i)}$），则A到决策边界的距离为垂线段$AB$，记之为$\\gamma^{(i)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**下面具体求解$\\gamma^{(i)}$**\n",
    "\n",
    "求解思路：我们已知 *(1) AB垂直于超平面; (2) B位于超平面$\\omega^Txb=0$上; (3)$\\omega$是超平面的一个法向量*。 因此我们可以求解B的坐标，通过 **B落在超平面上** 这一事实构造等式求解方程。\n",
    "\n",
    "具体而言，B的坐标等于*A的坐标沿着法向量方向回退$\\gamma^{(i)}$个长度的距离*，因此我们有：\n",
    "$$ \\omega^T(x^{(i)}-\\gamma^{(i)}\\frac{\\omega}{||\\omega||})+b=0$$\n",
    "\n",
    "通过求解上述方程，即可求得：\n",
    "$$\n",
    "\\gamma^{(i)}=\\frac{w^T x^{(i)}+b}{\\|w\\|}=\\left(\\frac{w}{\\|w\\|}\\right)^T x^{(i)}+\\frac{b}{\\|w\\|}\n",
    "$$\n",
    "\n",
    "更一般地，考虑到正负样本的不同，更一般的公式为：\n",
    "$$ \\gamma^{(i)}=y^{(i)}\\left(\\left(\\frac{w}{\\|w\\|}\\right)^T x^{(i)}+\\frac{b}{\\|w\\|}\\right)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**说明：**\n",
    "1. 当$||\\omega|| =1$时，Geometric Margin 等价于 Functional Margin\n",
    "2. 由于进行了正则化，Geo Margin杜绝了Func Margin存在的缩放问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同理，定义训练集（$S = \\{(x^{(i)}, y^{(i)}); i = 1, \\dots, m\\}$）的geometric margin：\n",
    "$$\\gamma = \\min_{i=1,\\dots,m} \\hat{\\gamma}^{(i)}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Optimal Margin Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "朴素地，我们希望得到的分割平面尽可能使得上文的margin $\\gamma$足够大，下面就是要量化这一想法。\n",
    "\n",
    "在本节的讨论中，我们都假设数据集是**线性可分的（linearly separable）**，即存在一些超平面可以将正负样本完全分开。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 朴素的优化问题\n",
    "\n",
    "上述的想法可以概括为如下优化问题：\n",
    "\n",
    "$$ \\begin{align*} \\max_{\\gamma, \\omega, b} \\quad &\\gamma \\\\\n",
    "\\text{s.t. } \\quad &y^{(i)}(\\omega^Tx^{(i)} + b) \\geq \\gamma, \\quad i = 1, \\ldots, m \\\\ \n",
    "&||\\omega|| = 1 \\end{align*} $$\n",
    "\n",
    "即，希望能够最大化$\\gamma$，同时满足所有的训练样本的*func. margin*都至少为$\\gamma$。同时参见5.3的讨论可知，$||\\omega|| = 1$使得* geo. margin *等价于* func. margin*。\n",
    "\n",
    "然而上述内容是***非凸优化*** 的，因此下给出一个等价的凸优化问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 改进的优化问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "归根到底我们希望得到的是Geo Margin最大的分类器，因此通过下列方式去掉非凸部分$||\\omega||=1$：\n",
    "\n",
    "$$ \\begin{align*} \\max_{\\hat\\gamma, \\omega, b} \\quad &\\frac{\\hat\\gamma}{||\\omega||} \\\\\n",
    "\\text{s.t. } \\quad &y^{(i)}(\\omega^Tx^{(i)} + b) \\geq \\hat\\gamma, \\quad i = 1, \\ldots, m \\end{align*} $$\n",
    "\n",
    "然而这里的目标函数$\\frac{\\hat\\gamma}{||\\omega||}$依然是非凸的(sad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 最终的凸优化问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最终我们得到的凸优化问题如下所示（这里我们先给出具体形式再解释其原理）：\n",
    "$$\n",
    "\\begin{align}\n",
    "\\min_{\\gamma,\\omega,b} \\quad & \\frac{1}{2}||\\omega||^2 \\\\\n",
    "\\text{s.t.} \\quad & y^{(i)}(\\omega^Tx^{(i)}+b) \\geq 1, \\quad i=1,\\cdots,m\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 首先我们可以规定$\\hat\\gamma = 1$，则原始优化问题可以转化为$\\max \\{1/||\\omega||\\}$\n",
    "\n",
    "    这是因为正如前文在介绍func. margin时所说，我们可以通过任意成倍地更改$\\omega, b$的值来改变func. margin的值，而这不会影响到我们的最优化问题的解。\n",
    "\n",
    "- 其次，我们可以将$\\max \\{1/||\\omega||\\}$转化为$\\min \\{||\\omega||^2\\}$，这是显然的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过上述变换最终得到的优化问题是凸优化可解的。这里的优化解即为**optimal margin classifier**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 Lagrange Duality 拉格朗日对偶"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[bilibili: “拉格朗日对偶问题”如何直观理解？“KKT条件” “Slater条件” “凸优化”打包理解](https://www.bilibili.com/video/BV1HP4y1Y79e/?share_source=copy_web&vd_source=9471c7cd3fca9ffedd9167aefed57c6d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intro: Lagrange Multiplier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "考虑如下形式的优化问题：\n",
    "$$\\begin{aligned}\n",
    "\\min_{w} &\\quad f(w) \\\\\n",
    "\\text{s.t.} &\\quad h_i(w) = 0, \\quad i = 1, \\ldots, l \n",
    "\\end{aligned}$$\n",
    "\n",
    "上述的优化问题可以通过**拉格朗日乘子法**来求解。\n",
    "\n",
    "\n",
    "具体地，我们可以得到*Lagarangian*：\n",
    "$$ \\mathcal{L(w,\\beta)}= f(w) + \\sum_{i=1}^l \\beta_i h_i (w)$$\n",
    "其中一系列引入的参数$\\beta_i$称为拉格朗日乘子 *(Lagrange multipliers)* .\n",
    "\n",
    "通过求解下列方程组求解Lagrange Mult.：\n",
    "$$ \\partial\\mathcal{L}/\\partial w = 0; \\quad \\partial\\mathcal{L}/\\partial \\beta_i = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Primal Optimization Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里我们需要把上面的Lagrange Mult进行推广，考虑**约束条件中存在不等式**的情况（我们称这种优化问题为 ***primal optimization problem（原始问题）*** 。用数学语言表示为：\n",
    "$$ \\begin{align}\n",
    "\\min_{w} \\quad & f(w) \\\\\n",
    "\\text{s.t.} \\quad & g_i(w) \\leq 0, \\quad i = 1, \\cdots, k\n",
    "\\\\& h_i(w) = 0, \\quad i = 1, \\cdots, l\n",
    "\\end{align} $$\n",
    "\n",
    "$\\diamond$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因此引入***Generalized Lagrangian***，即\n",
    "$$\n",
    "\\mathcal{L}(w,\\alpha,\\beta) = f(w) + \\sum_{i=1}^k \\alpha_i g_i(w) + \\sum_{i=1}^l\\beta_i h_i(w)\n",
    "$$\n",
    "\n",
    "其中$\\alpha_i$和$\\beta_i$是拉格朗日乘子.\n",
    "\n",
    "$\\diamond$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**另外定义$\\theta_{\\mathcal{P}}$** *(primal)*\n",
    "$$ \\theta_{\\mathcal{P}}(w) = \\max_{\\alpha,\\beta \\textit{ s.t. } \\alpha_i\\ge 0} \\mathcal{L(w,\\alpha,\\beta)} \\quad\\dagger$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以证明,当存在违反约束（式(2)/(3)）中的条件时，$\\theta_{\\mathcal{P}} \\to \\infty$ （即令 $\\mathcal{L}$ 中违反条件的项前的系数$\\alpha / \\beta \\to \\infty$即可）；相反地，若全部满足约束，则$\\theta_{\\mathcal{P}} = f(w)$，即：\n",
    "$$\n",
    "\\theta_{\\mathcal{P}}(w)= \\begin{cases}f(w) & \\text { if } w \\text { satisfies primal constraints } \\\\ \\infty & \\text { otherwise. }\\end{cases}\n",
    "$$\n",
    "\n",
    "$\\diamond$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因此，通过$\\theta_{\\mathcal{P}}$，我们就可以将*primal problem*的（1）～（3）式加以概括。\n",
    "\n",
    "式（1）描述的问题是：$\\min_{w}f(w)$，在满足约束时，就等价于$\\min_w\\theta_{\\mathcal{P}}(w,b)$，再代入式$(\\dagger)$有：\n",
    "$$\\min_w\\theta_{\\mathcal{P}}(w,b)=\\min_w\\max_{\\alpha,\\beta: \\alpha_i\\geq0} \\mathcal{L}(w,\\alpha,\\beta)\\quad \\star$$\n",
    "\n",
    "而这一优化问题与primal problem是完全等价的，且具有相同的解。为方便起见，我们记$p^* := \\min_w\\theta_{\\mathcal{P}}(w)$，称$p^*$为 ***primal optimal value***。\n",
    "\n",
    "$\\diamond$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dual Optimization Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **理解：** 对偶是实质相同但从不同角度提出不同提法的一对问题。有时候原问题 (Primal Problem) 不太好解，但是对偶问题 (Dual Problem) 却很好解，我们就可以通过求解对偶问题来迂回地解答原问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "类似地**定义Lagrange 对偶函数$\\theta_{\\mathcal{D}}$** *(dual)*：\n",
    "$$\\theta_{\\mathcal{D}}(\\alpha,\\beta) = \\min_{w} \\mathcal{L}(w,\\alpha,\\beta) ,\\alpha\\ge0    \\quad \\dagger^2$$ \n",
    "\n",
    "*注意，$\\theta_{\\mathcal{D}}$和$\\theta_{\\mathcal{P}}$的区别在于：$\\theta_{\\mathcal{D}}$是关于$\\alpha$和$\\beta$的优化函数，而$\\theta_{\\mathcal{P}}$是关于$w$的优化函数。*\n",
    "\n",
    "另外可以证明，无论原问题函数$\\theta_{\\mathcal{P}}$的凹凸性，恒有对偶函数$\\theta_{\\mathcal{D}}$是一个凹函数.\n",
    "\n",
    "> 凹函数：$ f(\\theta x_1 + (1-\\theta)x_2) \\ge \\theta f(x_1) + (1-\\theta)f(x_2) $\n",
    "\n",
    "$\\diamond$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "顺势，我们引入***dual optimization problem***:\n",
    "$$ \\max \\limits_{\\alpha,\\beta:\\alpha_i\\ge0} \\theta_{\\mathcal{D}}(\\alpha,\\beta) = \\max\\limits_{\\alpha,\\beta:\\alpha_i\\ge0} \\min_w \\mathcal{L}(w,\\alpha,\\beta) \\quad \\star\\star$$\n",
    "\n",
    "并同理记之为$d^*$。\n",
    "\n",
    "注意到，$p^*$与$d^*$的唯一区别是二者$\\min,\\max$的顺序不同。\n",
    "\n",
    "\n",
    "$\\diamond$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 强、弱对偶条件\n",
    "\n",
    "#### a. 弱对偶\n",
    "\n",
    "在任何条件下，可以证明下列不等关系是恒成立的：\n",
    "$$d^* = \\max_{\\alpha,\\beta:\\alpha_i\\ge0} \\min_w L(w,\\alpha,\\beta) \\le \\min_w \\max_{\\alpha,\\beta:\\alpha_i\\ge0} L(w,\\alpha,\\beta) = p^*$$\n",
    "\n",
    "记$p^*-d^*$为duality gap.\n",
    "\n",
    "#### b. 强对偶\n",
    "\n",
    "而在一些*特定条件*下，该不等式取等，即\n",
    "$$d^* = p^*$$\n",
    "在这些条件下，我们就可以通过求解对偶问题来求解原始问题。\n",
    "\n",
    "##### b-1 强对偶条件（1）: Slater's condition （充要条件）\n",
    "\n",
    "- $f, g_i$是凸函数\n",
    "- $h_i$是仿射函数\n",
    "  - 仿射函数: $h_i(w) = a_i^Tw + b_i$\n",
    "- $g_i$是strictly feasible的\n",
    "  - strictly feasible: $\\exist w, \\textit{ s.t. } \\forall i, g_i(w) < 0$\n",
    "\n",
    "此时，则$\\exist w^*, \\alpha^*, \\beta^*$，使得 $w^*$是primal problem的解，$\\alpha^*, \\beta^*$是dual problem的解，且$p^* = d^*=\\mathcal{L}(w^*,\\alpha^*,\\beta^*)$。\n",
    "\n",
    "##### b-2 强对偶条件（2）: KKT条件（充要条件）必要条件\n",
    "\n",
    "此外，还有KKT条件，即\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial}{\\partial w_i} \\mathcal{L}\\left(w^*, \\alpha^*, \\beta^*\\right) & =0, \\quad i=1, \\ldots, n \\\\\n",
    "\\frac{\\partial}{\\partial \\beta_i} \\mathcal{L}\\left(w^*, \\alpha^*, \\beta^*\\right) & =0, \\quad i=1, \\ldots, l \\\\\n",
    "\\alpha_i^* g_i\\left(w^*\\right) & =0, \\quad i=1, \\ldots, k \\\\\n",
    "g_i\\left(w^*\\right) & \\leq 0, \\quad i=1, \\ldots, k \\\\\n",
    "\\alpha^* & \\geq 0, \\quad i=1, \\ldots, k\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "其中，称等式（3）为***dual complementarity***条件，其含义为：若$\\alpha_i^*>0$，则$g_i(w^*)=0$. \n",
    "\n",
    "通过后面的描述可见，SVM只需要少量的*support vectors*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.6 Optimal Margin Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回忆，在*Optimal Margin Classifiers*中，我们考虑如下优化问题：\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\min_{\\gamma, w, b} \\quad & \\frac{1}{2} \\|w\\|^2 \\\\\n",
    "\\text{s.t.} \\quad & g_i(w) = -y^{(i)}(w^Tx^{(i)} + b) + 1 \\leq 0, \\quad i = 1, \\ldots, m\n",
    "\\end{aligned}$$\n",
    "\n",
    "由上文KKT条件中的 *(3)dual complementarity* 条件可知，只有那些满足 $g_i(w) = 0$ 的样本才会对最优解有贡献（贡献体现在$a_i>0$）。而这里的 $g_i(w)=-y^{(i)}(w^Tx^{(i)} + b) + 1$表示恰好落在margin上的样本，即支持向量。\n",
    "\n",
    "具体参见下图：\n",
    "\n",
    "![](https://michael-1313341240.cos.ap-shanghai.myqcloud.com/202308151603962.png)\n",
    "\n",
    "图中实线表示的是决策边界，虚线表示的是margin。我们可以看到，只有在虚线上的一个x和两个o样本是对边际有贡献的，称为***support vectors***\n",
    "\n",
    "$\\diamond$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们由此往下，尝试通过对偶问题解决原问题。\n",
    "\n",
    "首先引入向量内积符号：$ \\langle x, y \\rangle = x^T y $\n",
    "\n",
    "我们目前需要解决的optimal margin classifier的优化问题是：\n",
    "$$ \\begin{align*}\n",
    "\\mathcal{L}(w,b,\\alpha) = \\frac12 ||w||^2 - \\sum_{i=1}^m \\alpha_i \\left( y^{(i)} (w^T x^{(i)} + b) - 1 \\right)\n",
    "\\end{align*} \\quad [\\dagger] $$\n",
    "\n",
    "回忆：\n",
    "$$\\theta_{\\mathcal{D}}(\\alpha,\\beta) = \\min_{w} \\mathcal{L}(w,\\alpha,\\beta) ,\\alpha\\ge0 $$ \n",
    "\n",
    "因此我们分别对$w$和$b$求偏导，令其为0，得到：\n",
    "$$ \\begin{align}\n",
    "\\nabla_w \\mathcal{L}(w,b,\\alpha) &= w - \\sum_{i=1}^m \\alpha_i y^{(i)} x^{(i)} = 0 \\\\\n",
    "\\nabla_b \\mathcal{L}(w,b,\\alpha) &= \\sum_{i=1}^m \\alpha_i y^{(i)} = 0\n",
    "\\end{align} $$\n",
    "\n",
    "由(1) 解得：\n",
    "$$w = \\sum_{i=1}^m \\alpha_i y^{(i)} x^{(i)} \\quad [\\ast]$$\n",
    "\n",
    "直观的理解，这里的含义是指参数$w$一定是原始的训练集的线性组合。一个直觉的理解可以回忆线性回归中$\\hat\\beta = (X^T X)^{-1} X^T y$\n",
    "\n",
    "将这里的$w$代入$[\\dagger]$并化简，得到\n",
    "$$\\begin{align*}\n",
    "\\mathcal{L}(w,b,\\alpha) &= \\sum_{i=1}^m \\alpha_i - \\frac12 \\sum_{i,j=1}^m \\alpha_i \\alpha_j y^{(i)} y^{(j)} \\langle x^{(i)}, x^{(j)} \\rangle - b\\sum_{i=1}^m \\alpha_i y^{(i)} \\\\ \n",
    "&= \\sum_{i=1}^m \\alpha_i - \\frac12 \\sum_{i,j=1}^m \\alpha_i \\alpha_j y^{(i)} y^{(j)} \\langle x^{(i)}, x^{(j)} \\rangle  \\textit{ (by eqn. (2)) } \\\\\n",
    "\\end{align*}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最终可以得到如下的对偶问题 **(Dual Optimizatoin Problem of SVM)**：\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\max_{\\alpha} \\quad & W(\\alpha) = \\sum_{i=1}^m \\alpha_i - \\frac{1}{2} \\sum_{i,j=1}^m y^{(i)} y^{(j)} \\alpha_i \\alpha_j \\langle x^{(i)}, x^{(j)} \\rangle \\\\\n",
    "\\text{s.t.} \\quad & \\alpha_i \\geq 0, \\quad i = 1, \\ldots, m \\\\\n",
    "& \\sum_{i=1}^m \\alpha_i y^{(i)} = 0\n",
    "\\end{align*}$$\n",
    "\n",
    "可验证，本对偶问题符合KKT条件。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**预测过程**\n",
    "1. 求解 $a_i, b$\n",
    "2. 预测$h_{w,b}(x)=g(w^Tx+b) = g((\\sum_i a_i y^{(i)} x^{(i)})^Tx+b)=g(\\sum_i\\alpha_iy^{(i)}\\langle x^{(i)},x\\rangle+b)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.7 Kernels 核方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel Methods 思路\n",
    "\n",
    "**(1) 将算法写作含$\\langle x^{(i)},x^{(j)}\\rangle \\textit{( or note as } \\langle x,z\\rangle \\textit{) }$的形式**\n",
    "\n",
    "**(2) 映射 $x_\\textit{ (low dim)} \\rightarrow \\phi(x) _\\textit{ (high dim)}$**\n",
    "\n",
    "**(3) 寻找容易可行的计算$K(x,z) = \\phi(x)^T\\phi(z)$的方法**\n",
    "\n",
    "**(4) 讲上述算法中的$\\langle x,z\\rangle$替换为$K(x,z)$**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 例子"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设$x,z\\in\\mathbb{R}^n$\n",
    "\n",
    "在本例中，假设有三个特征$x_1,...,x_3$，以及映射函数：\n",
    "$$ \\phi(x) = [x_1x_1, x_1x_2,x_1x_3,...,x_3x_3]^T $$\n",
    "需要指出，正常直接计算$\\phi(x)$ （或$\\phi(x)^T\\phi(z)$）需要$O(n^2)$的时间\n",
    "\n",
    "$\\diamond$\n",
    "\n",
    "为化简计算，考虑如下核方法：\n",
    "$$ K(x,z) = \\phi(x)^T\\phi(z) =^{?} (x^Tz)^2 = ... = \\sum_{i,j=1}^n (x_ix_j)(z_iz_j) $$\n",
    "这个核方法的计算复杂度为$O(n)$\n",
    "\n",
    "下证明[?]处等式的成立性：\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
