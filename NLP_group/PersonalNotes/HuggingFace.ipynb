{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hugging Face"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Environment Settings 环境配置"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 在conda环境中安装基础python依赖：\n",
    "    ```bash\n",
    "    conda create -n transformer\n",
    "    conda activate transformer\n",
    "    pip install transformer datasets evaluate peft accelerate gradio optimum sentencepiece jupyterlab scikit-learn pandas matplotlib tensorboard nltk rouge\n",
    "    ```\n",
    "\n",
    "- 网络问题更改本地电脑hosts：*本条操作待验证*\n",
    "  - hosts储存路径为: *C:\\Windows\\System32\\drivers\\etc\\hosts*\n",
    "  - 添加如下内容："
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# HuggingFace\n",
    "185.199.108.133 raw.githubusercontent.com\n",
    "185.199.109.133 raw.githubusercontent.com\n",
    "185.199.110.133 raw.githubusercontent.com\n",
    "185.199.111.133 raw.githubusercontent.com\n",
    "2606:50c0:8000::154 raw.githubusercontent.com\n",
    "2606:50c0:8001::154 raw.githubusercontent.com\n",
    "2606:50c0:8002::154 raw.githubusercontent.com\n",
    "2606:50c0:8003::154 raw.githubusercontent.com\n",
    "# End of HuggingFace section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Pipeline 工具"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Pipeline 概念\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Pipeline Official Doc](https://huggingface.co/transformers/main_classes/pipelines.html#transformers.pipeline)\n",
    "\n",
    "- pipeline是一个封装好的工具，可以通过直接的声明进行调用，完成一些预先设定好的NLP任务\n",
    "\n",
    "- pipeline的大致syntax为（较常用的一些opt参数）：`pipeline(task=\"\", model=\"\", tokenizer=\"\", device=)`\n",
    "\n",
    "![20230716105734](https://michael-1313341240.cos.ap-shanghai.myqcloud.com/20230716105734.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Pipeline支持的Task\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- task主要可以包括的任务有：\n",
    "  - 文本分类（包含情感分析）: `task = \"text-classification\"` `task = \"zero-shot-classification\"`\n",
    "  - Token 分类（包含命名实体识别 NER）: `task = \"token-classification\"`\n",
    "  - 人机对话: `task = \"conversational\"`\n",
    "  - 文档问答: `task = \"document-question-answering\"`\n",
    "  - 问题回答: `task = \"question-answering\"`\n",
    "  - 表格回答: `task = \"table-question-answering\"`\n",
    "  - 填空: `task = \"fill-mask\"`\n",
    "  - 生成摘要: `task = \"summarization\"`\n",
    "  - Text-to-text 文本生成: `task = \"text2text-generation\"`\n",
    "  - 机器翻译: `task = \"translation\"`\n",
    "  - 跨模态任务: `task = \"image-to-text\"` `task = \"visual-question-answering\"`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*可以通过下列命令查看 **pipeline 支持的所有任务***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('audio-classification', {'impl': <class 'transformers.pipelines.audio_classification.AudioClassificationPipeline'>, 'tf': (), 'pt': (<class 'transformers.models.auto.modeling_auto.AutoModelForAudioClassification'>,), 'default': {'model': {'pt': ('superb/wav2vec2-base-superb-ks', '372e048')}}, 'type': 'audio'})\n",
      "('automatic-speech-recognition', {'impl': <class 'transformers.pipelines.automatic_speech_recognition.AutomaticSpeechRecognitionPipeline'>, 'tf': (), 'pt': (<class 'transformers.models.auto.modeling_auto.AutoModelForCTC'>, <class 'transformers.models.auto.modeling_auto.AutoModelForSpeechSeq2Seq'>), 'default': {'model': {'pt': ('facebook/wav2vec2-base-960h', '55bb623')}}, 'type': 'multimodal'})\n",
      "('feature-extraction', {'impl': <class 'transformers.pipelines.feature_extraction.FeatureExtractionPipeline'>, 'tf': (), 'pt': (<class 'transformers.models.auto.modeling_auto.AutoModel'>,), 'default': {'model': {'pt': ('distilbert-base-cased', '935ac13'), 'tf': ('distilbert-base-cased', '935ac13')}}, 'type': 'multimodal'})\n",
      "('text-classification', {'impl': <class 'transformers.pipelines.text_classification.TextClassificationPipeline'>, 'tf': (), 'pt': (<class 'transformers.models.auto.modeling_auto.AutoModelForSequenceClassification'>,), 'default': {'model': {'pt': ('distilbert-base-uncased-finetuned-sst-2-english', 'af0f99b'), 'tf': ('distilbert-base-uncased-finetuned-sst-2-english', 'af0f99b')}}, 'type': 'text'})\n",
      "('token-classification', {'impl': <class 'transformers.pipelines.token_classification.TokenClassificationPipeline'>, 'tf': (), 'pt': (<class 'transformers.models.auto.modeling_auto.AutoModelForTokenClassification'>,), 'default': {'model': {'pt': ('dbmdz/bert-large-cased-finetuned-conll03-english', 'f2482bf'), 'tf': ('dbmdz/bert-large-cased-finetuned-conll03-english', 'f2482bf')}}, 'type': 'text'})\n",
      "('question-answering', {'impl': <class 'transformers.pipelines.question_answering.QuestionAnsweringPipeline'>, 'tf': (), 'pt': (<class 'transformers.models.auto.modeling_auto.AutoModelForQuestionAnswering'>,), 'default': {'model': {'pt': ('distilbert-base-cased-distilled-squad', '626af31'), 'tf': ('distilbert-base-cased-distilled-squad', '626af31')}}, 'type': 'text'})\n",
      "('table-question-answering', {'impl': <class 'transformers.pipelines.table_question_answering.TableQuestionAnsweringPipeline'>, 'pt': (<class 'transformers.models.auto.modeling_auto.AutoModelForTableQuestionAnswering'>,), 'tf': (), 'default': {'model': {'pt': ('google/tapas-base-finetuned-wtq', '69ceee2'), 'tf': ('google/tapas-base-finetuned-wtq', '69ceee2')}}, 'type': 'text'})\n",
      "('visual-question-answering', {'impl': <class 'transformers.pipelines.visual_question_answering.VisualQuestionAnsweringPipeline'>, 'pt': (<class 'transformers.models.auto.modeling_auto.AutoModelForVisualQuestionAnswering'>,), 'tf': (), 'default': {'model': {'pt': ('dandelin/vilt-b32-finetuned-vqa', '4355f59')}}, 'type': 'multimodal'})\n",
      "('document-question-answering', {'impl': <class 'transformers.pipelines.document_question_answering.DocumentQuestionAnsweringPipeline'>, 'pt': (<class 'transformers.models.auto.modeling_auto.AutoModelForDocumentQuestionAnswering'>,), 'tf': (), 'default': {'model': {'pt': ('impira/layoutlm-document-qa', '52e01b3')}}, 'type': 'multimodal'})\n",
      "('fill-mask', {'impl': <class 'transformers.pipelines.fill_mask.FillMaskPipeline'>, 'tf': (), 'pt': (<class 'transformers.models.auto.modeling_auto.AutoModelForMaskedLM'>,), 'default': {'model': {'pt': ('distilroberta-base', 'ec58a5b'), 'tf': ('distilroberta-base', 'ec58a5b')}}, 'type': 'text'})\n",
      "('summarization', {'impl': <class 'transformers.pipelines.text2text_generation.SummarizationPipeline'>, 'tf': (), 'pt': (<class 'transformers.models.auto.modeling_auto.AutoModelForSeq2SeqLM'>,), 'default': {'model': {'pt': ('sshleifer/distilbart-cnn-12-6', 'a4f8f3e'), 'tf': ('t5-small', 'd769bba')}}, 'type': 'text'})\n",
      "('translation', {'impl': <class 'transformers.pipelines.text2text_generation.TranslationPipeline'>, 'tf': (), 'pt': (<class 'transformers.models.auto.modeling_auto.AutoModelForSeq2SeqLM'>,), 'default': {('en', 'fr'): {'model': {'pt': ('t5-base', '686f1db'), 'tf': ('t5-base', '686f1db')}}, ('en', 'de'): {'model': {'pt': ('t5-base', '686f1db'), 'tf': ('t5-base', '686f1db')}}, ('en', 'ro'): {'model': {'pt': ('t5-base', '686f1db'), 'tf': ('t5-base', '686f1db')}}}, 'type': 'text'})\n",
      "('text2text-generation', {'impl': <class 'transformers.pipelines.text2text_generation.Text2TextGenerationPipeline'>, 'tf': (), 'pt': (<class 'transformers.models.auto.modeling_auto.AutoModelForSeq2SeqLM'>,), 'default': {'model': {'pt': ('t5-base', '686f1db'), 'tf': ('t5-base', '686f1db')}}, 'type': 'text'})\n",
      "('text-generation', {'impl': <class 'transformers.pipelines.text_generation.TextGenerationPipeline'>, 'tf': (), 'pt': (<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>,), 'default': {'model': {'pt': ('gpt2', '6c0e608'), 'tf': ('gpt2', '6c0e608')}}, 'type': 'text'})\n",
      "('zero-shot-classification', {'impl': <class 'transformers.pipelines.zero_shot_classification.ZeroShotClassificationPipeline'>, 'tf': (), 'pt': (<class 'transformers.models.auto.modeling_auto.AutoModelForSequenceClassification'>,), 'default': {'model': {'pt': ('facebook/bart-large-mnli', 'c626438'), 'tf': ('roberta-large-mnli', '130fb28')}, 'config': {'pt': ('facebook/bart-large-mnli', 'c626438'), 'tf': ('roberta-large-mnli', '130fb28')}}, 'type': 'text'})\n",
      "('zero-shot-image-classification', {'impl': <class 'transformers.pipelines.zero_shot_image_classification.ZeroShotImageClassificationPipeline'>, 'tf': (), 'pt': (<class 'transformers.models.auto.modeling_auto.AutoModelForZeroShotImageClassification'>,), 'default': {'model': {'pt': ('openai/clip-vit-base-patch32', 'f4881ba'), 'tf': ('openai/clip-vit-base-patch32', 'f4881ba')}}, 'type': 'multimodal'})\n",
      "('zero-shot-audio-classification', {'impl': <class 'transformers.pipelines.zero_shot_audio_classification.ZeroShotAudioClassificationPipeline'>, 'tf': (), 'pt': (<class 'transformers.models.auto.modeling_auto.AutoModel'>,), 'default': {'model': {'pt': ('laion/clap-htsat-fused', '973b6e5')}}, 'type': 'multimodal'})\n",
      "('conversational', {'impl': <class 'transformers.pipelines.conversational.ConversationalPipeline'>, 'tf': (), 'pt': (<class 'transformers.models.auto.modeling_auto.AutoModelForSeq2SeqLM'>, <class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>), 'default': {'model': {'pt': ('microsoft/DialoGPT-medium', '8bada3b'), 'tf': ('microsoft/DialoGPT-medium', '8bada3b')}}, 'type': 'text'})\n",
      "('image-classification', {'impl': <class 'transformers.pipelines.image_classification.ImageClassificationPipeline'>, 'tf': (), 'pt': (<class 'transformers.models.auto.modeling_auto.AutoModelForImageClassification'>,), 'default': {'model': {'pt': ('google/vit-base-patch16-224', '5dca96d'), 'tf': ('google/vit-base-patch16-224', '5dca96d')}}, 'type': 'image'})\n",
      "('image-segmentation', {'impl': <class 'transformers.pipelines.image_segmentation.ImageSegmentationPipeline'>, 'tf': (), 'pt': (<class 'transformers.models.auto.modeling_auto.AutoModelForImageSegmentation'>, <class 'transformers.models.auto.modeling_auto.AutoModelForSemanticSegmentation'>), 'default': {'model': {'pt': ('facebook/detr-resnet-50-panoptic', 'fc15262')}}, 'type': 'multimodal'})\n",
      "('image-to-text', {'impl': <class 'transformers.pipelines.image_to_text.ImageToTextPipeline'>, 'tf': (), 'pt': (<class 'transformers.models.auto.modeling_auto.AutoModelForVision2Seq'>,), 'default': {'model': {'pt': ('ydshieh/vit-gpt2-coco-en', '65636df'), 'tf': ('ydshieh/vit-gpt2-coco-en', '65636df')}}, 'type': 'multimodal'})\n",
      "('object-detection', {'impl': <class 'transformers.pipelines.object_detection.ObjectDetectionPipeline'>, 'tf': (), 'pt': (<class 'transformers.models.auto.modeling_auto.AutoModelForObjectDetection'>,), 'default': {'model': {'pt': ('facebook/detr-resnet-50', '2729413')}}, 'type': 'multimodal'})\n",
      "('zero-shot-object-detection', {'impl': <class 'transformers.pipelines.zero_shot_object_detection.ZeroShotObjectDetectionPipeline'>, 'tf': (), 'pt': (<class 'transformers.models.auto.modeling_auto.AutoModelForZeroShotObjectDetection'>,), 'default': {'model': {'pt': ('google/owlvit-base-patch32', '17740e1')}}, 'type': 'multimodal'})\n",
      "('depth-estimation', {'impl': <class 'transformers.pipelines.depth_estimation.DepthEstimationPipeline'>, 'tf': (), 'pt': (<class 'transformers.models.auto.modeling_auto.AutoModelForDepthEstimation'>,), 'default': {'model': {'pt': ('Intel/dpt-large', 'e93beec')}}, 'type': 'image'})\n",
      "('video-classification', {'impl': <class 'transformers.pipelines.video_classification.VideoClassificationPipeline'>, 'tf': (), 'pt': (<class 'transformers.models.auto.modeling_auto.AutoModelForVideoClassification'>,), 'default': {'model': {'pt': ('MCG-NJU/videomae-base-finetuned-kinetics', '4800870')}}, 'type': 'video'})\n",
      "('mask-generation', {'impl': <class 'transformers.pipelines.mask_generation.MaskGenerationPipeline'>, 'tf': (), 'pt': (<class 'transformers.models.auto.modeling_auto.AutoModelForMaskGeneration'>,), 'default': {'model': {'pt': ('facebook/sam-vit-huge', '997b15')}}, 'type': 'multimodal'})\n"
     ]
    }
   ],
   "source": [
    "from transformers.pipelines import SUPPORTED_TASKS\n",
    "print(*SUPPORTED_TASKS.items(),sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Pipeline的创建与使用（例）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 在HuggingFace的[Model](https://huggingface.co/models)分区可以寻找想要用的模型\n",
    "- 点击进入模型详细页面后，可以看到具体描述和demo交互\n",
    "\n",
    "![20230716120431](https://michael-1313341240.cos.ap-shanghai.myqcloud.com/20230716120431.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'financial news', 'score': 0.5913024544715881}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification,AutoTokenizer,pipeline\n",
    "# model是指定这个task要使用的nlp模型\n",
    "model = AutoModelForSequenceClassification.from_pretrained('uer/roberta-base-finetuned-chinanews-chinese')\n",
    "# tokenizer是指定task的分词器，其具体含义会在后面进行学习\n",
    "tokenizer = AutoTokenizer.from_pretrained('uer/roberta-base-finetuned-chinanews-chinese')\n",
    "# 通过pipeline(...)就会生成这样一个封装好的工具，这里命名为pipe\n",
    "## 此处的task为sentiment analysis，即情感分析\n",
    "pipe = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer)\n",
    "# 调用pipe，配合相应的输入，就可以得到nlp任务的相应输出\n",
    "pipe(\"暴雪被微软收购了！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**使用GPU加速计算**\n",
    "\n",
    "通过`pipeline(, ... , device = 0)`显式加载第0张GPU进行加速计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU:0.005329703807830811\n",
      "CPU:0.01029831838607788\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "times = []\n",
    "\n",
    "# GPU inference\n",
    "pipe_gpu = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer,device=0)\n",
    "for i in range(1000):\n",
    "    torch.cuda.synchronize()\n",
    "    start = time.time()\n",
    "    pipe_gpu(\"暴雪被微软收购了！\")\n",
    "    torch.cuda.synchronize()\n",
    "    end = time.time()\n",
    "    times.append(end - start)\n",
    "print(f\"GPU:{sum(times) / 1000}\")\n",
    "\n",
    "# CPU inference\n",
    "for i in range(1000):\n",
    "    torch.cuda.synchronize()\n",
    "    start = time.time()\n",
    "    pipe(\"暴雪被微软收购了！\")\n",
    "    torch.cuda.synchronize()\n",
    "    end = time.time()\n",
    "    times.append(end - start)\n",
    "print(f\"CPU:{sum(times) / 1000}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Pipeline的背后实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. 初始化Tokenizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "tokenizer = AutoTokenizer.from_pretrained\\\n",
    "    (\"uer/roberta-base-finetuned-dianping-chinese\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. 初始化Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87eccdf407c64e51bbcf3d98a755ba3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/409M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained\\\n",
    "    (\"uer/roberta-base-finetuned-dianping-chinese\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForSequenceClassification(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(21128, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 查看模型结构\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. 数据输入&通过`tokenizer`预处理**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_txt = \"我很喜欢你！\"\n",
    "input = tokenizer(input_txt, return_tensors=\"pt\") # pt即pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 101, 2769, 2523, 1599, 3614,  872, 8013,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "# 查看分词后结果\n",
    "print(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. 模型预测**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res:\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.4593,  1.4171]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits:\n",
      "tensor([[-1.4593,  1.4171]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "res = model(**input) # 这一步得到的结果是一个未标准化的logits向量\n",
    "print(f\"res:\\n{res}\") # 输出res，这里列出了一系列中间操作的说明；数值上重点关注logits值\n",
    "logits = res.logits # 从res中提取logits向量\n",
    "print(f\"logits:\\n{logits}\") # 输出logits向量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. 输出结果**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits_softmax:\ttensor([[0.0533, 0.9467]], grad_fn=<SoftmaxBackward0>)\n",
      "pred:\t1\n",
      "result:\tpositive (stars 4 and 5)\n"
     ]
    }
   ],
   "source": [
    "logits_softmax = torch.softmax(logits,dim=-1) # 对logits向量进行softmax归一化操作\n",
    "print(f\"logits_softmax:\\t{logits_softmax}\") # 输出归一化后的logits向量\n",
    "\n",
    "pred = torch.argmax(logits_softmax).item() # 从归一化后的logits向量中提取最大值的索引 \n",
    "print(f\"pred:\\t{pred}\") # 输出预测结果，这里的pred是一个数字索引，需要转换为索引对应的标签\n",
    "\n",
    "result = model.config.id2label.get(pred) # 通过id2label字典，将索引转换为标签\n",
    "print(f\"result:\\t{result}\") # 输出标签"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Tokenizer 概念"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**基本NLP数据预处理流程**\n",
    "\n",
    "1. 分词：将文本数据分词成字、字词等\n",
    "2. 构建词典：构建词典，filter一些过低、过高的单词等\n",
    "3. 数据转换：将词典映射成数字序列\n",
    "4. 数据填充与截断\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Tokenizer的使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "demo = \"那些痛的记忆，落在春的泥土里\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. 加载Tokenizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertTokenizerFast(name_or_path='uer/roberta-base-finetuned-dianping-chinese', vocab_size=21128, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True)\n"
     ]
    }
   ],
   "source": [
    "# 从huggingface的模型库中加载分词器\n",
    "tokenizer = AutoTokenizer.from_pretrained\\\n",
    "    (\"uer/roberta-base-finetuned-dianping-chinese\")\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*也可以将tokenizer保存到本地再从本地调用*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained(\"./roberta_tokenizer\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./roberta_tokenizer\")\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. 利用`tokenizer.tokenize()`进行分词**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['那', '些', '痛', '的', '记', '忆', '，', '落', '在', '春', '的', '泥', '土', '里']\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(demo)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*对于tokenizer词典，可以通过以下方法进行查看：*\n",
    "\n",
    "`tokenizer.vocab()`：查看词典\n",
    "\n",
    "`tokenizer.vocab_size`：查看词典大小"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3\\*. 利用`tokenizer.convert_to_ids()`进行索引转换**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "目的：token -> id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6929, 763, 4578, 4638, 6381, 2554, 8024, 5862, 1762, 3217, 4638, 3799, 1759, 7027]\n"
     ]
    }
   ],
   "source": [
    "# 将词序列转换为id序列\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*相反地，也可以通过`tokenizer.convert_ids_to_tokens(ids)`进行逆变换*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['那', '些', '痛', '的', '记', '忆', '，', '落', '在', '春', '的', '泥', '土', '里']\n",
      "那 些 痛 的 记 忆 ， 落 在 春 的 泥 土 里\n"
     ]
    }
   ],
   "source": [
    "# 将id序列转换为token序列\n",
    "tokens = tokenizer.convert_ids_to_tokens(ids)\n",
    "print(tokens)\n",
    "\n",
    "# 将token序列转换为string\n",
    "str_sen = tokenizer.convert_tokens_to_string(tokens)\n",
    "print(str_sen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. 利用`tokenizer.encode()`快捷转换**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**作用：token -> id**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 6929, 763, 4578, 4638, 6381, 2554, 8024, 5862, 1762, 3217, 4638, 3799, 1759, 7027, 102]\n"
     ]
    }
   ],
   "source": [
    "# 将字符串转换为id序列，又称之为编码\n",
    "ids = tokenizer.encode(demo, add_special_tokens=True)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注：\n",
    "1. 若和上面的`tokenizer.convert_to_ids()`同时使用，会发现`tokenizer.encode()`的结果会在前后加上`[CLS]`和`[SEP]`，也就是输出中的[101],[102]. 其本身的目的是为了方便进行模型输入的构建。\n",
    "2. 若不希望加入`[CLS]`和`[SEP]`，可以通过`add_special_tokens=False`进行设置。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "相对应，可以通过`tokenizer.decode()`进行逆变换\n",
    "\n",
    "**作用：id -> token**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] 那 些 痛 的 记 忆 ， 落 在 春 的 泥 土 里 [SEP]\n"
     ]
    }
   ],
   "source": [
    "# 将id序列转换为字符串，又称之为解码\n",
    "str_sen = tokenizer.decode(ids, skip_special_tokens=False)\n",
    "print(str_sen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. 填充与截断**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "说明：在进行分词的过程中，会有些句子过长或过短，因此需要进行填充与截断以得到长度适中的数据。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**填充**\n",
    "\n",
    "利用`tokenizer.encode(..., padding = \"max_length\", max_length = )` 进行填充"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 6929, 763, 4578, 4638, 6381, 2554, 8024, 5862, 1762, 3217, 4638, 3799, 1759, 7027, 102, 0, 0, 0, 0]\n",
      "[CLS] 那 些 痛 的 记 忆 ， 落 在 春 的 泥 土 里 [SEP] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "# 填充\n",
    "ids = tokenizer.encode(demo, padding=\"max_length\", max_length=20)\n",
    "print(ids)\n",
    "print(tokenizer.decode(ids, skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*注：观察上面的输出可以发现最后几个位置为0，如果decode回去会发现这里是`[PAD]`，也就是填充的位置。*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**截断**\n",
    "\n",
    "利用`tokenizer.encode(..., truncation = True, max_length = )` 进行截断"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 6929, 763, 4578, 102]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 截断\n",
    "ids = tokenizer.encode(demo, max_length=5, truncation=True)\n",
    "ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. 利用`tokenizer.encode_plus`完成 Attention Mask**\n",
    "\n",
    "对于某些模型来说，单纯的填充是不够的，需要另外建立一个attention_mask的bool数组来指示那部分是真实输出，那部分是填充内容。\n",
    "\n",
    "这一功能在调用`tokenizer.encode_plus`时，会自动完成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 6929, 763, 4578, 4638, 6381, 2554, 8024, 5862, 1762, 3217, 4638, 3799, 1759, 7027, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer.encode_plus(demo, padding=\"max_length\", max_length=15)\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**！最后的封装**\n",
    "\n",
    "上述全部流程最后被完全封装进了`tokenizer`中，因此在实际操作中，可以直接调用`tokenizer`进行数据预处理。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
