{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Hugging Face"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 0. Environment Settings 环境配置"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "- 在conda环境中安装基础python依赖：\n",
    "    ```bash\n",
    "    conda create -n transformer\n",
    "    conda activate transformer\n",
    "    pip install transformer datasets evaluate peft accelerate gradio optimum sentencepiece jupyterlab scikit-learn pandas matplotlib tensorboard nltk rouge\n",
    "    ```\n",
    "\n",
    "- 网络问题更改本地电脑hosts：*本条操作待验证*\n",
    "  - hosts储存路径为: *C:\\Windows\\System32\\drivers\\etc\\hosts*\n",
    "  - 添加如下内容："
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "pycharm": {
     "name": "#%% raw\n"
    }
   },
   "source": [
    "# HuggingFace\n",
    "185.199.108.133 raw.githubusercontent.com\n",
    "185.199.109.133 raw.githubusercontent.com\n",
    "185.199.110.133 raw.githubusercontent.com\n",
    "185.199.111.133 raw.githubusercontent.com\n",
    "2606:50c0:8000::154 raw.githubusercontent.com\n",
    "2606:50c0:8001::154 raw.githubusercontent.com\n",
    "2606:50c0:8002::154 raw.githubusercontent.com\n",
    "2606:50c0:8003::154 raw.githubusercontent.com\n",
    "# End of HuggingFace section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1. Pipeline 工具"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 1.1 Pipeline 概念\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "[Pipeline Official Doc](https://huggingface.co/transformers/main_classes/pipelines.html#transformers.pipeline)\n",
    "\n",
    "- pipeline是一个封装好的工具，可以通过直接的声明进行调用，完成一些预先设定好的NLP任务\n",
    "\n",
    "- pipeline的大致syntax为（较常用的一些opt参数）：`pipeline(task=\"\", model=\"\", tokenizer=\"\", device=)`\n",
    "\n",
    "![20230716105734](https://michael-1313341240.cos.ap-shanghai.myqcloud.com/20230716105734.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 1.2 Pipeline支持的Task\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "- task主要可以包括的任务有：\n",
    "  - 文本分类（包含情感分析）: `task = \"text-classification\"` `task = \"zero-shot-classification\"`\n",
    "  - Token 分类（包含命名实体识别 NER）: `task = \"token-classification\"`\n",
    "  - 人机对话: `task = \"conversational\"`\n",
    "  - 文档问答: `task = \"document-question-answering\"`\n",
    "  - 问题回答: `task = \"question-answering\"`\n",
    "  - 表格回答: `task = \"table-question-answering\"`\n",
    "  - 填空: `task = \"fill-mask\"`\n",
    "  - 生成摘要: `task = \"summarization\"`\n",
    "  - Text-to-text 文本生成: `task = \"text2text-generation\"`\n",
    "  - 机器翻译: `task = \"translation\"`\n",
    "  - 跨模态任务: `task = \"image-to-text\"` `task = \"visual-question-answering\"`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "*可以通过下列命令查看 **pipeline 支持的所有任务***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('audio-classification', {'impl': <class 'transformers.pipelines.audio_classification.AudioClassificationPipeline'>, 'tf': (), 'pt': (<class 'transformers.models.auto.modeling_auto.AutoModelForAudioClassification'>,), 'default': {'model': {'pt': ('superb/wav2vec2-base-superb-ks', '372e048')}}, 'type': 'audio'})\n",
      "('automatic-speech-recognition', {'impl': <class 'transformers.pipelines.automatic_speech_recognition.AutomaticSpeechRecognitionPipeline'>, 'tf': (), 'pt': (<class 'transformers.models.auto.modeling_auto.AutoModelForCTC'>, <class 'transformers.models.auto.modeling_auto.AutoModelForSpeechSeq2Seq'>), 'default': {'model': {'pt': ('facebook/wav2vec2-base-960h', '55bb623')}}, 'type': 'multimodal'})\n",
      "('feature-extraction', {'impl': <class 'transformers.pipelines.feature_extraction.FeatureExtractionPipeline'>, 'tf': (), 'pt': (<class 'transformers.models.auto.modeling_auto.AutoModel'>,), 'default': {'model': {'pt': ('distilbert-base-cased', '935ac13'), 'tf': ('distilbert-base-cased', '935ac13')}}, 'type': 'multimodal'})\n",
      "('text-classification', {'impl': <class 'transformers.pipelines.text_classification.TextClassificationPipeline'>, 'tf': (), 'pt': (<class 'transformers.models.auto.modeling_auto.AutoModelForSequenceClassification'>,), 'default': {'model': {'pt': ('distilbert-base-uncased-finetuned-sst-2-english', 'af0f99b'), 'tf': ('distilbert-base-uncased-finetuned-sst-2-english', 'af0f99b')}}, 'type': 'text'})\n",
      "('token-classification', {'impl': <class 'transformers.pipelines.token_classification.TokenClassificationPipeline'>, 'tf': (), 'pt': (<class 'transformers.models.auto.modeling_auto.AutoModelForTokenClassification'>,), 'default': {'model': {'pt': ('dbmdz/bert-large-cased-finetuned-conll03-english', 'f2482bf'), 'tf': ('dbmdz/bert-large-cased-finetuned-conll03-english', 'f2482bf')}}, 'type': 'text'})\n",
      "('question-answering', {'impl': <class 'transformers.pipelines.question_answering.QuestionAnsweringPipeline'>, 'tf': (), 'pt': (<class 'transformers.models.auto.modeling_auto.AutoModelForQuestionAnswering'>,), 'default': {'model': {'pt': ('distilbert-base-cased-distilled-squad', '626af31'), 'tf': ('distilbert-base-cased-distilled-squad', '626af31')}}, 'type': 'text'})\n",
      "('table-question-answering', {'impl': <class 'transformers.pipelines.table_question_answering.TableQuestionAnsweringPipeline'>, 'pt': (<class 'transformers.models.auto.modeling_auto.AutoModelForTableQuestionAnswering'>,), 'tf': (), 'default': {'model': {'pt': ('google/tapas-base-finetuned-wtq', '69ceee2'), 'tf': ('google/tapas-base-finetuned-wtq', '69ceee2')}}, 'type': 'text'})\n",
      "('visual-question-answering', {'impl': <class 'transformers.pipelines.visual_question_answering.VisualQuestionAnsweringPipeline'>, 'pt': (<class 'transformers.models.auto.modeling_auto.AutoModelForVisualQuestionAnswering'>,), 'tf': (), 'default': {'model': {'pt': ('dandelin/vilt-b32-finetuned-vqa', '4355f59')}}, 'type': 'multimodal'})\n",
      "('document-question-answering', {'impl': <class 'transformers.pipelines.document_question_answering.DocumentQuestionAnsweringPipeline'>, 'pt': (<class 'transformers.models.auto.modeling_auto.AutoModelForDocumentQuestionAnswering'>,), 'tf': (), 'default': {'model': {'pt': ('impira/layoutlm-document-qa', '52e01b3')}}, 'type': 'multimodal'})\n",
      "('fill-mask', {'impl': <class 'transformers.pipelines.fill_mask.FillMaskPipeline'>, 'tf': (), 'pt': (<class 'transformers.models.auto.modeling_auto.AutoModelForMaskedLM'>,), 'default': {'model': {'pt': ('distilroberta-base', 'ec58a5b'), 'tf': ('distilroberta-base', 'ec58a5b')}}, 'type': 'text'})\n",
      "('summarization', {'impl': <class 'transformers.pipelines.text2text_generation.SummarizationPipeline'>, 'tf': (), 'pt': (<class 'transformers.models.auto.modeling_auto.AutoModelForSeq2SeqLM'>,), 'default': {'model': {'pt': ('sshleifer/distilbart-cnn-12-6', 'a4f8f3e'), 'tf': ('t5-small', 'd769bba')}}, 'type': 'text'})\n",
      "('translation', {'impl': <class 'transformers.pipelines.text2text_generation.TranslationPipeline'>, 'tf': (), 'pt': (<class 'transformers.models.auto.modeling_auto.AutoModelForSeq2SeqLM'>,), 'default': {('en', 'fr'): {'model': {'pt': ('t5-base', '686f1db'), 'tf': ('t5-base', '686f1db')}}, ('en', 'de'): {'model': {'pt': ('t5-base', '686f1db'), 'tf': ('t5-base', '686f1db')}}, ('en', 'ro'): {'model': {'pt': ('t5-base', '686f1db'), 'tf': ('t5-base', '686f1db')}}}, 'type': 'text'})\n",
      "('text2text-generation', {'impl': <class 'transformers.pipelines.text2text_generation.Text2TextGenerationPipeline'>, 'tf': (), 'pt': (<class 'transformers.models.auto.modeling_auto.AutoModelForSeq2SeqLM'>,), 'default': {'model': {'pt': ('t5-base', '686f1db'), 'tf': ('t5-base', '686f1db')}}, 'type': 'text'})\n",
      "('text-generation', {'impl': <class 'transformers.pipelines.text_generation.TextGenerationPipeline'>, 'tf': (), 'pt': (<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>,), 'default': {'model': {'pt': ('gpt2', '6c0e608'), 'tf': ('gpt2', '6c0e608')}}, 'type': 'text'})\n",
      "('zero-shot-classification', {'impl': <class 'transformers.pipelines.zero_shot_classification.ZeroShotClassificationPipeline'>, 'tf': (), 'pt': (<class 'transformers.models.auto.modeling_auto.AutoModelForSequenceClassification'>,), 'default': {'model': {'pt': ('facebook/bart-large-mnli', 'c626438'), 'tf': ('roberta-large-mnli', '130fb28')}, 'config': {'pt': ('facebook/bart-large-mnli', 'c626438'), 'tf': ('roberta-large-mnli', '130fb28')}}, 'type': 'text'})\n",
      "('zero-shot-image-classification', {'impl': <class 'transformers.pipelines.zero_shot_image_classification.ZeroShotImageClassificationPipeline'>, 'tf': (), 'pt': (<class 'transformers.models.auto.modeling_auto.AutoModelForZeroShotImageClassification'>,), 'default': {'model': {'pt': ('openai/clip-vit-base-patch32', 'f4881ba'), 'tf': ('openai/clip-vit-base-patch32', 'f4881ba')}}, 'type': 'multimodal'})\n",
      "('zero-shot-audio-classification', {'impl': <class 'transformers.pipelines.zero_shot_audio_classification.ZeroShotAudioClassificationPipeline'>, 'tf': (), 'pt': (<class 'transformers.models.auto.modeling_auto.AutoModel'>,), 'default': {'model': {'pt': ('laion/clap-htsat-fused', '973b6e5')}}, 'type': 'multimodal'})\n",
      "('conversational', {'impl': <class 'transformers.pipelines.conversational.ConversationalPipeline'>, 'tf': (), 'pt': (<class 'transformers.models.auto.modeling_auto.AutoModelForSeq2SeqLM'>, <class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>), 'default': {'model': {'pt': ('microsoft/DialoGPT-medium', '8bada3b'), 'tf': ('microsoft/DialoGPT-medium', '8bada3b')}}, 'type': 'text'})\n",
      "('image-classification', {'impl': <class 'transformers.pipelines.image_classification.ImageClassificationPipeline'>, 'tf': (), 'pt': (<class 'transformers.models.auto.modeling_auto.AutoModelForImageClassification'>,), 'default': {'model': {'pt': ('google/vit-base-patch16-224', '5dca96d'), 'tf': ('google/vit-base-patch16-224', '5dca96d')}}, 'type': 'image'})\n",
      "('image-segmentation', {'impl': <class 'transformers.pipelines.image_segmentation.ImageSegmentationPipeline'>, 'tf': (), 'pt': (<class 'transformers.models.auto.modeling_auto.AutoModelForImageSegmentation'>, <class 'transformers.models.auto.modeling_auto.AutoModelForSemanticSegmentation'>), 'default': {'model': {'pt': ('facebook/detr-resnet-50-panoptic', 'fc15262')}}, 'type': 'multimodal'})\n",
      "('image-to-text', {'impl': <class 'transformers.pipelines.image_to_text.ImageToTextPipeline'>, 'tf': (), 'pt': (<class 'transformers.models.auto.modeling_auto.AutoModelForVision2Seq'>,), 'default': {'model': {'pt': ('ydshieh/vit-gpt2-coco-en', '65636df'), 'tf': ('ydshieh/vit-gpt2-coco-en', '65636df')}}, 'type': 'multimodal'})\n",
      "('object-detection', {'impl': <class 'transformers.pipelines.object_detection.ObjectDetectionPipeline'>, 'tf': (), 'pt': (<class 'transformers.models.auto.modeling_auto.AutoModelForObjectDetection'>,), 'default': {'model': {'pt': ('facebook/detr-resnet-50', '2729413')}}, 'type': 'multimodal'})\n",
      "('zero-shot-object-detection', {'impl': <class 'transformers.pipelines.zero_shot_object_detection.ZeroShotObjectDetectionPipeline'>, 'tf': (), 'pt': (<class 'transformers.models.auto.modeling_auto.AutoModelForZeroShotObjectDetection'>,), 'default': {'model': {'pt': ('google/owlvit-base-patch32', '17740e1')}}, 'type': 'multimodal'})\n",
      "('depth-estimation', {'impl': <class 'transformers.pipelines.depth_estimation.DepthEstimationPipeline'>, 'tf': (), 'pt': (<class 'transformers.models.auto.modeling_auto.AutoModelForDepthEstimation'>,), 'default': {'model': {'pt': ('Intel/dpt-large', 'e93beec')}}, 'type': 'image'})\n",
      "('video-classification', {'impl': <class 'transformers.pipelines.video_classification.VideoClassificationPipeline'>, 'tf': (), 'pt': (<class 'transformers.models.auto.modeling_auto.AutoModelForVideoClassification'>,), 'default': {'model': {'pt': ('MCG-NJU/videomae-base-finetuned-kinetics', '4800870')}}, 'type': 'video'})\n",
      "('mask-generation', {'impl': <class 'transformers.pipelines.mask_generation.MaskGenerationPipeline'>, 'tf': (), 'pt': (<class 'transformers.models.auto.modeling_auto.AutoModelForMaskGeneration'>,), 'default': {'model': {'pt': ('facebook/sam-vit-huge', '997b15')}}, 'type': 'multimodal'})\n"
     ]
    }
   ],
   "source": [
    "from transformers.pipelines import SUPPORTED_TASKS\n",
    "print(*SUPPORTED_TASKS.items(),sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 1.3 Pipeline的创建与使用（例）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "- 在HuggingFace的[Model](https://huggingface.co/models)分区可以寻找想要用的模型\n",
    "- 点击进入模型详细页面后，可以看到具体描述和demo交互\n",
    "\n",
    "![20230716120431](https://michael-1313341240.cos.ap-shanghai.myqcloud.com/20230716120431.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading pytorch_model.bin:   8%|▊         | 31.5M/409M [00:25<01:26, 4.36MB/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[6], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[39mfrom\u001B[39;00m \u001B[39mtransformers\u001B[39;00m \u001B[39mimport\u001B[39;00m AutoModelForSequenceClassification,AutoTokenizer,pipeline\n\u001B[1;32m      2\u001B[0m \u001B[39m# model是指定这个task要使用的nlp模型\u001B[39;00m\n\u001B[0;32m----> 3\u001B[0m model \u001B[39m=\u001B[39m AutoModelForSequenceClassification\u001B[39m.\u001B[39;49mfrom_pretrained(\u001B[39m'\u001B[39;49m\u001B[39muer/roberta-base-finetuned-chinanews-chinese\u001B[39;49m\u001B[39m'\u001B[39;49m)\n\u001B[1;32m      4\u001B[0m \u001B[39m# tokenizer是指定task的分词器，其具体含义会在后面进行学习\u001B[39;00m\n\u001B[1;32m      5\u001B[0m tokenizer \u001B[39m=\u001B[39m AutoTokenizer\u001B[39m.\u001B[39mfrom_pretrained(\u001B[39m'\u001B[39m\u001B[39muer/roberta-base-finetuned-chinanews-chinese\u001B[39m\u001B[39m'\u001B[39m)\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/transformer_mac/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:467\u001B[0m, in \u001B[0;36m_BaseAutoModelClass.from_pretrained\u001B[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001B[0m\n\u001B[1;32m    465\u001B[0m \u001B[39melif\u001B[39;00m \u001B[39mtype\u001B[39m(config) \u001B[39min\u001B[39;00m \u001B[39mcls\u001B[39m\u001B[39m.\u001B[39m_model_mapping\u001B[39m.\u001B[39mkeys():\n\u001B[1;32m    466\u001B[0m     model_class \u001B[39m=\u001B[39m _get_model_class(config, \u001B[39mcls\u001B[39m\u001B[39m.\u001B[39m_model_mapping)\n\u001B[0;32m--> 467\u001B[0m     \u001B[39mreturn\u001B[39;00m model_class\u001B[39m.\u001B[39;49mfrom_pretrained(\n\u001B[1;32m    468\u001B[0m         pretrained_model_name_or_path, \u001B[39m*\u001B[39;49mmodel_args, config\u001B[39m=\u001B[39;49mconfig, \u001B[39m*\u001B[39;49m\u001B[39m*\u001B[39;49mhub_kwargs, \u001B[39m*\u001B[39;49m\u001B[39m*\u001B[39;49mkwargs\n\u001B[1;32m    469\u001B[0m     )\n\u001B[1;32m    470\u001B[0m \u001B[39mraise\u001B[39;00m \u001B[39mValueError\u001B[39;00m(\n\u001B[1;32m    471\u001B[0m     \u001B[39mf\u001B[39m\u001B[39m\"\u001B[39m\u001B[39mUnrecognized configuration class \u001B[39m\u001B[39m{\u001B[39;00mconfig\u001B[39m.\u001B[39m\u001B[39m__class__\u001B[39m\u001B[39m}\u001B[39;00m\u001B[39m for this kind of AutoModel: \u001B[39m\u001B[39m{\u001B[39;00m\u001B[39mcls\u001B[39m\u001B[39m.\u001B[39m\u001B[39m__name__\u001B[39m\u001B[39m}\u001B[39;00m\u001B[39m.\u001B[39m\u001B[39m\\n\u001B[39;00m\u001B[39m\"\u001B[39m\n\u001B[1;32m    472\u001B[0m     \u001B[39mf\u001B[39m\u001B[39m\"\u001B[39m\u001B[39mModel type should be one of \u001B[39m\u001B[39m{\u001B[39;00m\u001B[39m'\u001B[39m\u001B[39m, \u001B[39m\u001B[39m'\u001B[39m\u001B[39m.\u001B[39mjoin(c\u001B[39m.\u001B[39m\u001B[39m__name__\u001B[39m\u001B[39m \u001B[39m\u001B[39mfor\u001B[39;00m\u001B[39m \u001B[39mc\u001B[39m \u001B[39m\u001B[39min\u001B[39;00m\u001B[39m \u001B[39m\u001B[39mcls\u001B[39m\u001B[39m.\u001B[39m_model_mapping\u001B[39m.\u001B[39mkeys())\u001B[39m}\u001B[39;00m\u001B[39m.\u001B[39m\u001B[39m\"\u001B[39m\n\u001B[1;32m    473\u001B[0m )\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/transformer_mac/lib/python3.10/site-packages/transformers/modeling_utils.py:2432\u001B[0m, in \u001B[0;36mPreTrainedModel.from_pretrained\u001B[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001B[0m\n\u001B[1;32m   2417\u001B[0m \u001B[39mtry\u001B[39;00m:\n\u001B[1;32m   2418\u001B[0m     \u001B[39m# Load from URL or cache if already cached\u001B[39;00m\n\u001B[1;32m   2419\u001B[0m     cached_file_kwargs \u001B[39m=\u001B[39m {\n\u001B[1;32m   2420\u001B[0m         \u001B[39m\"\u001B[39m\u001B[39mcache_dir\u001B[39m\u001B[39m\"\u001B[39m: cache_dir,\n\u001B[1;32m   2421\u001B[0m         \u001B[39m\"\u001B[39m\u001B[39mforce_download\u001B[39m\u001B[39m\"\u001B[39m: force_download,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   2430\u001B[0m         \u001B[39m\"\u001B[39m\u001B[39m_commit_hash\u001B[39m\u001B[39m\"\u001B[39m: commit_hash,\n\u001B[1;32m   2431\u001B[0m     }\n\u001B[0;32m-> 2432\u001B[0m     resolved_archive_file \u001B[39m=\u001B[39m cached_file(pretrained_model_name_or_path, filename, \u001B[39m*\u001B[39;49m\u001B[39m*\u001B[39;49mcached_file_kwargs)\n\u001B[1;32m   2434\u001B[0m     \u001B[39m# Since we set _raise_exceptions_for_missing_entries=False, we don't get an exception but a None\u001B[39;00m\n\u001B[1;32m   2435\u001B[0m     \u001B[39m# result when internet is up, the repo and revision exist, but the file does not.\u001B[39;00m\n\u001B[1;32m   2436\u001B[0m     \u001B[39mif\u001B[39;00m resolved_archive_file \u001B[39mis\u001B[39;00m \u001B[39mNone\u001B[39;00m \u001B[39mand\u001B[39;00m filename \u001B[39m==\u001B[39m _add_variant(SAFE_WEIGHTS_NAME, variant):\n\u001B[1;32m   2437\u001B[0m         \u001B[39m# Maybe the checkpoint is sharded, we try to grab the index name in this case.\u001B[39;00m\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/transformer_mac/lib/python3.10/site-packages/transformers/utils/hub.py:417\u001B[0m, in \u001B[0;36mcached_file\u001B[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001B[0m\n\u001B[1;32m    414\u001B[0m user_agent \u001B[39m=\u001B[39m http_user_agent(user_agent)\n\u001B[1;32m    415\u001B[0m \u001B[39mtry\u001B[39;00m:\n\u001B[1;32m    416\u001B[0m     \u001B[39m# Load from URL or cache if already cached\u001B[39;00m\n\u001B[0;32m--> 417\u001B[0m     resolved_file \u001B[39m=\u001B[39m hf_hub_download(\n\u001B[1;32m    418\u001B[0m         path_or_repo_id,\n\u001B[1;32m    419\u001B[0m         filename,\n\u001B[1;32m    420\u001B[0m         subfolder\u001B[39m=\u001B[39;49m\u001B[39mNone\u001B[39;49;00m \u001B[39mif\u001B[39;49;00m \u001B[39mlen\u001B[39;49m(subfolder) \u001B[39m==\u001B[39;49m \u001B[39m0\u001B[39;49m \u001B[39melse\u001B[39;49;00m subfolder,\n\u001B[1;32m    421\u001B[0m         repo_type\u001B[39m=\u001B[39;49mrepo_type,\n\u001B[1;32m    422\u001B[0m         revision\u001B[39m=\u001B[39;49mrevision,\n\u001B[1;32m    423\u001B[0m         cache_dir\u001B[39m=\u001B[39;49mcache_dir,\n\u001B[1;32m    424\u001B[0m         user_agent\u001B[39m=\u001B[39;49muser_agent,\n\u001B[1;32m    425\u001B[0m         force_download\u001B[39m=\u001B[39;49mforce_download,\n\u001B[1;32m    426\u001B[0m         proxies\u001B[39m=\u001B[39;49mproxies,\n\u001B[1;32m    427\u001B[0m         resume_download\u001B[39m=\u001B[39;49mresume_download,\n\u001B[1;32m    428\u001B[0m         use_auth_token\u001B[39m=\u001B[39;49muse_auth_token,\n\u001B[1;32m    429\u001B[0m         local_files_only\u001B[39m=\u001B[39;49mlocal_files_only,\n\u001B[1;32m    430\u001B[0m     )\n\u001B[1;32m    432\u001B[0m \u001B[39mexcept\u001B[39;00m RepositoryNotFoundError:\n\u001B[1;32m    433\u001B[0m     \u001B[39mraise\u001B[39;00m \u001B[39mEnvironmentError\u001B[39;00m(\n\u001B[1;32m    434\u001B[0m         \u001B[39mf\u001B[39m\u001B[39m\"\u001B[39m\u001B[39m{\u001B[39;00mpath_or_repo_id\u001B[39m}\u001B[39;00m\u001B[39m is not a local folder and is not a valid model identifier \u001B[39m\u001B[39m\"\u001B[39m\n\u001B[1;32m    435\u001B[0m         \u001B[39m\"\u001B[39m\u001B[39mlisted on \u001B[39m\u001B[39m'\u001B[39m\u001B[39mhttps://huggingface.co/models\u001B[39m\u001B[39m'\u001B[39m\u001B[39m\\n\u001B[39;00m\u001B[39mIf this is a private repository, make sure to \u001B[39m\u001B[39m\"\u001B[39m\n\u001B[1;32m    436\u001B[0m         \u001B[39m\"\u001B[39m\u001B[39mpass a token having permission to this repo with `use_auth_token` or log in with \u001B[39m\u001B[39m\"\u001B[39m\n\u001B[1;32m    437\u001B[0m         \u001B[39m\"\u001B[39m\u001B[39m`huggingface-cli login` and pass `use_auth_token=True`.\u001B[39m\u001B[39m\"\u001B[39m\n\u001B[1;32m    438\u001B[0m     )\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/transformer_mac/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:118\u001B[0m, in \u001B[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    115\u001B[0m \u001B[39mif\u001B[39;00m check_use_auth_token:\n\u001B[1;32m    116\u001B[0m     kwargs \u001B[39m=\u001B[39m smoothly_deprecate_use_auth_token(fn_name\u001B[39m=\u001B[39mfn\u001B[39m.\u001B[39m\u001B[39m__name__\u001B[39m, has_token\u001B[39m=\u001B[39mhas_token, kwargs\u001B[39m=\u001B[39mkwargs)\n\u001B[0;32m--> 118\u001B[0m \u001B[39mreturn\u001B[39;00m fn(\u001B[39m*\u001B[39;49margs, \u001B[39m*\u001B[39;49m\u001B[39m*\u001B[39;49mkwargs)\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/transformer_mac/lib/python3.10/site-packages/huggingface_hub/file_download.py:1364\u001B[0m, in \u001B[0;36mhf_hub_download\u001B[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout)\u001B[0m\n\u001B[1;32m   1361\u001B[0m \u001B[39mwith\u001B[39;00m temp_file_manager() \u001B[39mas\u001B[39;00m temp_file:\n\u001B[1;32m   1362\u001B[0m     logger\u001B[39m.\u001B[39minfo(\u001B[39m\"\u001B[39m\u001B[39mdownloading \u001B[39m\u001B[39m%s\u001B[39;00m\u001B[39m to \u001B[39m\u001B[39m%s\u001B[39;00m\u001B[39m\"\u001B[39m, url, temp_file\u001B[39m.\u001B[39mname)\n\u001B[0;32m-> 1364\u001B[0m     http_get(\n\u001B[1;32m   1365\u001B[0m         url_to_download,\n\u001B[1;32m   1366\u001B[0m         temp_file,\n\u001B[1;32m   1367\u001B[0m         proxies\u001B[39m=\u001B[39;49mproxies,\n\u001B[1;32m   1368\u001B[0m         resume_size\u001B[39m=\u001B[39;49mresume_size,\n\u001B[1;32m   1369\u001B[0m         headers\u001B[39m=\u001B[39;49mheaders,\n\u001B[1;32m   1370\u001B[0m         expected_size\u001B[39m=\u001B[39;49mexpected_size,\n\u001B[1;32m   1371\u001B[0m     )\n\u001B[1;32m   1373\u001B[0m \u001B[39mif\u001B[39;00m local_dir \u001B[39mis\u001B[39;00m \u001B[39mNone\u001B[39;00m:\n\u001B[1;32m   1374\u001B[0m     logger\u001B[39m.\u001B[39minfo(\u001B[39mf\u001B[39m\u001B[39m\"\u001B[39m\u001B[39mStoring \u001B[39m\u001B[39m{\u001B[39;00murl\u001B[39m}\u001B[39;00m\u001B[39m in cache at \u001B[39m\u001B[39m{\u001B[39;00mblob_path\u001B[39m}\u001B[39;00m\u001B[39m\"\u001B[39m)\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/transformer_mac/lib/python3.10/site-packages/huggingface_hub/file_download.py:541\u001B[0m, in \u001B[0;36mhttp_get\u001B[0;34m(url, temp_file, proxies, resume_size, headers, timeout, max_retries, expected_size)\u001B[0m\n\u001B[1;32m    531\u001B[0m     displayed_name \u001B[39m=\u001B[39m \u001B[39mf\u001B[39m\u001B[39m\"\u001B[39m\u001B[39m(…)\u001B[39m\u001B[39m{\u001B[39;00mdisplayed_name[\u001B[39m-\u001B[39m\u001B[39m20\u001B[39m:]\u001B[39m}\u001B[39;00m\u001B[39m\"\u001B[39m\n\u001B[1;32m    533\u001B[0m progress \u001B[39m=\u001B[39m tqdm(\n\u001B[1;32m    534\u001B[0m     unit\u001B[39m=\u001B[39m\u001B[39m\"\u001B[39m\u001B[39mB\u001B[39m\u001B[39m\"\u001B[39m,\n\u001B[1;32m    535\u001B[0m     unit_scale\u001B[39m=\u001B[39m\u001B[39mTrue\u001B[39;00m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    539\u001B[0m     disable\u001B[39m=\u001B[39m\u001B[39mbool\u001B[39m(logger\u001B[39m.\u001B[39mgetEffectiveLevel() \u001B[39m==\u001B[39m logging\u001B[39m.\u001B[39mNOTSET),\n\u001B[1;32m    540\u001B[0m )\n\u001B[0;32m--> 541\u001B[0m \u001B[39mfor\u001B[39;00m chunk \u001B[39min\u001B[39;00m r\u001B[39m.\u001B[39miter_content(chunk_size\u001B[39m=\u001B[39m\u001B[39m10\u001B[39m \u001B[39m*\u001B[39m \u001B[39m1024\u001B[39m \u001B[39m*\u001B[39m \u001B[39m1024\u001B[39m):\n\u001B[1;32m    542\u001B[0m     \u001B[39mif\u001B[39;00m chunk:  \u001B[39m# filter out keep-alive new chunks\u001B[39;00m\n\u001B[1;32m    543\u001B[0m         progress\u001B[39m.\u001B[39mupdate(\u001B[39mlen\u001B[39m(chunk))\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/transformer_mac/lib/python3.10/site-packages/requests/models.py:816\u001B[0m, in \u001B[0;36mResponse.iter_content.<locals>.generate\u001B[0;34m()\u001B[0m\n\u001B[1;32m    814\u001B[0m \u001B[39mif\u001B[39;00m \u001B[39mhasattr\u001B[39m(\u001B[39mself\u001B[39m\u001B[39m.\u001B[39mraw, \u001B[39m\"\u001B[39m\u001B[39mstream\u001B[39m\u001B[39m\"\u001B[39m):\n\u001B[1;32m    815\u001B[0m     \u001B[39mtry\u001B[39;00m:\n\u001B[0;32m--> 816\u001B[0m         \u001B[39myield from\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mraw\u001B[39m.\u001B[39mstream(chunk_size, decode_content\u001B[39m=\u001B[39m\u001B[39mTrue\u001B[39;00m)\n\u001B[1;32m    817\u001B[0m     \u001B[39mexcept\u001B[39;00m ProtocolError \u001B[39mas\u001B[39;00m e:\n\u001B[1;32m    818\u001B[0m         \u001B[39mraise\u001B[39;00m ChunkedEncodingError(e)\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/transformer_mac/lib/python3.10/site-packages/urllib3/response.py:628\u001B[0m, in \u001B[0;36mHTTPResponse.stream\u001B[0;34m(self, amt, decode_content)\u001B[0m\n\u001B[1;32m    626\u001B[0m \u001B[39melse\u001B[39;00m:\n\u001B[1;32m    627\u001B[0m     \u001B[39mwhile\u001B[39;00m \u001B[39mnot\u001B[39;00m is_fp_closed(\u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_fp):\n\u001B[0;32m--> 628\u001B[0m         data \u001B[39m=\u001B[39m \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49mread(amt\u001B[39m=\u001B[39;49mamt, decode_content\u001B[39m=\u001B[39;49mdecode_content)\n\u001B[1;32m    630\u001B[0m         \u001B[39mif\u001B[39;00m data:\n\u001B[1;32m    631\u001B[0m             \u001B[39myield\u001B[39;00m data\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/transformer_mac/lib/python3.10/site-packages/urllib3/response.py:567\u001B[0m, in \u001B[0;36mHTTPResponse.read\u001B[0;34m(self, amt, decode_content, cache_content)\u001B[0m\n\u001B[1;32m    564\u001B[0m fp_closed \u001B[39m=\u001B[39m \u001B[39mgetattr\u001B[39m(\u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_fp, \u001B[39m\"\u001B[39m\u001B[39mclosed\u001B[39m\u001B[39m\"\u001B[39m, \u001B[39mFalse\u001B[39;00m)\n\u001B[1;32m    566\u001B[0m \u001B[39mwith\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_error_catcher():\n\u001B[0;32m--> 567\u001B[0m     data \u001B[39m=\u001B[39m \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49m_fp_read(amt) \u001B[39mif\u001B[39;00m \u001B[39mnot\u001B[39;00m fp_closed \u001B[39melse\u001B[39;00m \u001B[39mb\u001B[39m\u001B[39m\"\u001B[39m\u001B[39m\"\u001B[39m\n\u001B[1;32m    568\u001B[0m     \u001B[39mif\u001B[39;00m amt \u001B[39mis\u001B[39;00m \u001B[39mNone\u001B[39;00m:\n\u001B[1;32m    569\u001B[0m         flush_decoder \u001B[39m=\u001B[39m \u001B[39mTrue\u001B[39;00m\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/transformer_mac/lib/python3.10/site-packages/urllib3/response.py:533\u001B[0m, in \u001B[0;36mHTTPResponse._fp_read\u001B[0;34m(self, amt)\u001B[0m\n\u001B[1;32m    530\u001B[0m     \u001B[39mreturn\u001B[39;00m buffer\u001B[39m.\u001B[39mgetvalue()\n\u001B[1;32m    531\u001B[0m \u001B[39melse\u001B[39;00m:\n\u001B[1;32m    532\u001B[0m     \u001B[39m# StringIO doesn't like amt=None\u001B[39;00m\n\u001B[0;32m--> 533\u001B[0m     \u001B[39mreturn\u001B[39;00m \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49m_fp\u001B[39m.\u001B[39;49mread(amt) \u001B[39mif\u001B[39;00m amt \u001B[39mis\u001B[39;00m \u001B[39mnot\u001B[39;00m \u001B[39mNone\u001B[39;00m \u001B[39melse\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_fp\u001B[39m.\u001B[39mread()\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/transformer_mac/lib/python3.10/http/client.py:466\u001B[0m, in \u001B[0;36mHTTPResponse.read\u001B[0;34m(self, amt)\u001B[0m\n\u001B[1;32m    463\u001B[0m \u001B[39mif\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mlength \u001B[39mis\u001B[39;00m \u001B[39mnot\u001B[39;00m \u001B[39mNone\u001B[39;00m \u001B[39mand\u001B[39;00m amt \u001B[39m>\u001B[39m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mlength:\n\u001B[1;32m    464\u001B[0m     \u001B[39m# clip the read to the \"end of response\"\u001B[39;00m\n\u001B[1;32m    465\u001B[0m     amt \u001B[39m=\u001B[39m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mlength\n\u001B[0;32m--> 466\u001B[0m s \u001B[39m=\u001B[39m \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49mfp\u001B[39m.\u001B[39;49mread(amt)\n\u001B[1;32m    467\u001B[0m \u001B[39mif\u001B[39;00m \u001B[39mnot\u001B[39;00m s \u001B[39mand\u001B[39;00m amt:\n\u001B[1;32m    468\u001B[0m     \u001B[39m# Ideally, we would raise IncompleteRead if the content-length\u001B[39;00m\n\u001B[1;32m    469\u001B[0m     \u001B[39m# wasn't satisfied, but it might break compatibility.\u001B[39;00m\n\u001B[1;32m    470\u001B[0m     \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_close_conn()\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/transformer_mac/lib/python3.10/socket.py:705\u001B[0m, in \u001B[0;36mSocketIO.readinto\u001B[0;34m(self, b)\u001B[0m\n\u001B[1;32m    703\u001B[0m \u001B[39mwhile\u001B[39;00m \u001B[39mTrue\u001B[39;00m:\n\u001B[1;32m    704\u001B[0m     \u001B[39mtry\u001B[39;00m:\n\u001B[0;32m--> 705\u001B[0m         \u001B[39mreturn\u001B[39;00m \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49m_sock\u001B[39m.\u001B[39;49mrecv_into(b)\n\u001B[1;32m    706\u001B[0m     \u001B[39mexcept\u001B[39;00m timeout:\n\u001B[1;32m    707\u001B[0m         \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_timeout_occurred \u001B[39m=\u001B[39m \u001B[39mTrue\u001B[39;00m\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/transformer_mac/lib/python3.10/ssl.py:1274\u001B[0m, in \u001B[0;36mSSLSocket.recv_into\u001B[0;34m(self, buffer, nbytes, flags)\u001B[0m\n\u001B[1;32m   1270\u001B[0m     \u001B[39mif\u001B[39;00m flags \u001B[39m!=\u001B[39m \u001B[39m0\u001B[39m:\n\u001B[1;32m   1271\u001B[0m         \u001B[39mraise\u001B[39;00m \u001B[39mValueError\u001B[39;00m(\n\u001B[1;32m   1272\u001B[0m           \u001B[39m\"\u001B[39m\u001B[39mnon-zero flags not allowed in calls to recv_into() on \u001B[39m\u001B[39m%s\u001B[39;00m\u001B[39m\"\u001B[39m \u001B[39m%\u001B[39m\n\u001B[1;32m   1273\u001B[0m           \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m\u001B[39m__class__\u001B[39m)\n\u001B[0;32m-> 1274\u001B[0m     \u001B[39mreturn\u001B[39;00m \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49mread(nbytes, buffer)\n\u001B[1;32m   1275\u001B[0m \u001B[39melse\u001B[39;00m:\n\u001B[1;32m   1276\u001B[0m     \u001B[39mreturn\u001B[39;00m \u001B[39msuper\u001B[39m()\u001B[39m.\u001B[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/transformer_mac/lib/python3.10/ssl.py:1130\u001B[0m, in \u001B[0;36mSSLSocket.read\u001B[0;34m(self, len, buffer)\u001B[0m\n\u001B[1;32m   1128\u001B[0m \u001B[39mtry\u001B[39;00m:\n\u001B[1;32m   1129\u001B[0m     \u001B[39mif\u001B[39;00m buffer \u001B[39mis\u001B[39;00m \u001B[39mnot\u001B[39;00m \u001B[39mNone\u001B[39;00m:\n\u001B[0;32m-> 1130\u001B[0m         \u001B[39mreturn\u001B[39;00m \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49m_sslobj\u001B[39m.\u001B[39;49mread(\u001B[39mlen\u001B[39;49m, buffer)\n\u001B[1;32m   1131\u001B[0m     \u001B[39melse\u001B[39;00m:\n\u001B[1;32m   1132\u001B[0m         \u001B[39mreturn\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_sslobj\u001B[39m.\u001B[39mread(\u001B[39mlen\u001B[39m)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification,AutoTokenizer,pipeline\n",
    "# model是指定这个task要使用的nlp模型\n",
    "model = AutoModelForSequenceClassification.from_pretrained('uer/roberta-base-finetuned-chinanews-chinese')\n",
    "# tokenizer是指定task的分词器，其具体含义会在后面进行学习\n",
    "tokenizer = AutoTokenizer.from_pretrained('uer/roberta-base-finetuned-chinanews-chinese')\n",
    "# 通过pipeline(...)就会生成这样一个封装好的工具，这里命名为pipe\n",
    "## 此处的task为sentiment analysis，即情感分析\n",
    "pipe = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer)\n",
    "# 调用pipe，配合相应的输入，就可以得到nlp任务的相应输出\n",
    "pipe(\"暴雪被微软收购了！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**使用GPU加速计算**\n",
    "\n",
    "通过`pipeline(, ... , device = 0)`显式加载第0张GPU进行加速计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU:0.005329703807830811\n",
      "CPU:0.01029831838607788\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "times = []\n",
    "\n",
    "# GPU inference\n",
    "pipe_gpu = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer,device=0)\n",
    "for i in range(1000):\n",
    "    torch.cuda.synchronize()\n",
    "    start = time.time()\n",
    "    pipe_gpu(\"暴雪被微软收购了！\")\n",
    "    torch.cuda.synchronize()\n",
    "    end = time.time()\n",
    "    times.append(end - start)\n",
    "print(f\"GPU:{sum(times) / 1000}\")\n",
    "\n",
    "# CPU inference\n",
    "for i in range(1000):\n",
    "    torch.cuda.synchronize()\n",
    "    start = time.time()\n",
    "    pipe(\"暴雪被微软收购了！\")\n",
    "    torch.cuda.synchronize()\n",
    "    end = time.time()\n",
    "    times.append(end - start)\n",
    "print(f\"CPU:{sum(times) / 1000}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 1.4 Pipeline的背后实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**1. 初始化Tokenizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "tokenizer = AutoTokenizer.from_pretrained\\\n",
    "    (\"uer/roberta-base-finetuned-dianping-chinese\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**2. 初始化Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87eccdf407c64e51bbcf3d98a755ba3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/409M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained\\\n",
    "    (\"uer/roberta-base-finetuned-dianping-chinese\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForSequenceClassification(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(21128, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 查看模型结构\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**3. 数据输入&通过`tokenizer`预处理**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "input_txt = \"我很喜欢你！\"\n",
    "input = tokenizer(input_txt, return_tensors=\"pt\") # pt即pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 101, 2769, 2523, 1599, 3614,  872, 8013,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "# 查看分词后结果\n",
    "print(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**4. 模型预测**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res:\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.4593,  1.4171]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits:\n",
      "tensor([[-1.4593,  1.4171]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "res = model(**input) # 这一步得到的结果是一个未标准化的logits向量\n",
    "print(f\"res:\\n{res}\") # 输出res，这里列出了一系列中间操作的说明；数值上重点关注logits值\n",
    "logits = res.logits # 从res中提取logits向量\n",
    "print(f\"logits:\\n{logits}\") # 输出logits向量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**5. 输出结果**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits_softmax:\ttensor([[0.0533, 0.9467]], grad_fn=<SoftmaxBackward0>)\n",
      "pred:\t1\n",
      "result:\tpositive (stars 4 and 5)\n"
     ]
    }
   ],
   "source": [
    "logits_softmax = torch.softmax(logits,dim=-1) # 对logits向量进行softmax归一化操作\n",
    "print(f\"logits_softmax:\\t{logits_softmax}\") # 输出归一化后的logits向量\n",
    "\n",
    "pred = torch.argmax(logits_softmax).item() # 从归一化后的logits向量中提取最大值的索引 \n",
    "print(f\"pred:\\t{pred}\") # 输出预测结果，这里的pred是一个数字索引，需要转换为索引对应的标签\n",
    "\n",
    "result = model.config.id2label.get(pred) # 通过id2label字典，将索引转换为标签\n",
    "print(f\"result:\\t{result}\") # 输出标签"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2. Tokenizer 分词器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 2.1 Tokenizer 概念"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**基本NLP数据预处理流程**\n",
    "\n",
    "1. 分词：将文本数据分词成字、字词等\n",
    "2. 构建词典：构建词典，filter一些过低、过高的单词等\n",
    "3. 数据转换：将词典映射成数字序列\n",
    "4. 数据填充与截断\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 2.2 Tokenizer的使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "demo = \"那些痛的记忆，落在春的泥土里\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Step1. 加载Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertTokenizerFast(name_or_path='uer/roberta-base-finetuned-dianping-chinese', vocab_size=21128, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True)\n"
     ]
    }
   ],
   "source": [
    "# 从huggingface的模型库中加载分词器\n",
    "tokenizer = AutoTokenizer.from_pretrained\\\n",
    "    (\"uer/roberta-base-finetuned-dianping-chinese\")\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "也可以将tokenizer保存到本地再从本地调用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained(\"./roberta_tokenizer\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./roberta_tokenizer\")\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "在调用非官方上传的远程仓库中的tokenizer时，要加入`trust_remote_code=True`参数，例如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatGLMTokenizer(name_or_path='THUDM/chatglm-6b', vocab_size=130344, model_max_length=2048, is_fast=False, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<sop>', 'eos_token': '<eop>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer_chatglm = AutoTokenizer.from_pretrained\\\n",
    "    (\"THUDM/chatglm-6b\",trust_remote_code = True)\n",
    "print(tokenizer_chatglm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Step2. 利用`tokenizer.tokenize()`进行分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['那', '些', '痛', '的', '记', '忆', '，', '落', '在', '春', '的', '泥', '土', '里']\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(demo)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "*对于tokenizer词典，可以通过以下方法进行查看：*\n",
    "\n",
    "`tokenizer.vocab()`：查看词典\n",
    "\n",
    "`tokenizer.vocab_size`：查看词典大小"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Step3\\*. 利用`tokenizer.convert_to_ids()`进行索引转换"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "目的：token -> id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6929, 763, 4578, 4638, 6381, 2554, 8024, 5862, 1762, 3217, 4638, 3799, 1759, 7027]\n"
     ]
    }
   ],
   "source": [
    "# 将词序列转换为id序列\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "*相反地，也可以通过`tokenizer.convert_ids_to_tokens(ids)`进行逆变换*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['那', '些', '痛', '的', '记', '忆', '，', '落', '在', '春', '的', '泥', '土', '里']\n",
      "那 些 痛 的 记 忆 ， 落 在 春 的 泥 土 里\n"
     ]
    }
   ],
   "source": [
    "# 将id序列转换为token序列\n",
    "tokens = tokenizer.convert_ids_to_tokens(ids)\n",
    "print(tokens)\n",
    "\n",
    "# 将token序列转换为string\n",
    "str_sen = tokenizer.convert_tokens_to_string(tokens)\n",
    "print(str_sen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Step3. 利用`tokenizer.encode()`快捷转换"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**作用：token -> id**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 6929, 763, 4578, 4638, 6381, 2554, 8024, 5862, 1762, 3217, 4638, 3799, 1759, 7027, 102]\n"
     ]
    }
   ],
   "source": [
    "# 将字符串转换为id序列，又称之为编码\n",
    "ids = tokenizer.encode(demo, add_special_tokens=True)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "注：\n",
    "1. 若和上面的`tokenizer.convert_to_ids()`同时使用，会发现`tokenizer.encode()`的结果会在前后加上`[CLS]`和`[SEP]`，也就是输出中的[101],[102]. 其本身的目的是为了方便进行模型输入的构建。\n",
    "2. 若不希望加入`[CLS]`和`[SEP]`，可以通过`add_special_tokens=False`进行设置。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "相对应，可以通过`tokenizer.decode()`进行逆变换\n",
    "\n",
    "**作用：id -> token**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] 那 些 痛 的 记 忆 ， 落 在 春 的 泥 土 里 [SEP]\n"
     ]
    }
   ],
   "source": [
    "# 将id序列转换为字符串，又称之为解码\n",
    "str_sen = tokenizer.decode(ids, skip_special_tokens=False)\n",
    "print(str_sen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Step4. 填充与截断"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "说明：在进行分词的过程中，会有些句子过长或过短，因此需要进行填充与截断以得到长度适中的数据。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**填充**\n",
    "\n",
    "利用`tokenizer.encode(..., padding = \"max_length\", max_length = )` 进行填充"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 6929, 763, 4578, 4638, 6381, 2554, 8024, 5862, 1762, 3217, 4638, 3799, 1759, 7027, 102, 0, 0, 0, 0]\n",
      "[CLS] 那 些 痛 的 记 忆 ， 落 在 春 的 泥 土 里 [SEP] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "# 填充\n",
    "ids = tokenizer.encode(demo, padding=\"max_length\", max_length=20)\n",
    "print(ids)\n",
    "print(tokenizer.decode(ids, skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "*注：观察上面的输出可以发现最后几个位置为0，如果decode回去会发现这里是`[PAD]`，也就是填充的位置。*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**截断**\n",
    "\n",
    "利用`tokenizer.encode(..., truncation = True, max_length = )` 进行截断"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 6929, 763, 4578, 102]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 截断\n",
    "ids = tokenizer.encode(demo, max_length=5, truncation=True)\n",
    "ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Attention Mask**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "对于某些模型来说，单纯的填充是不够的，需要另外建立一个attention_mask的bool数组来指示那部分是真实输出，那部分是填充内容。\n",
    "\n",
    "这一功能在调用`tokenizer.encode_plus`时，会自动完成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 6929, 763, 4578, 4638, 6381, 2554, 8024, 5862, 1762, 3217, 4638, 3799, 1759, 7027, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer.encode_plus(demo, padding=\"max_length\", max_length=15)\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "等价地，上述全部流程最后被完全封装进了`tokenizer`中，因此在实际操作中，可以直接调用`tokenizer`进行数据预处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 6929, 763, 4578, 4638, 6381, 2554, 8024, 5862, 1762, 3217, 4638, 3799, 1759, 7027, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(demo, padding=\"max_length\", max_length=15)\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Step 5. Batch数据的处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "`tokenizer()`支持对batch数据的处理，只需要将数据以list的形式传入即可。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[101, 6929, 763, 4578, 4638, 6381, 2554, 102], [101, 5862, 1762, 3217, 4638, 3799, 1759, 7027, 102], [101, 3996, 1075, 749, 1920, 1765, 8024, 2458, 1139, 678, 671, 702, 5709, 2108, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "demo2 = [\"那些痛的记忆\",\"落在春的泥土里\",\"滋养了大地，开出下一个花季\"]\n",
    "res= tokenizer(demo2)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 2.3 Fast&Slow Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "在最开始调用tokenizer的时候，如果输出引入的分词器，可以看到有如下输出结果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertTokenizerFast(name_or_path='uer/roberta-base-finetuned-dianping-chinese', vocab_size=21128, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True)\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "注意到这里我们使用的是`BertTokenizerFast`；因此便引入了Fast 与 Slow Tokenizer的比较。\n",
    "\n",
    "简而言之，Fast Tokenizer是基于Rust语言的实现；而Slow Tokenizer是基于python实现的。\n",
    "\n",
    "在引入tokenizer的时候，可以通过`AutoTokenizer.from_pretrained(...,use_fast = False)`来指定使用Slow Tokenizer。"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Model 模型调用"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.1 模型的加载"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**直接调用python命令**"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, AutoModel, AutoTokenizer\n",
    "# model = AutoModel.from_pretrained(\"hfl/rbt3\", force_download=True)\n",
    "model = AutoModel.from_pretrained(\"hfl/rbt3\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**加载本地下载好的Model**"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = AutoModel.from_pretrained(\"rbt3\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**通过git clone模型仓库**"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!git lfs clone \"https://huggingface.co/hfl/rbt3\" --include=\"*.bin\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "*通过`lfs ... --include`指定需要下载的文档格式*"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.2 模型的config参数"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "config = AutoConfig.from_pretrained(\"hfl/rbt3\")\n",
    "config"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.3 模型调用"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sen = \"充满鲜花的世界到底在哪里\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"hfl/rbt3\")\n",
    "inputs = tokenizer(sen, return_tensors=\"pt\")\n",
    "inputs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = AutoModel.from_pretrained(\"hfl/rbt3\", output_attentions=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "output = model(**inputs)\n",
    "output"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5. Dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5.1 加载在线数据集"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from datasets import *"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Basic Loading"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "[HuggingFace_datasets](https://huggingface.co/datasets)\n",
    "\n",
    "注意，在下载数据集时易出现网络问题，需要科学上网。\n",
    "\n",
    "这里贴一个抱抱脸中国对此的[说明推文 - Datasets 使用小贴士: 探索解决数据集无法下载的问题](https://mp.weixin.qq.com/s/e_Krti4U7TxPuFWmUep80Q)。\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "datasets= load_dataset(\"madao33/new-title-chinese\")\n",
    "print(datasets)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Loading a Sub-dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 通过添加第二个参数的方式可以直接加入一个数据集的子任务数据集\n",
    "boolq_datasets = load_dataset(\"super_glue\",\"boolq\")\n",
    "print(boolq_datasets)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Loading Part of the Dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 只加载训练集/测试集\n",
    "trainset1 = load_dataset(\"madao33/new-title-chinese\",split=\"train\")\n",
    "# 加载指定长度的数据集\n",
    "trainset2 = load_dataset(\"madao33/new-title-chinese\",split=\"train[10:100]\") \n",
    "# 加载指定比例的数据集（这里返回了一个列表）\n",
    "trainset3 = load_dataset(\"madao33/new-title-chinese\",split=[\"train[:50%]\",\"validation[10%:]\"]) # 注意这里只能是%，并且这里分别表示前50%和后10%"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5.2 查看数据集"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "datasets[\"train\"][:2]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "datasets[\"train\"][\"title\"][:3]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "datasets[\"train\"].column_names"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "datasets[\"train\"].features"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5.3 数据集的划分（split）, 选取（select）， 过滤（filter）"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**划分**：将数据集划分为训练集、测试集等"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 首先载入数据集并观察一下，发现这个数据集主要有'question'...'label'共四个feature\n",
    "data = boolq_datasets[\"train\"]\n",
    "print(data,\"\\n\",data[:2])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 可以通过额外添加指令指定数据集划分的比例依据\n",
    "data.train_test_split(test_size = 0.1, stratify_by_column=\"label\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**数据集的选取与过滤**"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "datasets[\"train\"].select([0,1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "这与上面的直接调取数据查看不同，若用上面的方法`datasets[\"train\"][:3]`直接调用的话则会返回一个python的字典数据；而通过`datasets[\"train\"].select(...)`则仍然会返回一个`Dataset`对象，相当于是抽取了一个子集。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "这里数据的筛选还可以通过`filter(lambda example: ...)`函数来进行高级筛选："
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "filtered_data = datasets[\"train\"].filter(lambda example: \"中国\" in example[\"title\"])\n",
    "print(filtered_data[:2])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5.4 数据集的处理（map）"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def add_prefix(example):\n",
    "    example[\"title\"] = \"标题：\" + example[\"title\"]\n",
    "    return example\n",
    "prefix_data = datasets[\"train\"].map(add_prefix)\n",
    "print(prefix_data[:2])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "可以额外声明:\n",
    "1. `.map(...,batched = True)`来进行batch数据的处理\n",
    "2. `.map(...,num_proc = NUM )`来进行多进程处理（NUM为进程数）\n",
    "3. `.map(...,remove_colomns=[...])`来删除某些列"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5.5 在线数据集的保存与加载"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "processed_datasets.save_to_disk(\"...\")\n",
    "processed_dataset.load_from_disk(\"...\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5.6 加载本地数据集"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 直接加载\n",
    "dataset = load_dataset(\"csv\", data_files = \"ADDRESS\", split=\"train\")\n",
    "# 加载csv\n",
    "dataset = Dataset.from_csv(\"ADDRESS\")\n",
    "# 加载一个文件夹下的所有csv文件\n",
    "dataset = load_dataset(\"csv\", data_dir =\"ADDRESS\", split=\"train\")\n",
    "# 通过pandas加载（进行格式转换）\n",
    "import pandas as pd\n",
    "pd_data = pd.read_csv(\"ADDRESS\")\n",
    "dataset = Dataset.from_pandas(pd_data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Practice 1: 利用AutoClass 拆解 Pipeline"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "粗浅的说，Pipeline的构成为：\n",
    "- A Tokenizer instance in charge of mapping raw textual input to token\n",
    "- A Model instance\n",
    "- Some (optional) post processing for enhancing model’s output\n",
    "来源：Pipelines ‒ transformers 3.0.2 documentation\n",
    "\n",
    "因此我们可以将封装好的这个Pipeline进行拆解，分别自行指定Tokenizer、Model来实现对Pipeline的复现。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MyPipelineForSA(object):\n",
    "    def __init__(self, checkpoint):\n",
    "        \"\"\"\n",
    "        初始化 MyPipelineForSA 类的实例\n",
    "        \n",
    "        参数：\n",
    "            - checkpoint：预训练模型的路径或名称\n",
    "        \"\"\"\n",
    "        # 预训练模型的分词器 Tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "        # 预训练模型（含下游模型）Model\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "    \n",
    "    def __call__(self, sent_list):\n",
    "        \"\"\"\n",
    "        对给定的句子列表进行情感分析的预测\n",
    "        \n",
    "        参数：\n",
    "            - sent_list：待分析的句子列表\n",
    "            \n",
    "        返回：\n",
    "            - prediction：包含情感分析结果的字典列表\n",
    "        \"\"\"\n",
    "        # 使用预训练模型的分词器对输入文本进行分词和编码，并进行填充\n",
    "        inputs = self.tokenizer(sent_list, padding=True, return_tensors=\"pt\")\n",
    "        # 使用预训练模型进行情感分析的前向传播计算\n",
    "        outputs = self.model(**inputs)\n",
    "        # 后处理\n",
    "        # 对输出进行 softmax 操作，得到各类别的概率分布\n",
    "        outputs = F.softmax(outputs.logits, dim=1)\n",
    "        # 获取预测结果中每个类别的标签和对应的分数\n",
    "        id2label = self.model.config.id2label\n",
    "        prediction = list(map(lambda x: {\"label\": id2label[x.argmax().item()],\n",
    "                                          \"score\": x.max().item()}, outputs))\n",
    "        return prediction"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 创建 MyPipelineForSA 类的实例，并指定预训练模型的 checkpoint\n",
    "classifier = MyPipelineForSA(checkpoint=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "# 调用实例，对给定的句子列表进行情感分析的预测\n",
    "result = classifier([\"I love HuggingFace!\", \"I hate it so much!\"])\n",
    "print(result)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Practice 2: 模型微调"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**1. 数据集的加载与清洗**"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import pandas as pd\n",
    "# 加载数据集\n",
    "data = pd.read_csv(\"ChnSentiCorp_htl_all.csv\")\n",
    "# 数据清洗\n",
    "data = data.dropna()\n",
    "data.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**2. 创建Dataset**"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    ''' \n",
    "    自定义数据集类，继承自 torch.utils.data.Dataset\n",
    "    '''\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        '''\n",
    "        初始化 MyDataset 类的实例\n",
    "        '''\n",
    "        # 调用父类的初始化方法\n",
    "        super().__init__() \n",
    "        # 加载数据集\n",
    "        self.data = pd.read_csv(\"ChnSentiCorp_htl_all.csv\")\n",
    "        # 数据清洗\n",
    "        self.data = self.data.dropna()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        '''\n",
    "        根据给定的索引index, 返回对应的数据\n",
    "        '''\n",
    "        # 获取该条数据对应的文本和标签\n",
    "        text = self.data.iloc[index][0]\n",
    "        label = self.data.iloc[index][1]\n",
    "        # 返回该条数据\n",
    "        return text, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        '''\n",
    "        返回数据集的总长度\n",
    "        '''\n",
    "        return len(self.data)\n",
    "    \n",
    "dataset = MyDataset()\n",
    "for i in range(5):\n",
    "    print(*dataset[i], sep=\"\\t\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**3. 划分数据集**"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "trainset, validset = random_split(dataset, lengths=[0.1,0.9]) "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**4. 创建DataLoader（数据容器）**"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "trainloader = DataLoader(trainset, batch_size=32, shuffle=True)\n",
    "validloader = DataLoader(validset, batch_size=64, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model 模型调用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 模型的加载"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**直接调用python命令**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at hfl/rbt3 were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig, AutoModel, AutoTokenizer\n",
    "# model = AutoModel.from_pretrained(\"hfl/rbt3\", force_download=True)\n",
    "model = AutoModel.from_pretrained(\"hfl/rbt3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**加载本地下载好的Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModel.from_pretrained(\"rbt3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**通过git clone模型仓库**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git lfs clone \"https://huggingface.co/hfl/rbt3\" --include=\"*.bin\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*通过`lfs ... --include`指定需要下载的文档格式*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 模型的config参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"_name_or_path\": \"hfl/rbt3\",\n",
       "  \"architectures\": [\n",
       "    \"BertForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"directionality\": \"bidi\",\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 3,\n",
       "  \"output_past\": true,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"pooler_fc_size\": 768,\n",
       "  \"pooler_num_attention_heads\": 12,\n",
       "  \"pooler_num_fc_layers\": 3,\n",
       "  \"pooler_size_per_head\": 128,\n",
       "  \"pooler_type\": \"first_token_transform\",\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.30.2\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 21128\n",
       "}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = AutoConfig.from_pretrained(\"hfl/rbt3\")\n",
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 模型调用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 1041, 4007, 7831, 5709, 4638,  686, 4518, 1168, 2419, 1762, 1525,\n",
       "         7027,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sen = \"充满鲜花的世界到底在哪里\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"hfl/rbt3\")\n",
    "inputs = tokenizer(sen, return_tensors=\"pt\")\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at hfl/rbt3 were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = AutoModel.from_pretrained(\"hfl/rbt3\", output_attentions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 0.1389,  0.2598,  0.7094,  ...,  0.5497, -0.3638, -0.3704],\n",
       "         [-0.2151, -0.2608,  0.3973,  ..., -0.5889,  0.0549, -0.6675],\n",
       "         [ 0.4479,  0.6136,  0.4378,  ..., -0.3829,  0.1403, -0.1688],\n",
       "         ...,\n",
       "         [-0.2985,  1.0488,  0.4977,  ..., -0.3147,  0.2493,  0.3804],\n",
       "         [ 0.0230,  0.3447, -0.0087,  ..., -0.2531,  0.2566,  0.2947],\n",
       "         [ 0.1331,  0.2608,  0.7080,  ...,  0.5577, -0.3658, -0.3668]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-1.4063e-01, -9.9451e-01, -9.9995e-01, -8.4820e-01,  3.9574e-01,\n",
       "         -5.0347e-01, -1.8221e-01, -3.5995e-01,  7.5781e-01,  9.9639e-01,\n",
       "          9.6377e-02, -1.0000e+00, -4.0018e-01,  9.9965e-01, -9.9999e-01,\n",
       "          9.9979e-01,  9.6614e-01,  9.7879e-01, -9.9583e-01,  1.0068e-01,\n",
       "         -9.9609e-01, -6.1354e-01,  2.4066e-01,  9.9634e-01,  9.7757e-01,\n",
       "         -9.9047e-01, -9.9921e-01, -1.3862e-01, -9.6030e-01, -9.9838e-01,\n",
       "         -8.2831e-01, -9.9971e-01,  6.5882e-02, -2.5917e-01,  9.7350e-01,\n",
       "         -1.0955e-01,  8.0674e-04, -6.4379e-01, -9.7473e-01, -9.9587e-01,\n",
       "         -1.9262e-01,  9.7777e-01, -2.6452e-01,  9.9886e-01, -6.9075e-02,\n",
       "          1.5614e-01,  9.9989e-01,  9.8846e-01, -3.6739e-01, -9.2930e-01,\n",
       "          1.2949e-02, -3.0651e-01, -8.6668e-01,  9.8346e-01, -2.9200e-01,\n",
       "          1.9615e-01,  9.8839e-01, -9.9994e-01, -9.9883e-01,  8.7925e-01,\n",
       "         -9.9943e-01,  9.9155e-01,  9.9599e-01,  9.7877e-01,  1.9747e-01,\n",
       "          9.9691e-01,  9.9596e-01,  8.7949e-01, -1.6544e-01, -9.9996e-01,\n",
       "         -1.7405e-01, -7.5938e-01, -9.9959e-01,  1.0596e-02, -2.3614e-01,\n",
       "         -9.9191e-01,  9.9983e-01,  1.4899e-01,  9.9922e-01,  3.2912e-01,\n",
       "         -9.9940e-01,  2.8068e-01, -1.0011e-01,  2.4835e-01,  9.9540e-01,\n",
       "          9.9996e-01, -2.6162e-01, -8.6140e-01, -8.2316e-02, -9.9934e-01,\n",
       "         -2.8744e-01,  9.9399e-01,  9.9997e-01, -9.9277e-01,  9.9986e-01,\n",
       "         -7.2464e-01, -1.4603e-01,  7.2059e-02, -9.8713e-01,  8.6783e-01,\n",
       "         -3.0634e-01, -6.1843e-02,  9.9998e-01,  9.7734e-01,  1.1490e-01,\n",
       "         -9.9773e-01, -9.8997e-01,  9.8467e-01, -9.8235e-01, -1.5611e-01,\n",
       "          9.9999e-01,  4.0015e-01,  1.0000e+00,  9.9662e-01,  9.9944e-01,\n",
       "         -9.9906e-01, -2.8592e-01, -3.4781e-02, -9.9926e-01,  9.9867e-01,\n",
       "         -9.7968e-01,  6.7041e-01, -7.0561e-01,  1.6643e-01, -3.1491e-02,\n",
       "         -9.9961e-01,  1.1175e-01,  3.7749e-02, -9.7743e-01, -9.9498e-01,\n",
       "         -9.9092e-01, -9.9859e-01,  9.8945e-01,  9.7294e-01,  4.9780e-01,\n",
       "         -5.4456e-01, -2.4110e-02, -4.2214e-01, -9.9991e-01, -9.9509e-01,\n",
       "         -9.9995e-01,  8.9680e-01, -2.1783e-01,  9.7491e-01, -9.7987e-01,\n",
       "          9.9526e-01, -9.9614e-01,  9.9915e-01,  9.9835e-01,  1.8279e-01,\n",
       "         -3.6607e-01, -6.6475e-02, -9.9063e-01,  1.3636e-01, -1.9187e-01,\n",
       "          9.6973e-01,  9.9495e-01,  5.4517e-01, -9.5739e-01,  9.9993e-01,\n",
       "         -9.9645e-01,  5.1877e-01,  2.2814e-01,  9.7052e-01,  1.0000e+00,\n",
       "         -9.9997e-01, -3.2279e-02, -9.9999e-01,  8.0340e-01, -2.5824e-01,\n",
       "          9.9904e-01,  9.9217e-01,  9.3548e-01,  9.9697e-01, -2.0648e-02,\n",
       "         -9.9940e-01,  5.2216e-01, -9.9949e-01,  7.8518e-01,  9.9997e-01,\n",
       "          3.6543e-01, -3.9956e-02,  9.9996e-01, -6.1936e-01,  9.3934e-01,\n",
       "         -4.1105e-02,  8.4905e-02, -9.9459e-01,  2.7417e-01, -4.3812e-04,\n",
       "          9.5316e-01, -9.1963e-01,  3.7576e-01,  4.1502e-01,  1.1503e-01,\n",
       "          3.5468e-02, -9.8965e-01, -9.9859e-01,  9.9940e-01,  9.7919e-01,\n",
       "          6.4261e-01, -2.2353e-01,  9.9998e-01,  3.2043e-01,  9.9924e-01,\n",
       "          3.1156e-01,  8.4560e-01, -5.1390e-01,  9.8435e-01,  6.8223e-02,\n",
       "          6.2153e-01, -1.1336e-01,  9.9983e-01, -8.4321e-01, -9.9933e-01,\n",
       "          1.3252e-01, -7.0528e-01, -2.2470e-01, -9.9305e-01, -2.8116e-01,\n",
       "         -1.1257e-01,  9.9943e-01, -3.1116e-01, -7.5558e-01,  9.4658e-01,\n",
       "         -9.9664e-01,  1.1877e-01, -9.9995e-01, -9.9385e-01,  9.9893e-01,\n",
       "         -2.1677e-01, -1.0000e+00,  8.3211e-01,  9.9993e-01, -8.1706e-01,\n",
       "          9.9871e-01,  9.4426e-02, -9.9881e-01, -6.7999e-04, -3.5723e-01,\n",
       "         -1.0000e+00, -9.8824e-01, -9.9998e-01, -8.0851e-01,  1.3033e-01,\n",
       "          9.6120e-01, -9.9998e-01,  1.4790e-01, -9.9980e-01,  9.9035e-01,\n",
       "          9.9972e-01,  2.9053e-02, -3.4656e-01,  9.9997e-01,  2.0320e-01,\n",
       "         -1.6226e-01, -3.5063e-01,  1.6760e-01,  1.4113e-01,  6.7519e-01,\n",
       "         -1.0080e-01,  9.9779e-01, -9.9942e-01, -2.1528e-01, -3.1575e-01,\n",
       "         -9.9983e-01,  7.0913e-03,  9.1796e-01, -8.7557e-02, -6.4330e-02,\n",
       "          8.7479e-03,  2.1146e-01,  9.7539e-01,  9.8852e-01,  9.9999e-01,\n",
       "          9.6419e-01,  9.9994e-01,  9.9995e-01, -5.3390e-02, -9.8973e-01,\n",
       "         -4.2281e-01, -1.4561e-01, -9.9961e-01, -9.3230e-01, -9.9556e-01,\n",
       "          9.6840e-01, -1.9719e-01,  1.0000e+00, -9.9996e-01,  9.9996e-01,\n",
       "         -8.8144e-01,  2.8713e-01,  7.9789e-02, -7.5939e-02, -6.4828e-02,\n",
       "          1.1863e-01,  9.4943e-01, -1.9688e-02,  9.4466e-01,  9.8227e-01,\n",
       "         -5.0938e-01, -1.1236e-02,  5.5613e-01, -1.7696e-02,  9.4170e-01,\n",
       "         -7.9218e-01,  9.7407e-01,  1.5749e-01,  9.9225e-01,  5.8802e-01,\n",
       "         -9.9998e-01,  9.9802e-01, -9.9845e-01, -3.2877e-01, -9.9944e-01,\n",
       "         -9.9060e-01, -1.0326e-01, -9.9892e-01,  2.8974e-01,  9.5189e-01,\n",
       "          6.9147e-02,  4.9127e-01, -9.9921e-01, -8.6631e-01,  8.5233e-01,\n",
       "          9.9760e-01, -9.9995e-01,  9.6020e-01,  9.9132e-01, -9.0448e-01,\n",
       "         -7.0910e-02,  8.7370e-01, -9.6550e-01,  9.9777e-01, -9.9992e-01,\n",
       "          8.1169e-02,  9.8330e-01,  2.9378e-01, -9.9909e-01, -3.0845e-01,\n",
       "         -5.7776e-02,  2.3462e-01, -5.6809e-02,  9.9897e-01,  9.1558e-02,\n",
       "         -1.9487e-02, -9.9984e-01, -9.6202e-01, -4.4025e-01, -2.4167e-01,\n",
       "          9.8762e-01, -9.9987e-01, -2.2499e-01,  9.9743e-01, -9.4785e-01,\n",
       "          2.5627e-01, -3.9827e-01, -2.3373e-01,  1.3497e-02, -9.7817e-01,\n",
       "         -9.9940e-01, -9.9337e-01,  9.9969e-01, -1.3915e-01,  1.0853e-02,\n",
       "          9.9650e-01,  9.9941e-01,  9.9950e-01, -9.4693e-01,  9.7093e-01,\n",
       "          9.9899e-01,  1.6367e-01, -4.1227e-01, -7.6146e-01, -2.3761e-01,\n",
       "         -5.3453e-01, -9.4692e-01,  1.5095e-01,  8.2981e-01, -2.7513e-01,\n",
       "         -9.7936e-01,  9.9879e-01,  9.9898e-01,  9.9998e-01, -5.7959e-02,\n",
       "         -7.3505e-01,  6.6825e-01, -9.5569e-01, -9.9281e-01, -1.5214e-01,\n",
       "          9.8766e-01, -9.9824e-01, -2.4025e-01,  5.9945e-01, -8.8889e-01,\n",
       "          9.9400e-01, -9.8694e-01,  2.2081e-01, -9.9999e-01, -9.9963e-01,\n",
       "         -9.9999e-01,  9.9996e-01, -1.6518e-01, -2.1280e-02, -9.9893e-01,\n",
       "          9.9994e-01,  9.8438e-01, -9.6176e-01, -2.4150e-01,  9.9892e-01,\n",
       "         -3.1196e-01,  6.8802e-02, -9.9998e-01, -9.9846e-01,  9.6030e-01,\n",
       "         -1.4239e-01,  9.9786e-01,  5.8693e-01, -9.8335e-01,  8.7246e-01,\n",
       "          9.6380e-01, -1.7335e-01, -9.7861e-01, -9.7318e-01, -2.1126e-01,\n",
       "          9.6908e-02, -3.8804e-01, -2.7792e-01,  4.9500e-01,  9.9786e-01,\n",
       "          1.2434e-01,  7.1606e-03,  4.9081e-02,  9.9995e-01,  4.7179e-01,\n",
       "         -9.9781e-01, -5.5224e-01, -7.5089e-02, -9.0299e-01, -9.7771e-01,\n",
       "         -9.9978e-01,  2.5768e-01,  1.4618e-01,  2.3048e-01, -9.4007e-01,\n",
       "         -9.9289e-01, -9.9728e-01,  4.6005e-01, -9.8909e-01, -9.8247e-01,\n",
       "         -1.1271e-01, -9.9924e-01, -9.5878e-01,  9.9618e-01, -9.9833e-01,\n",
       "          9.0430e-02,  8.5646e-01,  8.6650e-01, -9.9975e-01, -8.7476e-02,\n",
       "          9.8179e-01, -9.4415e-01, -8.1219e-02, -9.3142e-01, -8.8925e-02,\n",
       "         -7.3599e-01, -9.9988e-01,  3.8976e-02,  9.9892e-01,  9.9829e-01,\n",
       "          9.7336e-01,  9.5396e-01, -5.7765e-01,  9.2538e-01,  9.8345e-01,\n",
       "          9.9993e-01, -1.1011e-01, -1.4795e-01, -9.9992e-01, -2.1478e-01,\n",
       "          9.2638e-01,  1.6122e-01,  8.1283e-01, -9.9872e-01, -3.0464e-01,\n",
       "         -5.3351e-01, -2.8446e-02,  1.0000e+00,  8.7065e-01, -2.2922e-01,\n",
       "         -9.9999e-01,  9.8504e-02, -2.6696e-01, -2.3244e-01, -7.3800e-01,\n",
       "          3.8354e-01,  9.9999e-01, -9.9590e-01, -4.3010e-02, -9.9163e-01,\n",
       "         -9.9884e-01,  9.9997e-01, -9.9989e-01,  9.9863e-01,  9.7555e-01,\n",
       "         -9.8051e-01,  8.7039e-02, -2.9267e-01, -3.4146e-02,  8.4733e-02,\n",
       "         -1.6960e-02,  5.2536e-01,  9.1152e-02, -9.9996e-01, -1.6194e-01,\n",
       "          9.4191e-01, -1.2117e-01, -8.0590e-01, -9.8599e-01, -4.1122e-02,\n",
       "          9.9305e-01, -9.9921e-01, -9.9949e-01, -3.7270e-01,  8.8244e-02,\n",
       "          3.3418e-01, -1.0661e-01,  9.1298e-02,  2.2855e-01, -4.8751e-01,\n",
       "          3.5779e-02,  8.8763e-01, -8.4528e-01,  7.9064e-01, -7.9299e-01,\n",
       "         -9.1948e-01,  4.6812e-02,  9.9863e-01,  9.9955e-01, -9.9896e-01,\n",
       "         -9.9856e-01,  2.8395e-01,  2.1428e-02,  7.1750e-02,  9.9848e-01,\n",
       "         -1.2502e-01, -9.7974e-01,  6.4612e-03,  1.0404e-01, -8.3275e-02,\n",
       "         -4.9344e-01,  9.9999e-01, -9.9501e-01,  1.0000e+00, -9.9995e-01,\n",
       "          2.2734e-01,  1.4793e-03,  9.9987e-01, -9.9995e-01, -6.3180e-02,\n",
       "          9.9303e-01, -9.9973e-01, -3.5192e-01, -9.7621e-01,  8.8257e-01,\n",
       "         -1.9648e-01, -6.3609e-02,  6.5017e-01, -9.6388e-01, -7.7814e-02,\n",
       "         -9.9629e-01,  2.5712e-01,  9.6877e-01, -9.9914e-01, -6.4946e-01,\n",
       "         -9.9991e-01,  3.5734e-02,  1.9796e-01, -9.9962e-01,  7.7841e-01,\n",
       "          9.9989e-01,  4.6515e-02,  8.6736e-01, -9.9821e-01,  5.0398e-02,\n",
       "          1.1542e-01, -9.9070e-01,  7.8415e-03, -9.9920e-01,  6.6010e-01,\n",
       "         -9.7165e-01,  8.8524e-01, -9.9978e-01,  9.6270e-01,  8.6094e-01,\n",
       "         -4.4257e-01, -3.5038e-01,  2.6261e-03, -3.3917e-01, -9.9992e-01,\n",
       "          3.8141e-01, -9.9663e-01, -9.9948e-01, -5.3038e-01,  9.7117e-01,\n",
       "          8.8520e-01,  3.2873e-02,  9.9563e-01, -9.9489e-01,  1.0688e-01,\n",
       "         -2.7265e-01,  8.4171e-01,  1.0000e+00, -9.9328e-01, -9.9847e-01,\n",
       "          7.7954e-01, -9.9935e-01, -7.7084e-01,  1.0000e+00,  2.9522e-01,\n",
       "          9.9880e-01,  2.1176e-01, -9.6838e-01,  4.5931e-01,  2.2745e-01,\n",
       "          9.7224e-01, -2.5886e-01,  1.6438e-01,  9.9265e-01,  2.6835e-01,\n",
       "          1.3156e-01, -6.8207e-01,  9.8533e-01,  1.0185e-01, -7.3583e-02,\n",
       "          9.0911e-01, -9.3944e-01, -9.9304e-01, -9.9585e-01,  1.3368e-02,\n",
       "         -5.0449e-01, -1.9416e-01, -2.8329e-01,  8.2712e-01,  9.9976e-01,\n",
       "         -9.9967e-01,  9.1945e-01, -1.0000e+00, -9.9988e-01,  2.6670e-01,\n",
       "          4.4308e-02,  9.9732e-01,  2.6647e-01,  2.6547e-02, -2.2722e-01,\n",
       "         -9.4955e-01,  9.3128e-01, -9.9363e-01,  9.8606e-01, -1.3056e-01,\n",
       "          6.3806e-02,  9.9887e-01,  9.5349e-01,  1.9055e-02, -7.9508e-01,\n",
       "         -9.6415e-01,  1.1802e-01, -9.9515e-01,  9.9957e-01, -1.5373e-01,\n",
       "          4.6966e-02, -2.8084e-01, -9.8639e-03, -9.9649e-01, -9.9240e-01,\n",
       "          1.2427e-01,  9.8108e-01, -9.9951e-01,  9.9767e-01, -8.0492e-01,\n",
       "          9.8272e-01,  9.8678e-01,  1.0000e+00, -2.7015e-02,  5.8864e-01,\n",
       "         -9.9831e-01, -9.7882e-01,  9.9764e-01,  9.2152e-01,  9.9995e-01,\n",
       "          5.0687e-01,  9.1579e-01,  1.0969e-01, -9.9926e-01,  9.5815e-01,\n",
       "         -3.5082e-01, -8.7895e-02,  2.1467e-01, -9.6995e-01, -9.9987e-01,\n",
       "          9.9979e-01, -9.9996e-01, -9.9931e-01, -9.3583e-01, -9.9979e-01,\n",
       "          9.9459e-01,  5.0022e-01,  9.9742e-01,  6.6836e-01, -9.9828e-01,\n",
       "         -9.6559e-01, -1.2796e-01, -9.6678e-01, -9.1287e-01,  3.2972e-01,\n",
       "         -9.9999e-01, -6.8072e-02,  1.2889e-01, -7.0239e-01, -2.2057e-01,\n",
       "         -1.2654e-01,  5.1604e-01,  9.6687e-01,  4.8816e-01,  9.5910e-01,\n",
       "         -9.8927e-01, -9.9316e-01,  3.7848e-01, -1.0000e+00,  8.8190e-01,\n",
       "          9.9893e-01, -3.5254e-01,  4.7892e-02, -6.0605e-01, -1.9460e-01,\n",
       "         -9.9991e-01, -1.0000e+00,  9.4710e-01,  9.9993e-01,  2.5418e-01,\n",
       "         -9.9722e-01,  3.4602e-01, -9.9866e-01, -5.2500e-01,  7.8256e-01,\n",
       "          9.8633e-01, -9.9594e-01,  9.8155e-01, -5.6021e-01,  3.2908e-02,\n",
       "          9.5550e-01, -1.0000e+00,  3.3207e-02, -9.9991e-01,  9.9974e-01,\n",
       "         -1.0000e+00,  9.9406e-01, -3.2122e-01,  2.2475e-01,  1.7691e-02,\n",
       "          9.0121e-01, -9.9985e-01,  3.4553e-02,  9.6976e-01,  7.1243e-01,\n",
       "         -2.8234e-02,  9.9895e-01, -5.7258e-02]], grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=(tensor([[[[4.7035e-01, 1.5430e-04, 1.7125e-04,  ..., 2.2956e-04,\n",
       "           3.2780e-04, 5.2656e-01],\n",
       "          [5.8283e-03, 2.3563e-02, 8.9646e-02,  ..., 2.6164e-01,\n",
       "           5.2026e-02, 1.2151e-02],\n",
       "          [1.3539e-02, 4.4759e-02, 3.6907e-02,  ..., 2.3830e-01,\n",
       "           1.4195e-01, 7.0815e-03],\n",
       "          ...,\n",
       "          [3.5664e-02, 1.5121e-01, 1.0279e-01,  ..., 1.8083e-01,\n",
       "           5.1442e-02, 1.3592e-02],\n",
       "          [1.3234e-02, 6.2936e-02, 2.5070e-02,  ..., 2.3566e-01,\n",
       "           1.2692e-01, 6.8151e-03],\n",
       "          [4.4300e-01, 2.0296e-04, 4.2354e-04,  ..., 6.0615e-04,\n",
       "           5.5631e-04, 5.5266e-01]],\n",
       "\n",
       "         [[9.9291e-01, 2.4462e-05, 2.7456e-05,  ..., 1.4319e-04,\n",
       "           6.3476e-05, 2.7523e-03],\n",
       "          [1.5024e-02, 8.4193e-08, 9.8490e-01,  ..., 4.2279e-09,\n",
       "           4.7679e-07, 3.9398e-06],\n",
       "          [1.5955e-02, 9.5890e-01, 2.2059e-04,  ..., 1.6505e-05,\n",
       "           3.2658e-09, 7.7935e-05],\n",
       "          ...,\n",
       "          [1.9245e-01, 5.6673e-08, 1.5455e-04,  ..., 4.1222e-03,\n",
       "           1.5349e-02, 1.1576e-02],\n",
       "          [2.3818e-02, 5.2617e-07, 2.3124e-08,  ..., 9.4588e-01,\n",
       "           2.2954e-06, 2.9986e-02],\n",
       "          [9.5187e-01, 1.5701e-06, 1.1059e-04,  ..., 4.1646e-04,\n",
       "           4.1344e-02, 5.1002e-03]],\n",
       "\n",
       "         [[1.8782e-01, 7.1126e-02, 6.7147e-02,  ..., 2.7628e-02,\n",
       "           2.6333e-02, 2.3435e-01],\n",
       "          [5.3616e-01, 1.8188e-01, 3.5393e-02,  ..., 9.1398e-03,\n",
       "           7.2134e-03, 1.5139e-01],\n",
       "          [4.6626e-01, 1.4677e-01, 2.3470e-01,  ..., 2.1056e-03,\n",
       "           2.2415e-03, 9.6302e-02],\n",
       "          ...,\n",
       "          [1.0930e-01, 2.0342e-02, 2.9442e-02,  ..., 7.7984e-02,\n",
       "           6.1986e-03, 4.8876e-02],\n",
       "          [3.5699e-02, 7.3967e-03, 8.3075e-03,  ..., 9.4315e-03,\n",
       "           2.1355e-02, 1.6524e-02],\n",
       "          [4.4255e-02, 1.6442e-02, 3.7247e-02,  ..., 9.3709e-02,\n",
       "           4.4195e-02, 1.4992e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[7.0107e-01, 1.0484e-02, 1.0572e-02,  ..., 4.2118e-02,\n",
       "           4.0057e-02, 6.8761e-02],\n",
       "          [4.0248e-01, 2.7270e-01, 1.2157e-02,  ..., 2.7764e-02,\n",
       "           4.7843e-02, 8.8753e-02],\n",
       "          [4.6369e-01, 1.9496e-02, 2.5813e-01,  ..., 2.6242e-02,\n",
       "           2.8069e-02, 8.7077e-02],\n",
       "          ...,\n",
       "          [4.1181e-01, 3.2080e-02, 1.7354e-02,  ..., 1.7052e-01,\n",
       "           1.6159e-02, 1.5380e-01],\n",
       "          [4.6064e-01, 2.0449e-02, 2.9882e-02,  ..., 4.9854e-02,\n",
       "           7.1038e-03, 1.4036e-01],\n",
       "          [3.3891e-01, 4.8548e-02, 6.7335e-02,  ..., 3.5295e-02,\n",
       "           2.5955e-02, 1.1858e-01]],\n",
       "\n",
       "         [[9.6247e-01, 1.2019e-02, 5.1308e-03,  ..., 1.5314e-03,\n",
       "           1.4968e-03, 2.8806e-03],\n",
       "          [6.1374e-03, 1.3441e-02, 9.0664e-01,  ..., 1.1627e-03,\n",
       "           7.7301e-04, 5.5713e-04],\n",
       "          [1.8902e-02, 1.8716e-02, 1.0491e-01,  ..., 8.9452e-04,\n",
       "           1.1320e-03, 1.3454e-02],\n",
       "          ...,\n",
       "          [5.2498e-03, 5.7765e-04, 2.2261e-03,  ..., 1.4101e-02,\n",
       "           1.9526e-01, 7.5667e-01],\n",
       "          [2.1934e-03, 2.8270e-05, 3.0057e-05,  ..., 1.7240e-03,\n",
       "           1.8040e-02, 9.7345e-01],\n",
       "          [9.9479e-01, 2.3554e-04, 3.3658e-05,  ..., 1.2287e-04,\n",
       "           2.6802e-04, 4.3510e-03]],\n",
       "\n",
       "         [[4.5382e-01, 2.9556e-02, 3.2806e-02,  ..., 2.3219e-02,\n",
       "           1.4113e-02, 2.7707e-01],\n",
       "          [8.5325e-01, 4.6501e-02, 1.2482e-02,  ..., 3.3738e-03,\n",
       "           1.4954e-03, 2.0690e-02],\n",
       "          [4.8797e-02, 9.1602e-01, 5.3934e-03,  ..., 1.6820e-03,\n",
       "           4.9408e-05, 4.0092e-04],\n",
       "          ...,\n",
       "          [1.8912e-01, 1.1305e-03, 2.9976e-03,  ..., 1.6919e-02,\n",
       "           1.5608e-02, 6.1063e-02],\n",
       "          [1.5262e-02, 5.2586e-03, 4.0634e-04,  ..., 8.9370e-01,\n",
       "           5.2268e-03, 8.1875e-03],\n",
       "          [1.0443e-01, 3.9989e-04, 2.6803e-03,  ..., 7.1636e-02,\n",
       "           1.6339e-01, 6.3452e-01]]]], grad_fn=<SoftmaxBackward0>), tensor([[[[4.4297e-01, 4.1505e-03, 8.2753e-03,  ..., 1.4513e-02,\n",
       "           2.3651e-02, 4.3092e-01],\n",
       "          [4.8905e-01, 8.8355e-03, 1.1187e-02,  ..., 4.3685e-05,\n",
       "           7.8192e-05, 4.8014e-01],\n",
       "          [7.3948e-02, 8.5266e-01, 5.6965e-05,  ..., 1.7230e-04,\n",
       "           1.9155e-07, 7.1809e-02],\n",
       "          ...,\n",
       "          [4.9329e-02, 1.9964e-05, 4.6443e-05,  ..., 5.5979e-04,\n",
       "           7.8736e-03, 4.8843e-02],\n",
       "          [1.1928e-01, 2.5915e-03, 8.5529e-06,  ..., 7.4861e-01,\n",
       "           2.2104e-03, 1.2170e-01],\n",
       "          [4.4361e-01, 3.9714e-03, 8.3366e-03,  ..., 1.3516e-02,\n",
       "           2.4646e-02, 4.3133e-01]],\n",
       "\n",
       "         [[4.6034e-01, 7.7966e-03, 5.9851e-03,  ..., 2.2457e-02,\n",
       "           5.3028e-03, 4.5285e-01],\n",
       "          [2.0691e-01, 4.1133e-03, 5.8409e-03,  ..., 1.7138e-01,\n",
       "           4.4872e-02, 2.0910e-01],\n",
       "          [1.2601e-01, 1.5253e-02, 1.6744e-02,  ..., 1.7413e-01,\n",
       "           6.0181e-02, 1.2756e-01],\n",
       "          ...,\n",
       "          [2.5772e-01, 1.0856e-02, 8.1338e-03,  ..., 1.9795e-01,\n",
       "           1.2480e-01, 2.6174e-01],\n",
       "          [2.8721e-01, 1.9525e-02, 1.6833e-02,  ..., 9.7633e-02,\n",
       "           6.0656e-02, 2.9166e-01],\n",
       "          [4.6210e-01, 7.1363e-03, 5.5820e-03,  ..., 2.1750e-02,\n",
       "           5.0757e-03, 4.5494e-01]],\n",
       "\n",
       "         [[4.9869e-01, 2.0316e-03, 2.8486e-03,  ..., 9.3867e-03,\n",
       "           2.8188e-03, 4.5706e-01],\n",
       "          [4.4250e-01, 8.1934e-02, 1.3818e-02,  ..., 1.5733e-03,\n",
       "           1.4506e-03, 4.3348e-01],\n",
       "          [4.5430e-01, 2.6632e-02, 5.3002e-02,  ..., 1.2306e-03,\n",
       "           3.5784e-03, 4.3706e-01],\n",
       "          ...,\n",
       "          [3.3562e-01, 4.5733e-04, 8.7943e-04,  ..., 2.4108e-01,\n",
       "           2.8964e-02, 3.2146e-01],\n",
       "          [3.5915e-01, 1.4712e-03, 1.2364e-03,  ..., 9.9484e-02,\n",
       "           1.1453e-01, 3.4624e-01],\n",
       "          [4.9839e-01, 2.0468e-03, 2.8726e-03,  ..., 9.6543e-03,\n",
       "           2.9183e-03, 4.5621e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[4.4760e-01, 1.1148e-02, 1.0810e-02,  ..., 1.1956e-02,\n",
       "           1.2713e-02, 4.2723e-01],\n",
       "          [1.2753e-01, 1.1786e-02, 1.2885e-02,  ..., 1.5180e-02,\n",
       "           1.1353e-02, 1.3156e-01],\n",
       "          [1.4326e-01, 5.7716e-03, 1.1376e-02,  ..., 3.2309e-02,\n",
       "           1.8556e-02, 1.4967e-01],\n",
       "          ...,\n",
       "          [3.4973e-01, 1.0452e-02, 7.2104e-03,  ..., 9.5135e-02,\n",
       "           7.3154e-02, 3.6016e-01],\n",
       "          [4.4301e-01, 1.5748e-02, 1.4927e-02,  ..., 6.4395e-03,\n",
       "           1.1044e-02, 4.3702e-01],\n",
       "          [4.4805e-01, 1.0848e-02, 1.0644e-02,  ..., 1.1850e-02,\n",
       "           1.2524e-02, 4.2817e-01]],\n",
       "\n",
       "         [[4.8558e-01, 5.2666e-03, 6.2241e-03,  ..., 6.3983e-03,\n",
       "           5.8310e-03, 4.6278e-01],\n",
       "          [4.3249e-01, 8.2565e-03, 7.1941e-03,  ..., 3.6066e-02,\n",
       "           1.0110e-02, 4.2281e-01],\n",
       "          [4.2018e-01, 2.3621e-02, 1.5045e-02,  ..., 2.8316e-02,\n",
       "           1.1713e-02, 4.1533e-01],\n",
       "          ...,\n",
       "          [5.0287e-02, 1.1523e-01, 1.1108e-01,  ..., 2.6369e-02,\n",
       "           1.2409e-02, 5.0326e-02],\n",
       "          [1.0371e-01, 6.4636e-02, 4.9482e-02,  ..., 2.4502e-02,\n",
       "           3.0271e-02, 1.0455e-01],\n",
       "          [4.8491e-01, 5.3393e-03, 6.3589e-03,  ..., 6.5022e-03,\n",
       "           5.7956e-03, 4.6286e-01]],\n",
       "\n",
       "         [[4.7228e-01, 6.6050e-03, 9.8667e-04,  ..., 5.2311e-03,\n",
       "           1.2080e-03, 4.5931e-01],\n",
       "          [2.1762e-01, 7.6164e-02, 1.2125e-01,  ..., 3.1648e-02,\n",
       "           2.2045e-02, 2.1018e-01],\n",
       "          [1.6495e-01, 1.3597e-01, 9.4501e-02,  ..., 1.6455e-02,\n",
       "           2.0473e-02, 1.5946e-01],\n",
       "          ...,\n",
       "          [1.9579e-01, 6.1887e-02, 2.1371e-01,  ..., 3.8634e-02,\n",
       "           4.1407e-02, 1.9043e-01],\n",
       "          [2.8053e-01, 5.2608e-02, 6.8512e-02,  ..., 2.2417e-02,\n",
       "           6.5316e-02, 2.8028e-01],\n",
       "          [4.7180e-01, 6.5460e-03, 9.7777e-04,  ..., 5.1156e-03,\n",
       "           1.1905e-03, 4.5913e-01]]]], grad_fn=<SoftmaxBackward0>), tensor([[[[4.1110e-01, 2.2656e-03, 6.3064e-03,  ..., 1.4748e-02,\n",
       "           2.3884e-02, 4.0475e-01],\n",
       "          [4.9172e-01, 8.2216e-03, 4.4449e-03,  ..., 3.7796e-04,\n",
       "           5.3824e-04, 4.8462e-01],\n",
       "          [4.9369e-01, 1.0555e-02, 4.1823e-03,  ..., 2.7265e-04,\n",
       "           4.2874e-04, 4.8469e-01],\n",
       "          ...,\n",
       "          [4.4570e-01, 8.1061e-04, 3.1448e-03,  ..., 5.4753e-03,\n",
       "           2.2297e-03, 4.4116e-01],\n",
       "          [3.5540e-01, 4.2539e-03, 9.6889e-03,  ..., 1.9359e-02,\n",
       "           1.0683e-02, 3.5020e-01],\n",
       "          [4.1145e-01, 2.2595e-03, 6.2832e-03,  ..., 1.4625e-02,\n",
       "           2.3798e-02, 4.0511e-01]],\n",
       "\n",
       "         [[2.7562e-02, 1.7206e-02, 5.6315e-02,  ..., 1.0357e-01,\n",
       "           2.4295e-01, 2.7375e-02],\n",
       "          [3.6644e-01, 3.1871e-02, 1.0469e-01,  ..., 2.8268e-04,\n",
       "           7.7746e-04, 3.5984e-01],\n",
       "          [3.2019e-01, 1.0766e-01, 4.3521e-03,  ..., 1.1850e-04,\n",
       "           5.7558e-05, 3.1646e-01],\n",
       "          ...,\n",
       "          [1.5416e-01, 7.9570e-03, 1.1826e-02,  ..., 9.5646e-03,\n",
       "           3.5576e-01, 1.5267e-01],\n",
       "          [4.4577e-01, 6.9746e-03, 5.6398e-04,  ..., 2.6785e-02,\n",
       "           1.0536e-02, 4.4383e-01],\n",
       "          [2.7581e-02, 1.7260e-02, 5.6413e-02,  ..., 1.0321e-01,\n",
       "           2.4414e-01, 2.7393e-02]],\n",
       "\n",
       "         [[4.8723e-01, 2.0277e-04, 2.4515e-04,  ..., 6.1315e-04,\n",
       "           7.7680e-03, 4.8323e-01],\n",
       "          [4.8767e-01, 4.5456e-03, 6.4000e-03,  ..., 1.3787e-04,\n",
       "           2.4159e-04, 4.8081e-01],\n",
       "          [4.6952e-01, 6.8351e-03, 2.2348e-02,  ..., 2.5455e-04,\n",
       "           8.7050e-04, 4.6303e-01],\n",
       "          ...,\n",
       "          [4.7203e-01, 5.7617e-03, 6.6113e-03,  ..., 3.8476e-03,\n",
       "           7.7448e-03, 4.7030e-01],\n",
       "          [4.4840e-01, 2.6126e-03, 1.6150e-02,  ..., 3.5734e-03,\n",
       "           2.2943e-02, 4.4564e-01],\n",
       "          [4.8715e-01, 2.0494e-04, 2.4790e-04,  ..., 6.2055e-04,\n",
       "           7.8198e-03, 4.8315e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[5.2369e-02, 6.2584e-02, 4.0558e-02,  ..., 3.1097e-02,\n",
       "           5.5026e-02, 5.1797e-02],\n",
       "          [3.3417e-01, 1.3243e-02, 1.6746e-02,  ..., 1.6653e-03,\n",
       "           1.6947e-03, 3.2954e-01],\n",
       "          [3.2986e-01, 1.2934e-02, 1.7195e-02,  ..., 1.9153e-03,\n",
       "           1.7495e-03, 3.2439e-01],\n",
       "          ...,\n",
       "          [4.8369e-01, 7.8902e-04, 3.2934e-04,  ..., 5.1911e-03,\n",
       "           1.2844e-02, 4.7965e-01],\n",
       "          [4.9950e-01, 1.6959e-04, 5.4620e-05,  ..., 5.7023e-04,\n",
       "           1.8110e-03, 4.9495e-01],\n",
       "          [5.2840e-02, 6.2675e-02, 4.0608e-02,  ..., 3.1007e-02,\n",
       "           5.4975e-02, 5.2264e-02]],\n",
       "\n",
       "         [[6.8859e-02, 1.6203e-02, 2.5371e-02,  ..., 9.2022e-02,\n",
       "           2.5524e-01, 6.8515e-02],\n",
       "          [4.9258e-01, 8.9607e-03, 6.4110e-03,  ..., 1.4348e-04,\n",
       "           8.2848e-05, 4.8403e-01],\n",
       "          [4.5925e-01, 6.6277e-02, 8.5969e-03,  ..., 1.8109e-04,\n",
       "           6.4849e-05, 4.5065e-01],\n",
       "          ...,\n",
       "          [2.8102e-01, 3.5403e-03, 5.3313e-03,  ..., 5.9108e-02,\n",
       "           2.1775e-02, 2.7554e-01],\n",
       "          [2.4594e-01, 2.1583e-03, 1.3360e-03,  ..., 6.3860e-02,\n",
       "           4.3246e-02, 2.4208e-01],\n",
       "          [6.8866e-02, 1.6173e-02, 2.5315e-02,  ..., 9.1958e-02,\n",
       "           2.5546e-01, 6.8522e-02]],\n",
       "\n",
       "         [[1.7645e-01, 1.7640e-02, 1.7685e-02,  ..., 1.8129e-02,\n",
       "           6.5621e-02, 1.7295e-01],\n",
       "          [4.4318e-01, 8.2770e-03, 7.1784e-02,  ..., 6.2585e-04,\n",
       "           4.3552e-04, 4.3824e-01],\n",
       "          [3.1853e-01, 1.3835e-02, 2.2655e-02,  ..., 1.1971e-03,\n",
       "           6.3209e-04, 3.1497e-01],\n",
       "          ...,\n",
       "          [2.8703e-01, 1.3852e-03, 3.7609e-03,  ..., 2.6161e-02,\n",
       "           3.5361e-01, 2.8430e-01],\n",
       "          [4.9303e-01, 2.8678e-04, 7.7348e-05,  ..., 2.0570e-03,\n",
       "           4.5610e-03, 4.9138e-01],\n",
       "          [1.7718e-01, 1.7722e-02, 1.7612e-02,  ..., 1.8047e-02,\n",
       "           6.5741e-02, 1.7367e-01]]]], grad_fn=<SoftmaxBackward0>)), cross_attentions=None)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model(**inputs)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 加载在线数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[HuggingFace_datasets](https://huggingface.co/datasets)\n",
    "\n",
    "注意，在下载数据集时易出现网络问题，需要科学上网。\n",
    "\n",
    "这里贴一个抱抱脸中国对此的[说明推文 - Datasets 使用小贴士: 探索解决数据集无法下载的问题](https://mp.weixin.qq.com/s/e_Krti4U7TxPuFWmUep80Q)。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (C:/Users/xinby/.cache/huggingface/datasets/madao33___csv/madao33--new-title-chinese-5c39318853719c17/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "435337a593b547aca6a36ee8adfd496a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['title', 'content'],\n",
      "        num_rows: 5850\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['title', 'content'],\n",
      "        num_rows: 1679\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "datasets= load_dataset(\"madao33/new-title-chinese\")\n",
    "print(datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading a Sub-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset super_glue (C:/Users/xinby/.cache/huggingface/datasets/super_glue/boolq/1.0.3/bb9675f958ebfee0d5d6dc5476fafe38c79123727a7258d515c450873dbdbbed)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02610a0333a44e98aad3fed3b014bf16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['question', 'passage', 'idx', 'label'],\n",
      "        num_rows: 9427\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['question', 'passage', 'idx', 'label'],\n",
      "        num_rows: 3270\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['question', 'passage', 'idx', 'label'],\n",
      "        num_rows: 3245\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# 通过添加第二个参数的方式可以直接加入一个数据集的子任务数据集\n",
    "boolq_datasets = load_dataset(\"super_glue\",\"boolq\")\n",
    "print(boolq_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Part of the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (C:/Users/xinby/.cache/huggingface/datasets/madao33___csv/madao33--new-title-chinese-5c39318853719c17/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d)\n",
      "Found cached dataset csv (C:/Users/xinby/.cache/huggingface/datasets/madao33___csv/madao33--new-title-chinese-5c39318853719c17/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d)\n",
      "Found cached dataset csv (C:/Users/xinby/.cache/huggingface/datasets/madao33___csv/madao33--new-title-chinese-5c39318853719c17/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83f2ec0648344b24ade811f5046f5921",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 只加载训练集/测试集\n",
    "trainset1 = load_dataset(\"madao33/new-title-chinese\",split=\"train\")\n",
    "# 加载指定长度的数据集\n",
    "trainset2 = load_dataset(\"madao33/new-title-chinese\",split=\"train[10:100]\") \n",
    "# 加载指定比例的数据集（这里返回了一个列表）\n",
    "trainset3 = load_dataset(\"madao33/new-title-chinese\",split=[\"train[:50%]\",\"validation[10%:]\"]) # 注意这里只能是%，并且这里分别表示前50%和后10%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 查看数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': ['望海楼美国打“台湾牌”是危险的赌博', '大力推进高校治理能力建设'],\n",
       " 'content': ['近期，美国国会众院通过法案，重申美国对台湾的承诺。对此，中国外交部发言人表示，有关法案严重违反一个中国原则和中美三个联合公报规定，粗暴干涉中国内政，中方对此坚决反对并已向美方提出严正交涉。\\n事实上，中国高度关注美国国内打“台湾牌”、挑战一中原则的危险动向。近年来，作为“亲台”势力大本营的美国国会动作不断，先后通过“与台湾交往法”“亚洲再保证倡议法”等一系列“挺台”法案，“2019财年国防授权法案”也多处触及台湾问题。今年3月，美参院亲台议员再抛“台湾保证法”草案。众院议员继而在4月提出众院版的草案并在近期通过。上述法案的核心目标是强化美台关系，并将台作为美“印太战略”的重要伙伴。同时，“亲台”议员还有意制造事端。今年2月，5名共和党参议员致信众议院议长，促其邀请台湾地区领导人在国会上发表讲话。这一动议显然有悖于美国与台湾的非官方关系，其用心是实质性改变美台关系定位。\\n上述动向出现并非偶然。在中美建交40周年之际，两国关系摩擦加剧，所谓“中国威胁论”再次沉渣泛起。美国对华认知出现严重偏差，对华政策中负面因素上升，保守人士甚至成立了“当前中国威胁委员会”。在此背景下，美国将台海关系作为战略抓手，通过打“台湾牌”在双边关系中增加筹码。特朗普就任后，国会对总统外交政策的约束力和塑造力加强。其实国会推动通过涉台法案对行政部门不具约束力，美政府在2018年并未提升美台官员互访级别，美军舰也没有“访问”台湾港口，保持着某种克制。但从美总统签署国会通过的法案可以看出，国会对外交产生了影响。立法也为政府对台政策提供更大空间。\\n然而，美国需要认真衡量打“台湾牌”成本。首先是美国应对危机的代价。美方官员和学者已明确发出警告，美国卷入台湾问题得不偿失。美国学者曾在媒体发文指出，如果台海爆发危机，美国可能需要“援助”台湾，进而导致新的冷战乃至与中国大陆的冲突。但如果美国让台湾自己面对，则有损美国的信誉，影响美盟友对同盟关系的支持。其次是对中美关系的危害。历史证明，中美合则两利、斗则两伤。中美关系是当今世界最重要的双边关系之一，保持中美关系的稳定发展，不仅符合两国和两国人民的根本利益，也是国际社会的普遍期待。美国蓄意挑战台湾问题的底线，加剧中美关系的复杂性和不确定性，损害两国在重要领域合作，损人又害己。\\n美国打“台湾牌”是一场危险的赌博。台湾问题是中国核心利益，中国政府和人民决不会对此坐视不理。中国敦促美方恪守一个中国原则和中美三个联合公报规定，阻止美国会审议推进有关法案，妥善处理涉台问题。美国悬崖勒马，才是明智之举。\\n（作者系中国国际问题研究院国际战略研究所副所长）',\n",
       "  '在推进“双一流”高校建设进程中，我们要紧紧围绕为党育人、为国育才，找准问题、破解难题，以一流意识和担当精神，大力推进高校的治理能力建设。\\n增强政治引领力。坚持党对高校工作的全面领导，始终把政治建设摆在首位，增强校党委的政治领导力，全面推进党的建设各项工作。落实立德树人根本任务，把培养社会主义建设者和接班人放在中心位置。紧紧抓住思想政治工作这条生命线，全面加强师生思想政治工作，推进“三全育人”综合改革，将思想政治工作贯穿学校教育管理服务全过程，努力让学生成为德才兼备、全面发展的人才。\\n提升人才聚集力。人才是创新的核心要素，创新驱动本质上是人才驱动。要坚持引育并举，建立绿色通道，探索知名专家举荐制，完善“一事一议”支持机制。在大力支持自然科学人才队伍建设的同时，实施哲学社会科学人才工程。立足实际，在条件成熟的学院探索“一院一策”改革。创新科研组织形式，为人才成长创设空间，建设更加崇尚学术、更加追求卓越、更加关爱学生、更加担当有为的学术共同体。\\n培养学生竞争力。遵循学生成长成才的规律培育人才，着力培养具有国际竞争力的拔尖创新人才和各类专门人才，使优势学科、优秀教师、优质资源、优良环境围绕立德树人的根本任务配置。淘汰“水课”，打造“金课”，全力打造世界一流本科教育。深入推进研究生教育综合改革，加强事关国家重大战略的高精尖急缺人才培养，建设具有国际竞争力的研究生教育。\\n激发科技创新力。在国家急需发展的领域挑大梁，就要更加聚焦科技前沿和国家需求，狠抓平台建设，包括加快牵头“武汉光源”建设步伐，积极参与国家实验室建设，建立校级大型科研仪器设备共享平台。关键核心技术领域“卡脖子”问题，归根结底是基础科学研究薄弱。要加大基础研究的支持力度，推进理论、技术和方法创新，鼓励支持重大原创和颠覆性技术创新，催生一批高水平、原创性研究成果。\\n发展社会服务力。在贡献和服务中体现价值，推动合作共建、多元投入的格局，大力推进政产学研用结合，强化科技成果转移转化及产业化。探索校城融合发展、校地联动发展的新模式，深度融入地方创新发展网络，为地方经济社会发展提供人才支撑，不断拓展和优化社会服务网络。\\n涵育文化软实力。加快体制机制改革，优化学校、学部、学院三级评审机制，充分发挥优秀学者特别是德才兼备的年轻学者在学术治理中的重要作用。牢固树立一流意识、紧紧围绕一流目标、认真执行一流标准，让成就一流事业成为普遍追求和行动自觉。培育具有强大凝聚力的大学文化，营造积极团结、向上向善、干事创业的氛围，让大学成为吸引和留住一大批优秀人才建功立业的沃土，让敢干事、肯干事、能干事的人有更多的荣誉感和获得感。\\n建设中国特色、世界一流大学不是等得来、喊得来的，而是脚踏实地拼出来、干出来的。对标一流，深化改革，坚持按章程办学，构建以一流质量标准为核心的制度规范体系，扎实推进学校综合改革，探索更具活力、更富效率的管理体制和运行机制，我们就一定能构建起具有中国特色的现代大学治理体系，进一步提升管理服务水平和工作效能。\\n（作者系武汉大学校长）']}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[\"train\"][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['望海楼美国打“台湾牌”是危险的赌博', '大力推进高校治理能力建设', '坚持事业为上选贤任能']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[\"train\"][\"title\"][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['title', 'content']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[\"train\"].column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': Value(dtype='string', id=None),\n",
       " 'content': Value(dtype='string', id=None)}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[\"train\"].features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 数据集的划分（split）, 选取（select）， 过滤（filter）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**划分**：将数据集划分为训练集、测试集等"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['question', 'passage', 'idx', 'label'],\n",
      "    num_rows: 9427\n",
      "}) \n",
      " {'question': ['do iran and afghanistan speak the same language', 'do good samaritan laws protect those who help at an accident'], 'passage': ['Persian language -- Persian (/ˈpɜːrʒən, -ʃən/), also known by its endonym Farsi (فارسی fārsi (fɒːɾˈsiː) ( listen)), is one of the Western Iranian languages within the Indo-Iranian branch of the Indo-European language family. It is primarily spoken in Iran, Afghanistan (officially known as Dari since 1958), and Tajikistan (officially known as Tajiki since the Soviet era), and some other regions which historically were Persianate societies and considered part of Greater Iran. It is written in the Persian alphabet, a modified variant of the Arabic script, which itself evolved from the Aramaic alphabet.', \"Good Samaritan law -- Good Samaritan laws offer legal protection to people who give reasonable assistance to those who are, or who they believe to be, injured, ill, in peril, or otherwise incapacitated. The protection is intended to reduce bystanders' hesitation to assist, for fear of being sued or prosecuted for unintentional injury or wrongful death. An example of such a law in common-law areas of Canada: a good Samaritan doctrine is a legal principle that prevents a rescuer who has voluntarily helped a victim in distress from being successfully sued for wrongdoing. Its purpose is to keep people from being reluctant to help a stranger in need for fear of legal repercussions should they make some mistake in treatment. By contrast, a duty to rescue law requires people to offer assistance and holds those who fail to do so liable.\"], 'idx': [0, 1], 'label': [1, 1]}\n"
     ]
    }
   ],
   "source": [
    "# 首先载入数据集并观察一下，发现这个数据集主要有'question'...'label'共四个feature\n",
    "data = boolq_datasets[\"train\"]\n",
    "print(data,\"\\n\",data[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question', 'passage', 'idx', 'label'],\n",
       "        num_rows: 8484\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['question', 'passage', 'idx', 'label'],\n",
       "        num_rows: 943\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 可以通过额外添加指令指定数据集划分的比例依据\n",
    "data.train_test_split(test_size = 0.1, stratify_by_column=\"label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**数据集的选取与过滤**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['title', 'content'],\n",
       "    num_rows: 2\n",
       "})"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[\"train\"].select([0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这与上面的直接调取数据查看不同，若用上面的方法`datasets[\"train\"][:3]`直接调用的话则会返回一个python的字典数据；而通过`datasets[\"train\"].select(...)`则仍然会返回一个`Dataset`对象，相当于是抽取了一个子集。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里数据的筛选还可以通过`filter(lambda example: ...)`函数来进行高级筛选："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\xinby\\.cache\\huggingface\\datasets\\madao33___csv\\madao33--new-title-chinese-5c39318853719c17\\0.0.0\\eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d\\cache-2d4b892f9bd3b103.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': ['聚焦两会，世界探寻中国成功秘诀', '望海楼中国经济的信心来自哪里'], 'content': ['世界眼中的中国两会，是一道集聚全国人民磅礴之力的风景线，不仅展现着新时代中国的意气风发，而且增添着世界的生机和动能\\n\\u3000\\u3000两会时间，世界研究中国的重要时间。\\n中国智慧、中国方略、中国成就集中展现，来自世界的热评此起彼伏。“中国两会对全球事务有着巨大影响力”“是时候对中国的突飞猛进有清醒的认识了”“中国方案为世界提供借鉴”……来自世界的这些叙述，折射着探寻成功秘诀的目光。各国媒体纷纷向北京增派记者，3000多名中外记者报名采访中国两会。世界瞩目中国两会，期待进一步读懂新时代的中国。\\n作为世界最大发展中国家和第二大经济体，中国不断创造着人类发展史上惊天动地的奇迹。中国提供着最高的对世界经济增长的贡献率、最高的对世界减贫事业的贡献率，中国建成了世界上最大的社会保障网、高速铁路网，中国科技创新在诸多领域实现并跑、领跑……国际舆论普遍认为，着眼自身治理能力现代化的中国，日益为全球治理作出重要贡献。\\n世界经济复苏发展，中国功不可没。中国国家统计局最新数据显示，2017年，中国国内生产总值增速达6.9%，占世界经济的比重达15%，稳居世界第二。“中国的成功故事与世界经济的命运紧密相连。”国际货币基金组织第一副总裁戴维·利普顿这样评价。“世行对中国经济发展有信心，对中国持续成为全球经济增长引擎有信心。”世界银行发展预测局局长阿伊汗·高斯为中国点赞。\\n国外不少人感叹，世界太需要像中国这样不断书写成功故事。但是，环顾当下的世界，一些国家深陷矛盾丛生、乱象频发的怪圈。在今年年初的达沃斯论坛年会和慕尼黑安全会议上，来自西方一些国家的人士纷纷大谈对社会分裂、政治极化、民粹主义盛行之忧。国际政治、社会领域诸多乱象，从若干侧面说明了失却方向之痛、道路探索之难。与此形成鲜明反差的是，中国的稳步发展和繁荣景象，无疑给不乏深忧的世界注入了昂扬振奋的力量。\\n中国的成功故事，揭示了中国道路自信、理论自信、制度自信、文化自信之源。中共十九大的胜利召开，为中国建设现代化强国指明了方向，为中华民族伟大复兴做了全面谋划。世界瞩目，十九大后的首次中国两会将如何围绕民众普遍关心的反腐倡廉、社会保障、教育公平、医疗改革、脱贫攻坚、依法治国、改革开放等一系列议题谋篇布局，中国智慧如何为破解各国面临的共同难题提供启示。\\n“立治有体，施治有序。”欧洲智库专家认为，中国在国家治理和推进改革方面展现的强大领导力，堪为世界各国推进改革的重要借鉴。不少报道过中国两会的外国记者表示，人大代表和政协委员积极为国家发展履职尽责、建言献策的情景让他们深刻体会到，中国的人民代表大会制度和中国共产党领导的多党合作和政治协商制度具有明显优越性，与西方一些执政党和反对党为了各自政治利益争论不休的场面有天壤之别。\\n站在新的历史起点上，中国蓄势待发。世界眼中的中国两会，是一道集聚全国人民磅礴之力的风景线，不仅展现着新时代中国的意气风发，而且增添着世界的生机和动能。\\n《 人民日报 》（ 2018年03月05日 \\xa0 06 版）\\n', '“中国经济超预期地稳定。”这是路透社对2019年中国经济开局成绩单的评价。中国经济正以欣欣向荣的态势，赢得国内外的信心和认可。\\n在国际上，许多外企负责人看好中国经济前景；投资机构将中国列入投资首选目的地；国际货币基金组织等一众机构纷纷上调对中国经济增速预期；在中国国内，市场预期明显改善。3月，制造业采购经理指数重回扩张区间，比上月上升1.3个百分点；一季度，消费者信心指数为124.6，比上年第四季度提高3.2。\\n市场预期和信心之所以关键，就在于其不仅基于过去经济的表现，更指向并影响着经济的未来。对中国经济的信心来自哪里？我们不妨透过几组关系来看一看。\\n一看近期表现平稳向好，中长期潜力可观。近期表现无需多言，一季度经济数据显示，中国经济总量和结构都呈现平稳向好态势，经济增速、就业、物价、国际收支等主要指标处在合理区间，新动能正成为增长新亮点。中长期，中国发展仍处于并将长期处于重要战略机遇期。从共建“一带一路”倡议、京津冀协同发展战略到长江经济带战略、粤港澳大湾区建设等，中国正以行动化机遇为发展的现实。中国经济发展的周期已经持续了40年，且创下了9%左右的平均增速，沿着改革开放的道路走下去，中国经济发展的周期还会继续延长。\\n二看供给和需求两侧空间广，后劲足。在供给端，供给侧结构性改革正在全产业进行。中国已拥有全球最完整的产业体系，在这个基础上，推动质量变革、效率变革、动力变革，中国产业的竞争力、产品的竞争力和企业的竞争力全面提升。在需求端，一个近14亿人口的大市场孕育着多样化的需求；一个4亿人左右而且快速增长的中等收入群体推动着消费升级不断向前；中国新型工业化、信息化、城镇化、农业现代化还处在同步发展阶段，城乡、区域之间发展的不平衡蕴藏着可观的空间。供需两端，既携手向前，又构建了中国经济巨大的发展韧性和广阔的回旋余地。\\n三看宏观政策储备充分，微观主体活力充沛，中国经济动力持久。宏观政策层面，今年以来积极效应正不断显现。稳健的货币政策保持流动性合理充裕，积极的财政政策加力提效。从个税、增值税、社保费调降到“放管服”等一系列改革，政策为经济平稳运行保驾护航。中国官方多次表示，中国政策储备还有很多，政策空间还很充分。微观主体层面，正与政策实现良性互动。一季度，全国日均新登记企业1.65万户，增长12.3%。工信部对1.5万家企业的调查显示，2/3的企业表示订单情况良好。以企业为主体，中国的创新亦呈现蓬勃朝气，一批新产业、新技术、新产品、新模式接连出现。政策保持连续性稳定性，微观主体活力不断增强，有着更可期的未来，中国经济亦有持久的动力和活力。\\n“中国经济是一片大海，而不是一个小池塘。”“狂风骤雨可以掀翻小池塘，但不能掀翻大海。”中国经济，有大海的魄力；我们更有足够的理由满怀信心，努力奋进，拥抱中国经济更好的未来。']}\n"
     ]
    }
   ],
   "source": [
    "filtered_data = datasets[\"train\"].filter(lambda example: \"中国\" in example[\"title\"])\n",
    "print(filtered_data[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 数据集的处理（map）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "574dbe532e384d37b92943d0c53ac7f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5850 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': ['标题：望海楼美国打“台湾牌”是危险的赌博', '标题：大力推进高校治理能力建设'], 'content': ['近期，美国国会众院通过法案，重申美国对台湾的承诺。对此，中国外交部发言人表示，有关法案严重违反一个中国原则和中美三个联合公报规定，粗暴干涉中国内政，中方对此坚决反对并已向美方提出严正交涉。\\n事实上，中国高度关注美国国内打“台湾牌”、挑战一中原则的危险动向。近年来，作为“亲台”势力大本营的美国国会动作不断，先后通过“与台湾交往法”“亚洲再保证倡议法”等一系列“挺台”法案，“2019财年国防授权法案”也多处触及台湾问题。今年3月，美参院亲台议员再抛“台湾保证法”草案。众院议员继而在4月提出众院版的草案并在近期通过。上述法案的核心目标是强化美台关系，并将台作为美“印太战略”的重要伙伴。同时，“亲台”议员还有意制造事端。今年2月，5名共和党参议员致信众议院议长，促其邀请台湾地区领导人在国会上发表讲话。这一动议显然有悖于美国与台湾的非官方关系，其用心是实质性改变美台关系定位。\\n上述动向出现并非偶然。在中美建交40周年之际，两国关系摩擦加剧，所谓“中国威胁论”再次沉渣泛起。美国对华认知出现严重偏差，对华政策中负面因素上升，保守人士甚至成立了“当前中国威胁委员会”。在此背景下，美国将台海关系作为战略抓手，通过打“台湾牌”在双边关系中增加筹码。特朗普就任后，国会对总统外交政策的约束力和塑造力加强。其实国会推动通过涉台法案对行政部门不具约束力，美政府在2018年并未提升美台官员互访级别，美军舰也没有“访问”台湾港口，保持着某种克制。但从美总统签署国会通过的法案可以看出，国会对外交产生了影响。立法也为政府对台政策提供更大空间。\\n然而，美国需要认真衡量打“台湾牌”成本。首先是美国应对危机的代价。美方官员和学者已明确发出警告，美国卷入台湾问题得不偿失。美国学者曾在媒体发文指出，如果台海爆发危机，美国可能需要“援助”台湾，进而导致新的冷战乃至与中国大陆的冲突。但如果美国让台湾自己面对，则有损美国的信誉，影响美盟友对同盟关系的支持。其次是对中美关系的危害。历史证明，中美合则两利、斗则两伤。中美关系是当今世界最重要的双边关系之一，保持中美关系的稳定发展，不仅符合两国和两国人民的根本利益，也是国际社会的普遍期待。美国蓄意挑战台湾问题的底线，加剧中美关系的复杂性和不确定性，损害两国在重要领域合作，损人又害己。\\n美国打“台湾牌”是一场危险的赌博。台湾问题是中国核心利益，中国政府和人民决不会对此坐视不理。中国敦促美方恪守一个中国原则和中美三个联合公报规定，阻止美国会审议推进有关法案，妥善处理涉台问题。美国悬崖勒马，才是明智之举。\\n（作者系中国国际问题研究院国际战略研究所副所长）', '在推进“双一流”高校建设进程中，我们要紧紧围绕为党育人、为国育才，找准问题、破解难题，以一流意识和担当精神，大力推进高校的治理能力建设。\\n增强政治引领力。坚持党对高校工作的全面领导，始终把政治建设摆在首位，增强校党委的政治领导力，全面推进党的建设各项工作。落实立德树人根本任务，把培养社会主义建设者和接班人放在中心位置。紧紧抓住思想政治工作这条生命线，全面加强师生思想政治工作，推进“三全育人”综合改革，将思想政治工作贯穿学校教育管理服务全过程，努力让学生成为德才兼备、全面发展的人才。\\n提升人才聚集力。人才是创新的核心要素，创新驱动本质上是人才驱动。要坚持引育并举，建立绿色通道，探索知名专家举荐制，完善“一事一议”支持机制。在大力支持自然科学人才队伍建设的同时，实施哲学社会科学人才工程。立足实际，在条件成熟的学院探索“一院一策”改革。创新科研组织形式，为人才成长创设空间，建设更加崇尚学术、更加追求卓越、更加关爱学生、更加担当有为的学术共同体。\\n培养学生竞争力。遵循学生成长成才的规律培育人才，着力培养具有国际竞争力的拔尖创新人才和各类专门人才，使优势学科、优秀教师、优质资源、优良环境围绕立德树人的根本任务配置。淘汰“水课”，打造“金课”，全力打造世界一流本科教育。深入推进研究生教育综合改革，加强事关国家重大战略的高精尖急缺人才培养，建设具有国际竞争力的研究生教育。\\n激发科技创新力。在国家急需发展的领域挑大梁，就要更加聚焦科技前沿和国家需求，狠抓平台建设，包括加快牵头“武汉光源”建设步伐，积极参与国家实验室建设，建立校级大型科研仪器设备共享平台。关键核心技术领域“卡脖子”问题，归根结底是基础科学研究薄弱。要加大基础研究的支持力度，推进理论、技术和方法创新，鼓励支持重大原创和颠覆性技术创新，催生一批高水平、原创性研究成果。\\n发展社会服务力。在贡献和服务中体现价值，推动合作共建、多元投入的格局，大力推进政产学研用结合，强化科技成果转移转化及产业化。探索校城融合发展、校地联动发展的新模式，深度融入地方创新发展网络，为地方经济社会发展提供人才支撑，不断拓展和优化社会服务网络。\\n涵育文化软实力。加快体制机制改革，优化学校、学部、学院三级评审机制，充分发挥优秀学者特别是德才兼备的年轻学者在学术治理中的重要作用。牢固树立一流意识、紧紧围绕一流目标、认真执行一流标准，让成就一流事业成为普遍追求和行动自觉。培育具有强大凝聚力的大学文化，营造积极团结、向上向善、干事创业的氛围，让大学成为吸引和留住一大批优秀人才建功立业的沃土，让敢干事、肯干事、能干事的人有更多的荣誉感和获得感。\\n建设中国特色、世界一流大学不是等得来、喊得来的，而是脚踏实地拼出来、干出来的。对标一流，深化改革，坚持按章程办学，构建以一流质量标准为核心的制度规范体系，扎实推进学校综合改革，探索更具活力、更富效率的管理体制和运行机制，我们就一定能构建起具有中国特色的现代大学治理体系，进一步提升管理服务水平和工作效能。\\n（作者系武汉大学校长）']}\n"
     ]
    }
   ],
   "source": [
    "def add_prefix(example):\n",
    "    example[\"title\"] = \"标题：\" + example[\"title\"]\n",
    "    return example\n",
    "prefix_data = datasets[\"train\"].map(add_prefix)\n",
    "print(prefix_data[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以额外声明:\n",
    "1. `.map(...,batched = True)`来进行batch数据的处理\n",
    "2. `.map(...,num_proc = NUM )`来进行多进程处理（NUM为进程数）\n",
    "3. `.map(...,remove_colomns=[...])`来删除某些列"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 在线数据集的保存与加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_datasets.save_to_disk(\"...\")\n",
    "processed_dataset.load_from_disk(\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6 加载本地数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 直接加载\n",
    "dataset = load_dataset(\"csv\", data_files = \"ADDRESS\", split=\"train\")\n",
    "# 加载csv\n",
    "dataset = Dataset.from_csv(\"ADDRESS\")\n",
    "# 加载一个文件夹下的所有csv文件\n",
    "dataset = load_dataset(\"csv\", data_dir =\"ADDRESS\", split=\"train\")\n",
    "# 通过pandas加载（进行格式转换）\n",
    "import pandas as pd\n",
    "pd_data = pd.read_csv(\"ADDRESS\")\n",
    "dataset = Dataset.from_pandas(pd_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice 1: 利用AutoClass 拆解 Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "粗浅的说，Pipeline的构成为：\n",
    "- A Tokenizer instance in charge of mapping raw textual input to token\n",
    "- A Model instance\n",
    "- Some (optional) post processing for enhancing model’s output\n",
    "来源：Pipelines ‒ transformers 3.0.2 documentation\n",
    "\n",
    "因此我们可以将封装好的这个Pipeline进行拆解，分别自行指定Tokenizer、Model来实现对Pipeline的复现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MyPipelineForSA(object):\n",
    "    def __init__(self, checkpoint):\n",
    "        \"\"\"\n",
    "        初始化 MyPipelineForSA 类的实例\n",
    "        \n",
    "        参数：\n",
    "            - checkpoint：预训练模型的路径或名称\n",
    "        \"\"\"\n",
    "        # 预训练模型的分词器 Tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "        # 预训练模型（含下游模型）Model\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "    \n",
    "    def __call__(self, sent_list):\n",
    "        \"\"\"\n",
    "        对给定的句子列表进行情感分析的预测\n",
    "        \n",
    "        参数：\n",
    "            - sent_list：待分析的句子列表\n",
    "            \n",
    "        返回：\n",
    "            - prediction：包含情感分析结果的字典列表\n",
    "        \"\"\"\n",
    "        # 使用预训练模型的分词器对输入文本进行分词和编码，并进行填充\n",
    "        inputs = self.tokenizer(sent_list, padding=True, return_tensors=\"pt\")\n",
    "        # 使用预训练模型进行情感分析的前向传播计算\n",
    "        outputs = self.model(**inputs)\n",
    "        # 后处理\n",
    "        # 对输出进行 softmax 操作，得到各类别的概率分布\n",
    "        outputs = F.softmax(outputs.logits, dim=1)\n",
    "        # 获取预测结果中每个类别的标签和对应的分数\n",
    "        id2label = self.model.config.id2label\n",
    "        prediction = list(map(lambda x: {\"label\": id2label[x.argmax().item()],\n",
    "                                          \"score\": x.max().item()}, outputs))\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9998130202293396}, {'label': 'NEGATIVE', 'score': 0.9995473027229309}]\n"
     ]
    }
   ],
   "source": [
    "# 创建 MyPipelineForSA 类的实例，并指定预训练模型的 checkpoint\n",
    "classifier = MyPipelineForSA(checkpoint=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "# 调用实例，对给定的句子列表进行情感分析的预测\n",
    "result = classifier([\"I love HuggingFace!\", \"I hate it so much!\"])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice 2: 模型微调"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. 数据集的加载与清洗**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>距离川沙公路较近,但是公交指示不对,如果是\"蔡陆线\"的话,会非常麻烦.建议用别的路线.房间较...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>商务大床房，房间很大，床有2M宽，整体感觉经济实惠不错!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>早餐太差，无论去多少人，那边也不加食品的。酒店应该重视一下这个问题了。房间本身很好。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>宾馆在小街道上，不大好找，但还好北京热心同胞很多~宾馆设施跟介绍的差不多，房间很小，确实挺小...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>CBD中心,周围没什么店铺,说5星有点勉强.不知道为什么卫生间没有电吹风</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                             review\n",
       "0      1  距离川沙公路较近,但是公交指示不对,如果是\"蔡陆线\"的话,会非常麻烦.建议用别的路线.房间较...\n",
       "1      1                       商务大床房，房间很大，床有2M宽，整体感觉经济实惠不错!\n",
       "2      1         早餐太差，无论去多少人，那边也不加食品的。酒店应该重视一下这个问题了。房间本身很好。\n",
       "3      1  宾馆在小街道上，不大好找，但还好北京热心同胞很多~宾馆设施跟介绍的差不多，房间很小，确实挺小...\n",
       "4      1               CBD中心,周围没什么店铺,说5星有点勉强.不知道为什么卫生间没有电吹风"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import pandas as pd\n",
    "# 加载数据集\n",
    "data = pd.read_csv(\"ChnSentiCorp_htl_all.csv\")\n",
    "# 数据清洗\n",
    "data = data.dropna()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. 创建Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\t距离川沙公路较近,但是公交指示不对,如果是\"蔡陆线\"的话,会非常麻烦.建议用别的路线.房间较为简单.\n",
      "1\t商务大床房，房间很大，床有2M宽，整体感觉经济实惠不错!\n",
      "1\t早餐太差，无论去多少人，那边也不加食品的。酒店应该重视一下这个问题了。房间本身很好。\n",
      "1\t宾馆在小街道上，不大好找，但还好北京热心同胞很多~宾馆设施跟介绍的差不多，房间很小，确实挺小，但加上低价位因素，还是无超所值的；环境不错，就在小胡同内，安静整洁，暖气好足-_-||。。。呵还有一大优势就是从宾馆出发，步行不到十分钟就可以到梅兰芳故居等等，京味小胡同，北海距离好近呢。总之，不错。推荐给节约消费的自助游朋友~比较划算，附近特色小吃很多~\n",
      "1\tCBD中心,周围没什么店铺,说5星有点勉强.不知道为什么卫生间没有电吹风\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    ''' \n",
    "    自定义数据集类，继承自 torch.utils.data.Dataset\n",
    "    '''\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        '''\n",
    "        初始化 MyDataset 类的实例\n",
    "        '''\n",
    "        # 调用父类的初始化方法\n",
    "        super().__init__() \n",
    "        # 加载数据集\n",
    "        self.data = pd.read_csv(\"ChnSentiCorp_htl_all.csv\")\n",
    "        # 数据清洗\n",
    "        self.data = self.data.dropna()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        '''\n",
    "        根据给定的索引index, 返回对应的数据\n",
    "        '''\n",
    "        # 获取该条数据对应的文本和标签\n",
    "        text = self.data.iloc[index][0]\n",
    "        label = self.data.iloc[index][1]\n",
    "        # 返回该条数据\n",
    "        return text, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        '''\n",
    "        返回数据集的总长度\n",
    "        '''\n",
    "        return len(self.data)\n",
    "    \n",
    "dataset = MyDataset()\n",
    "for i in range(5):\n",
    "    print(*dataset[i], sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. 划分数据集**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Sum of input lengths does not equal the length of the input dataset!",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[16], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[39mfrom\u001B[39;00m \u001B[39mtorch\u001B[39;00m\u001B[39m.\u001B[39;00m\u001B[39mutils\u001B[39;00m\u001B[39m.\u001B[39;00m\u001B[39mdata\u001B[39;00m \u001B[39mimport\u001B[39;00m random_split\n\u001B[0;32m----> 3\u001B[0m trainset, validset \u001B[39m=\u001B[39m random_split(dataset, lengths\u001B[39m=\u001B[39;49m[\u001B[39m0.1\u001B[39;49m,\u001B[39m0.9\u001B[39;49m]) \n",
      "File \u001B[0;32m~/opt/anaconda3/envs/transformer_mac/lib/python3.10/site-packages/torch/utils/data/dataset.py:492\u001B[0m, in \u001B[0;36mrandom_split\u001B[0;34m(dataset, lengths, generator)\u001B[0m\n\u001B[1;32m    490\u001B[0m \u001B[39m# Cannot verify that dataset is Sized\u001B[39;00m\n\u001B[1;32m    491\u001B[0m \u001B[39mif\u001B[39;00m \u001B[39msum\u001B[39m(lengths) \u001B[39m!=\u001B[39m \u001B[39mlen\u001B[39m(dataset):\n\u001B[0;32m--> 492\u001B[0m     \u001B[39mraise\u001B[39;00m \u001B[39mValueError\u001B[39;00m(\u001B[39m\"\u001B[39m\u001B[39mSum of input lengths does not equal the length of the input dataset!\u001B[39m\u001B[39m\"\u001B[39m)\n\u001B[1;32m    494\u001B[0m indices \u001B[39m=\u001B[39m randperm(\u001B[39msum\u001B[39m(lengths), generator\u001B[39m=\u001B[39mgenerator)\u001B[39m.\u001B[39mtolist()\n\u001B[1;32m    495\u001B[0m \u001B[39mreturn\u001B[39;00m [Subset(dataset, indices[offset \u001B[39m-\u001B[39m length : offset]) \u001B[39mfor\u001B[39;00m offset, length \u001B[39min\u001B[39;00m \u001B[39mzip\u001B[39m(_accumulate(lengths), lengths)]\n",
      "\u001B[0;31mValueError\u001B[0m: Sum of input lengths does not equal the length of the input dataset!"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "trainset, validset = random_split(dataset, lengths=[0.1,0.9]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. 创建DataLoader（数据容器）**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xinby/opt/anaconda3/envs/transformer_mac/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'trainset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[39mfrom\u001B[39;00m \u001B[39mtorch\u001B[39;00m\u001B[39m.\u001B[39;00m\u001B[39mutils\u001B[39;00m\u001B[39m.\u001B[39;00m\u001B[39mdata\u001B[39;00m \u001B[39mimport\u001B[39;00m DataLoader\n\u001B[0;32m----> 2\u001B[0m trainloader \u001B[39m=\u001B[39m DataLoader(trainset, batch_size\u001B[39m=\u001B[39m\u001B[39m32\u001B[39m, shuffle\u001B[39m=\u001B[39m\u001B[39mTrue\u001B[39;00m)\n\u001B[1;32m      3\u001B[0m validloader \u001B[39m=\u001B[39m DataLoader(validset, batch_size\u001B[39m=\u001B[39m\u001B[39m64\u001B[39m, shuffle\u001B[39m=\u001B[39m\u001B[39mFalse\u001B[39;00m)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'trainset' is not defined"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "trainloader = DataLoader(trainset, batch_size=32, shuffle=True)\n",
    "validloader = DataLoader(validset, batch_size=64, shuffle=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}