{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Decision Boundary / Separating Hyperplane"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Decision Boundary / Separating Hyperplane"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**回忆Logistics Regression**\n",
    "\n",
    "logistics的拟合函数是：$h_\\theta(x) = g(\\theta^Tx)$.\n",
    "\n",
    "若以正常的阈值$p=0.5$作为分界来看，$\\theta^Tx>0$与否便构成了一个决策边界，或称separating hyperplane. 数据点距离该决策平面越远，则判断其属于某一类别的confidence越高."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 绘制sigmoid函数图像\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    " \n",
    "z = np.arange(-10, 10, 0.1)\n",
    "phi_z = sigmoid(z)\n",
    "\n",
    "plt.plot(z, phi_z)\n",
    "plt.axvline(0.0, color='k') # 在x=0处画一条垂直的线\n",
    "plt.axhspan(0.0, 1.0, facecolor='1.0', edgecolor='k', ls='dashed') # 在y=0和y=1处画一条水平的虚线\n",
    "plt.axhline(y=0.5, ls='dashed', color='k') # 在y=0.5处画一条水平的虚线\n",
    "plt.yticks([0.0, 0.5, 1.0]) # 设置y轴的刻度\n",
    "plt.ylim(-0.1, 1.1) # 设置y轴的范围\n",
    "plt.xlabel('z')\n",
    "plt.ylabel('sigmoid (z)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Some Notations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在本小节，我们将主要讨论一个**二元分类问题**，采用**linear classifier**. \n",
    "\n",
    "记标签为$y\\in \\{-1,1\\}$，相关特征为$x$，模型参数为$\\omega, \\beta$，构建的拟合模型为:\n",
    "$$ h_{\\omega, b}(x) = g(\\omega^Tx + b)$$\n",
    "其中$g(z) = \\text{sign}(z)$，即$g(z) = 1$ if $z\\geq 0$，$g(z) = -1$ if $z<0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Functional & Geometic Margins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Functional Margin $\\gamma$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functional Margin 作用 ：\n",
    "\n",
    "度量预测的*正确性*与*置信度*。具体而言，Func Margin为正说明预测正确，其绝对值越大说明预测越置信。\n",
    "\n",
    "> 注：其实由下面的构造形式可知，正的Func Margin意味着$y^{(i)}$和$\\omega^Tx^{(i)}$同号，即预测正确。\n",
    "\n",
    "#### Functional Margin 定义公式：\n",
    "  \n",
    "  $$\\hat{\\gamma}^{(i)} = y^{(i)}(\\omega^Tx^{(i)} + b)$$\n",
    "\n",
    "#### Functional Margin 存在的问题\n",
    "\n",
    "缺少normalization。同倍改变$\\omega$和$b$，Func Margin会变大，但并不会改变实际的决策边界提高预测效果。\n",
    "\n",
    "#### 训练集的Functional Margin\n",
    "\n",
    "给定训练集$S = \\{(x^{(i)}, y^{(i)}); i = 1, ..., m\\}$，则训练集的Func Margin（记为$\\hat\\gamma$）为所有样本的Func Margin的最小值，即\n",
    "\n",
    "$$\\hat\\gamma = \\min_{i=1, ..., m} \\hat\\gamma^{(i)}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Geometric Margin $\\hat\\gamma$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现给出如图所示的决策边界$\\omega^Tx+b=0$：\n",
    "\n",
    "![](https://michael-1313341240.cos.ap-shanghai.myqcloud.com/202308131632970.png)\n",
    "\n",
    "在上图中，我们可以给出如下事实陈述：\n",
    "\n",
    "- 由解析几何知识可知，向量$\\omega$定与超平面$\\omega^Tx+b=0$垂直（正交），即$\\omega$是超平面的法向量\n",
    "- 记$A$为$y^{(i)}=1$的一样本点（其坐标可表示为向量$x^{(i)}$），则A到决策边界的距离为垂线段$AB$，记之为$\\gamma^{(i)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**下面具体求解$\\gamma^{(i)}$**\n",
    "\n",
    "求解思路：我们已知 *(1) AB垂直于超平面; (2) B位于超平面$\\omega^Txb=0$上; (3)$\\omega$是超平面的一个法向量*。 因此我们可以求解B的坐标，通过 **B落在超平面上** 这一事实构造等式求解方程。\n",
    "\n",
    "具体而言，B的坐标等于*A的坐标沿着法向量方向回退$\\gamma^{(i)}$个长度的距离*，因此我们有：\n",
    "$$ \\omega^T(x^{(i)}-\\gamma^{(i)}\\frac{\\omega}{||\\omega||})+b=0$$\n",
    "\n",
    "通过求解上述方程，即可求得：\n",
    "$$\n",
    "\\gamma^{(i)}=\\frac{w^T x^{(i)}+b}{\\|w\\|}=\\left(\\frac{w}{\\|w\\|}\\right)^T x^{(i)}+\\frac{b}{\\|w\\|}\n",
    "$$\n",
    "\n",
    "更一般地，考虑到正负样本的不同，更一般的公式为：\n",
    "$$ \\gamma^{(i)}=y^{(i)}\\left(\\left(\\frac{w}{\\|w\\|}\\right)^T x^{(i)}+\\frac{b}{\\|w\\|}\\right)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**说明：**\n",
    "1. 当$||\\omega|| =1$时，Geometric Margin 等价于 Functional Margin\n",
    "2. 由于进行了正则化，Geo Margin杜绝了Func Margin存在的缩放问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同理，定义训练集（$S = \\{(x^{(i)}, y^{(i)}); i = 1, \\dots, m\\}$）的geometric margin：\n",
    "$$\\gamma = \\min_{i=1,\\dots,m} \\hat{\\gamma}^{(i)}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Optimal Margin Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "朴素地，我们希望得到的分割平面尽可能使得上文的margin $\\gamma$足够大，下面就是要量化这一想法。\n",
    "\n",
    "在本节的讨论中，我们都假设数据集是**线性可分的（linearly separable）**，即存在一些超平面可以将正负样本完全分开。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 朴素的优化问题\n",
    "\n",
    "上述的想法可以概括为如下优化问题：\n",
    "\n",
    "$$ \\begin{align*} \\max_{\\gamma, \\omega, b} \\quad &\\gamma \\\\\n",
    "\\text{s.t. } \\quad &y^{(i)}(\\omega^Tx^{(i)} + b) \\geq \\gamma, \\quad i = 1, \\ldots, m \\\\ \n",
    "&||\\omega|| = 1 \\end{align*} $$\n",
    "\n",
    "即，希望能够最大化$\\gamma$，同时满足所有的训练样本的*func. margin*都至少为$\\gamma$。同时参见5.3的讨论可知，$||\\omega|| = 1$使得* geo. margin *等价于* func. margin*。\n",
    "\n",
    "然而上述内容是***非凸优化*** 的，因此下给出一个等价的凸优化问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 改进的优化问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "归根到底我们希望得到的是Geo Margin最大的分类器，因此通过下列方式去掉非凸部分$||\\omega||=1$：\n",
    "\n",
    "$$ \\begin{align*} \\max_{\\hat\\gamma, \\omega, b} \\quad &\\frac{\\hat\\gamma}{||\\omega||} \\\\\n",
    "\\text{s.t. } \\quad &y^{(i)}(\\omega^Tx^{(i)} + b) \\geq \\hat\\gamma, \\quad i = 1, \\ldots, m \\end{align*} $$\n",
    "\n",
    "然而这里的目标函数$\\frac{\\hat\\gamma}{||\\omega||}$依然是非凸的(sad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 最终的凸优化问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最终我们得到的凸优化问题如下所示（这里我们先给出具体形式再解释其原理）：\n",
    "$$\n",
    "\\begin{align}\n",
    "\\min_{\\gamma,\\omega,b} \\quad & \\frac{1}{2}||\\omega||^2 \\\\\n",
    "\\text{s.t.} \\quad & y^{(i)}(\\omega^Tx^{(i)}+b) \\geq 1, \\quad i=1,\\cdots,m\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 首先我们可以规定$\\hat\\gamma = 1$，则原始优化问题可以转化为$\\max \\{1/||\\omega||\\}$\n",
    "\n",
    "    这是因为正如前文在介绍func. margin时所说，我们可以通过任意成倍地更改$\\omega, b$的值来改变func. margin的值，而这不会影响到我们的最优化问题的解。\n",
    "\n",
    "- 其次，我们可以将$\\max \\{1/||\\omega||\\}$转化为$\\min \\{||\\omega||^2\\}$，这是显然的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过上述变换最终得到的优化问题是凸优化可解的。这里的优化解即为**optimal margin classifier**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 Lagrange Duality 拉格朗日对偶"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[bilibili: “拉格朗日对偶问题”如何直观理解？“KKT条件” “Slater条件” “凸优化”打包理解](https://www.bilibili.com/video/BV1HP4y1Y79e/?share_source=copy_web&vd_source=9471c7cd3fca9ffedd9167aefed57c6d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intro: Lagrange Multiplier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "考虑如下形式的优化问题：\n",
    "$$\\begin{aligned}\n",
    "\\min_{w} &\\quad f(w) \\\\\n",
    "\\text{s.t.} &\\quad h_i(w) = 0, \\quad i = 1, \\ldots, l \n",
    "\\end{aligned}$$\n",
    "\n",
    "上述的优化问题可以通过**拉格朗日乘子法**来求解。\n",
    "\n",
    "\n",
    "具体地，我们可以得到*Lagarangian*：\n",
    "$$ \\mathcal{L(w,\\beta)}= f(w) + \\sum_{i=1}^l \\beta_i h_i (w)$$\n",
    "其中一系列引入的参数$\\beta_i$称为拉格朗日乘子 *(Lagrange multipliers)* .\n",
    "\n",
    "通过求解下列方程组求解Lagrange Mult.：\n",
    "$$ \\partial\\mathcal{L}/\\partial w = 0; \\quad \\partial\\mathcal{L}/\\partial \\beta_i = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Primal Optimization Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里我们需要把上面的Lagrange Mult进行推广，考虑**约束条件中存在不等式**的情况（我们称这种优化问题为 ***primal optimization problem（原始问题）*** 。用数学语言表示为：\n",
    "$$ \\begin{align}\n",
    "\\min_{w} \\quad & f(w) \\\\\n",
    "\\text{s.t.} \\quad & g_i(w) \\leq 0, \\quad i = 1, \\cdots, k\n",
    "\\\\& h_i(w) = 0, \\quad i = 1, \\cdots, l\n",
    "\\end{align} $$\n",
    "\n",
    "$\\diamond$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因此引入***Generalized Lagrangian***，即\n",
    "$$\n",
    "\\mathcal{L}(w,\\alpha,\\beta) = f(w) + \\sum_{i=1}^k \\alpha_i g_i(w) + \\sum_{i=1}^l\\beta_i h_i(w)\n",
    "$$\n",
    "\n",
    "其中$\\alpha_i$和$\\beta_i$是拉格朗日乘子.\n",
    "\n",
    "$\\diamond$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**另外定义$\\theta_{\\mathcal{P}}$** *(primal)*\n",
    "$$ \\theta_{\\mathcal{P}}(w) = \\max_{\\alpha,\\beta \\textit{ s.t. } \\alpha_i\\ge 0} \\mathcal{L(w,\\alpha,\\beta)} \\quad\\dagger$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以证明,当存在违反约束（式(2)/(3)）中的条件时，$\\theta_{\\mathcal{P}} \\to \\infty$ （即令 $\\mathcal{L}$ 中违反条件的项前的系数$\\alpha / \\beta \\to \\infty$即可）；相反地，若全部满足约束，则$\\theta_{\\mathcal{P}} = f(w)$，即：\n",
    "$$\n",
    "\\theta_{\\mathcal{P}}(w)= \\begin{cases}f(w) & \\text { if } w \\text { satisfies primal constraints } \\\\ \\infty & \\text { otherwise. }\\end{cases}\n",
    "$$\n",
    "\n",
    "$\\diamond$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因此，通过$\\theta_{\\mathcal{P}}$，我们就可以将*primal problem*的（1）～（3）式加以概括。\n",
    "\n",
    "式（1）描述的问题是：$\\min_{w}f(w)$，在满足约束时，就等价于$\\min_w\\theta_{\\mathcal{P}}(w,b)$，再代入式$(\\dagger)$有：\n",
    "$$\\min_w\\theta_{\\mathcal{P}}(w,b)=\\min_w\\max_{\\alpha,\\beta: \\alpha_i\\geq0} \\mathcal{L}(w,\\alpha,\\beta)\\quad \\star$$\n",
    "\n",
    "而这一优化问题与primal problem是完全等价的，且具有相同的解。为方便起见，我们记$p^* := \\min_w\\theta_{\\mathcal{P}}(w)$，称$p^*$为 ***primal optimal value***。\n",
    "\n",
    "$\\diamond$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dual Optimization Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **理解：** 对偶是实质相同但从不同角度提出不同提法的一对问题。有时候原问题 (Primal Problem) 不太好解，但是对偶问题 (Dual Problem) 却很好解，我们就可以通过求解对偶问题来迂回地解答原问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "类似地**定义Lagrange 对偶函数$\\theta_{\\mathcal{D}}$** *(dual)*：\n",
    "$$\\theta_{\\mathcal{D}}(\\alpha,\\beta) = \\min_{w} \\mathcal{L}(w,\\alpha,\\beta) ,\\alpha\\ge0    \\quad \\dagger^2$$ \n",
    "\n",
    "*注意，$\\theta_{\\mathcal{D}}$和$\\theta_{\\mathcal{P}}$的区别在于：$\\theta_{\\mathcal{D}}$是关于$\\alpha$和$\\beta$的优化函数，而$\\theta_{\\mathcal{P}}$是关于$w$的优化函数。*\n",
    "\n",
    "另外可以证明，无论原问题函数$\\theta_{\\mathcal{P}}$的凹凸性，恒有对偶函数$\\theta_{\\mathcal{D}}$是一个凹函数.\n",
    "\n",
    "> 凹函数：$ f(\\theta x_1 + (1-\\theta)x_2) \\ge \\theta f(x_1) + (1-\\theta)f(x_2) $\n",
    "\n",
    "$\\diamond$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "顺势，我们引入***dual optimization problem***:\n",
    "$$ \\max \\limits_{\\alpha,\\beta:\\alpha_i\\ge0} \\theta_{\\mathcal{D}}(\\alpha,\\beta) = \\max\\limits_{\\alpha,\\beta:\\alpha_i\\ge0} \\min_w \\mathcal{L}(w,\\alpha,\\beta) \\quad \\star\\star$$\n",
    "\n",
    "并同理记之为$d^*$。\n",
    "\n",
    "注意到，$p^*$与$d^*$的唯一区别是二者$\\min,\\max$的顺序不同。\n",
    "\n",
    "\n",
    "$\\diamond$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 强、弱对偶条件\n",
    "\n",
    "#### a. 弱对偶\n",
    "\n",
    "在任何条件下，可以证明下列不等关系是恒成立的：\n",
    "$$d^* = \\max_{\\alpha,\\beta:\\alpha_i\\ge0} \\min_w L(w,\\alpha,\\beta) \\le \\min_w \\max_{\\alpha,\\beta:\\alpha_i\\ge0} L(w,\\alpha,\\beta) = p^*$$\n",
    "\n",
    "记$p^*-d^*$为duality gap.\n",
    "\n",
    "#### b. 强对偶\n",
    "\n",
    "而在一些*特定条件*下，该不等式取等，即\n",
    "$$d^* = p^*$$\n",
    "在这些条件下，我们就可以通过求解对偶问题来求解原始问题。\n",
    "\n",
    "##### b-1 强对偶条件（1）: Slater's condition （充要条件）\n",
    "\n",
    "- $f, g_i$是凸函数\n",
    "- $h_i$是仿射函数\n",
    "  - 仿射函数: $h_i(w) = a_i^Tw + b_i$\n",
    "- $g_i$是strictly feasible的\n",
    "  - strictly feasible: $\\exist w, \\textit{ s.t. } \\forall i, g_i(w) < 0$\n",
    "\n",
    "此时，则$\\exist w^*, \\alpha^*, \\beta^*$，使得 $w^*$是primal problem的解，$\\alpha^*, \\beta^*$是dual problem的解，且$p^* = d^*=\\mathcal{L}(w^*,\\alpha^*,\\beta^*)$。\n",
    "\n",
    "##### b-2 强对偶条件（2）: KKT条件（充要条件）必要条件\n",
    "\n",
    "此外，还有KKT条件，即\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial}{\\partial w_i} \\mathcal{L}\\left(w^*, \\alpha^*, \\beta^*\\right) & =0, \\quad i=1, \\ldots, n \\\\\n",
    "\\frac{\\partial}{\\partial \\beta_i} \\mathcal{L}\\left(w^*, \\alpha^*, \\beta^*\\right) & =0, \\quad i=1, \\ldots, l \\\\\n",
    "\\alpha_i^* g_i\\left(w^*\\right) & =0, \\quad i=1, \\ldots, k \\\\\n",
    "g_i\\left(w^*\\right) & \\leq 0, \\quad i=1, \\ldots, k \\\\\n",
    "\\alpha^* & \\geq 0, \\quad i=1, \\ldots, k\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "其中，称等式（3）为***dual complementarity***条件，其含义为：若$\\alpha_i^*>0$，则$g_i(w^*)=0$. \n",
    "\n",
    "通过后面的描述可见，SVM只需要少量的*support vectors*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.6 Optimal Margin Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回忆，在*Optimal Margin Classifiers*中，我们考虑如下优化问题：\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\min_{\\gamma, w, b} \\quad & \\frac{1}{2} \\|w\\|^2 \\\\\n",
    "\\text{s.t.} \\quad & g_i(w) = -y^{(i)}(w^Tx^{(i)} + b) + 1 \\leq 0, \\quad i = 1, \\ldots, m\n",
    "\\end{aligned}$$\n",
    "\n",
    "由上文KKT条件中的 *(3)dual complementarity* 条件可知，只有那些满足 $g_i(w) = 0$ 的样本才会对最优解有贡献（贡献体现在$a_i>0$）。而这里的 $g_i(w)=-y^{(i)}(w^Tx^{(i)} + b) + 1$表示恰好落在margin上的样本，即支持向量。\n",
    "\n",
    "具体参见下图：\n",
    "\n",
    "![](https://michael-1313341240.cos.ap-shanghai.myqcloud.com/202308151603962.png)\n",
    "\n",
    "图中实线表示的是决策边界，虚线表示的是margin。我们可以看到，只有在虚线上的一个x和两个o样本是对边际有贡献的，称为***support vectors***\n",
    "\n",
    "$\\diamond$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们由此往下，尝试通过对偶问题解决原问题。\n",
    "\n",
    "首先引入向量内积符号：$ \\langle x, y \\rangle = x^T y $\n",
    "\n",
    "我们目前需要解决的optimal margin classifier的优化问题是：\n",
    "$$ \\begin{align*}\n",
    "\\mathcal{L}(w,b,\\alpha) = \\frac12 ||w||^2 - \\sum_{i=1}^m \\alpha_i \\left( y^{(i)} (w^T x^{(i)} + b) - 1 \\right)\n",
    "\\end{align*} \\quad [\\dagger] $$\n",
    "\n",
    "回忆：\n",
    "$$\\theta_{\\mathcal{D}}(\\alpha,\\beta) = \\min_{w} \\mathcal{L}(w,\\alpha,\\beta) ,\\alpha\\ge0 $$ \n",
    "\n",
    "因此我们分别对$w$和$b$求偏导，令其为0，得到：\n",
    "$$ \\begin{align}\n",
    "\\nabla_w \\mathcal{L}(w,b,\\alpha) &= w - \\sum_{i=1}^m \\alpha_i y^{(i)} x^{(i)} = 0 \\\\\n",
    "\\nabla_b \\mathcal{L}(w,b,\\alpha) &= \\sum_{i=1}^m \\alpha_i y^{(i)} = 0\n",
    "\\end{align} $$\n",
    "\n",
    "由(1) 解得：\n",
    "$$w = \\sum_{i=1}^m \\alpha_i y^{(i)} x^{(i)} \\quad [\\ast]$$\n",
    "\n",
    "直观的理解，这里的含义是指参数$w$一定是原始的训练集的线性组合。一个直觉的理解可以回忆线性回归中$\\hat\\beta = (X^T X)^{-1} X^T y$\n",
    "\n",
    "将这里的$w$代入$[\\dagger]$并化简，得到\n",
    "$$\\begin{align*}\n",
    "\\mathcal{L}(w,b,\\alpha) &= \\sum_{i=1}^m \\alpha_i - \\frac12 \\sum_{i,j=1}^m \\alpha_i \\alpha_j y^{(i)} y^{(j)} \\langle x^{(i)}, x^{(j)} \\rangle - b\\sum_{i=1}^m \\alpha_i y^{(i)} \\\\ \n",
    "&= \\sum_{i=1}^m \\alpha_i - \\frac12 \\sum_{i,j=1}^m \\alpha_i \\alpha_j y^{(i)} y^{(j)} \\langle x^{(i)}, x^{(j)} \\rangle  \\textit{ (by eqn. (2)) } \\\\\n",
    "\\end{align*}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最终可以得到如下的对偶问题 **(Dual Optimizatoin Problem of SVM)**：\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\max_{\\alpha} \\quad & W(\\alpha) = \\sum_{i=1}^m \\alpha_i - \\frac{1}{2} \\sum_{i,j=1}^m y^{(i)} y^{(j)} \\alpha_i \\alpha_j \\langle x^{(i)}, x^{(j)} \\rangle \\\\\n",
    "\\text{s.t.} \\quad & \\alpha_i \\geq 0, \\quad i = 1, \\ldots, m \\\\\n",
    "& \\sum_{i=1}^m \\alpha_i y^{(i)} = 0\n",
    "\\end{align*}$$\n",
    "\n",
    "可验证，本对偶问题符合KKT条件。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**预测过程**\n",
    "1. 求解 $a_i, b$\n",
    "2. 预测$h_{w,b}(x)=g(w^Tx+b) = g((\\sum_i a_i y^{(i)} x^{(i)})^Tx+b)=g(\\sum_i\\alpha_iy^{(i)}\\langle x^{(i)},x\\rangle+b)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.7 Kernels 核方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel Methods 思路\n",
    "\n",
    "**(1) 将算法写作含$\\langle x^{(i)},x^{(j)}\\rangle \\textit{( or note as } \\langle x,z\\rangle \\textit{) }$的形式**\n",
    "\n",
    "**(2) 映射 $x_\\textit{ (low dim)} \\rightarrow \\phi(x) _\\textit{ (high dim)}$**\n",
    "\n",
    "**(3) 寻找容易可行的计算$K(x,z) = \\phi(x)^T\\phi(z)$的方法**\n",
    "\n",
    "**(4) 讲上述算法中的$\\langle x,z\\rangle$替换为$K(x,z)$**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 例子"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设$x,z\\in\\mathbb{R}^n$\n",
    "\n",
    "在本例中，假设有三个特征$x_1,...,x_3$，以及映射函数：\n",
    "$$ \\phi(x) = [x_1x_1, x_1x_2,x_1x_3,...,x_3x_3]^T $$\n",
    "需要指出，正常直接计算$\\phi(x)$ （或$\\phi(x)^T\\phi(z)$）需要$O(n^2)$的时间\n",
    "\n",
    "$\\diamond$\n",
    "\n",
    "为化简计算，考虑如下核方法：\n",
    "$$ K(x,z) = \\phi(x)^T\\phi(z) =^{?} (x^Tz)^2 = ... = \\sum_{i,j=1}^n (x_ix_j)(z_iz_j) $$\n",
    "这个核方法的计算复杂度为$O(n)$\n",
    "\n",
    "可以通过单纯的代数运算证明，[?]处等式在给定映射关系$\\phi$后是可以直接推出的。\n",
    "\n",
    "$\\diamond$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "再考虑另外一种核方法：\n",
    "\n",
    "$$\\begin{align*}\n",
    "K(x,z) &= (x^Tz+c)^2\\\\\n",
    "&= \\sum \\limits_{i,j=1}^n (x_ix_j)(z_i z_j) + \\sum \\limits_{i=1}^n (\\sqrt{2c}x_i)(\\sqrt{2c}z_i) + c^2\n",
    "\\end{align*}$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
